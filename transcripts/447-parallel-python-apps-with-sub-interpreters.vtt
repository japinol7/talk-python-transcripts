WEBVTT

00:00:00.000 --> 00:00:02.720
It's an exciting time for the capabilities of Python.

00:00:02.720 --> 00:00:06.160
We have the faster CPython initiative going strong,

00:00:06.160 --> 00:00:07.840
the recent async work,

00:00:07.840 --> 00:00:09.440
the adoption of typing,

00:00:09.440 --> 00:00:10.680
and on this episode,

00:00:10.680 --> 00:00:14.140
we discuss a new isolation and parallelization capability

00:00:14.140 --> 00:00:16.720
coming to Python through sub-interpreters.

00:00:16.720 --> 00:00:20.240
We have Eric Snow who spearheaded the work to get them added to

00:00:20.240 --> 00:00:24.560
Python 3.12 and is working on the Python API for them in 3.11,

00:00:24.560 --> 00:00:26.240
along with Anthony Shaw,

00:00:26.240 --> 00:00:28.240
who's been pushing the boundaries of what you

00:00:28.240 --> 00:00:30.680
can already do with sub-interpreters.

00:00:30.680 --> 00:00:32.200
This is Talk Python to Me,

00:00:32.200 --> 00:00:36.320
episode 446, recorded December 5th, 2023.

00:00:36.320 --> 00:00:51.080
[MUSIC]

00:00:51.080 --> 00:00:52.560
Welcome to Talk Python to Me,

00:00:52.560 --> 00:00:54.440
a weekly podcast on Python.

00:00:54.440 --> 00:00:56.160
This is your host, Michael Kennedy.

00:00:56.160 --> 00:00:58.640
Follow me on Mastodon, where I'm @mkennedy,

00:00:58.640 --> 00:01:01.240
and follow the podcast using @talkpython,

00:01:01.240 --> 00:01:03.720
both on fosstodon.org.

00:01:03.720 --> 00:01:06.120
Keep up with the show and listen to over seven years of

00:01:06.120 --> 00:01:08.880
past episodes at talkpython.fm.

00:01:08.880 --> 00:01:12.520
We've started streaming most of our episodes live on YouTube.

00:01:12.520 --> 00:01:14.280
Subscribe to our YouTube channel over at

00:01:14.280 --> 00:01:17.160
talkpython.fm/youtube to get notified

00:01:17.160 --> 00:01:20.360
about upcoming shows and be part of that episode.

00:01:20.360 --> 00:01:24.640
This episode is sponsored by PyBites Developer Mindset Program.

00:01:24.640 --> 00:01:26.640
PyBytes core mission is to help you break

00:01:26.640 --> 00:01:28.920
the vicious cycle of tutorial paralysis

00:01:28.920 --> 00:01:31.200
through developing real-world applications.

00:01:31.200 --> 00:01:33.760
The PyBites Developer Mindset Program will help you build

00:01:33.760 --> 00:01:37.760
the confidence you need to become a highly effective developer.

00:01:37.760 --> 00:01:39.960
It's brought to you by Sentry.

00:01:39.960 --> 00:01:42.120
Don't let those errors go unnoticed.

00:01:42.120 --> 00:01:46.160
Use Sentry. Get started at talkpython.fm/sentry.

00:01:46.160 --> 00:01:49.440
Anthony, Eric, hello, and welcome to Talk Python.

00:01:49.440 --> 00:01:50.000
>> Great.

00:01:50.000 --> 00:01:52.120
>> Hey guys. It's really good to have you both here.

00:01:52.120 --> 00:01:54.240
You both have been on the show before,

00:01:54.240 --> 00:01:55.960
which is awesome.

00:01:55.960 --> 00:01:58.800
Eric, we've talked about several interpreters before,

00:01:58.800 --> 00:02:02.080
but they were a dream almost at the time.

00:02:02.080 --> 00:02:02.560
>> That's right.

00:02:02.560 --> 00:02:04.760
>> Now, they feel pretty real.

00:02:04.760 --> 00:02:07.080
>> That's right. It's been a long time coming.

00:02:07.080 --> 00:02:09.040
I think the last time we talked,

00:02:09.040 --> 00:02:10.280
I've always been hopeful,

00:02:10.280 --> 00:02:12.720
but it seems like it was getting closer.

00:02:12.720 --> 00:02:16.320
With 3.12, we were able to land PerinterpreterGill,

00:02:16.320 --> 00:02:18.600
which was the last piece,

00:02:18.600 --> 00:02:20.560
the foundational part I wanted to do.

00:02:20.560 --> 00:02:22.920
A lot of cleanup, a lot of work that had to get done,

00:02:22.920 --> 00:02:25.200
but that last piece got in for 3.12.

00:02:25.200 --> 00:02:27.440
>> Excellent. So good.

00:02:27.440 --> 00:02:30.440
Maybe let's just do a quick check-in with you all.

00:02:30.440 --> 00:02:32.040
It's been a while.

00:02:32.040 --> 00:02:34.440
Anthony, start with you, I guess.

00:02:34.440 --> 00:02:36.280
Quick intro for people who don't know you,

00:02:36.280 --> 00:02:37.640
although I don't know how that's possible,

00:02:37.640 --> 00:02:39.440
and then just what you've been up to.

00:02:39.440 --> 00:02:41.400
>> Yeah. I'm Anthony Shaw.

00:02:41.400 --> 00:02:42.480
I work at Microsoft.

00:02:42.480 --> 00:02:44.400
I lead the Python advocacy team,

00:02:44.400 --> 00:02:49.240
and I do lots of Python stuff open source.

00:02:49.240 --> 00:02:52.240
Testing things, building tools,

00:02:52.240 --> 00:02:55.680
blogging, building projects, sharing things.

00:02:55.680 --> 00:02:58.400
>> You have a book, something about the inside of Python?

00:02:58.400 --> 00:03:00.080
>> Yeah. I forgot about that.

00:03:00.080 --> 00:03:02.280
Yeah. There's a book called CPython Internals,

00:03:02.280 --> 00:03:03.480
which is a book all about

00:03:03.480 --> 00:03:06.040
the Python compiler and how it works.

00:03:06.040 --> 00:03:07.960
>> You suppressed the memory of writing it?

00:03:07.960 --> 00:03:09.080
Like it was too traumatic,

00:03:09.080 --> 00:03:09.960
it's down there.

00:03:09.960 --> 00:03:12.120
>> Yeah. I keep forgetting.

00:03:12.120 --> 00:03:15.840
Yeah. That book was for 3.9,

00:03:15.840 --> 00:03:20.280
and people keep asking me if I'm going to update it for 3.13,

00:03:20.280 --> 00:03:22.800
maybe because things keep changing.

00:03:22.800 --> 00:03:23.680
>> Yeah.

00:03:23.680 --> 00:03:26.240
>> Things have been changing at a more rapid pace than they

00:03:26.240 --> 00:03:27.920
were a few years ago as well,

00:03:27.920 --> 00:03:30.120
so that maybe makes it more challenging.

00:03:30.120 --> 00:03:35.000
Yeah. Recently, I've been doing some more research as well.

00:03:35.000 --> 00:03:38.520
So I just finished my master's a few months ago,

00:03:38.520 --> 00:03:39.720
and I started my PhD,

00:03:39.720 --> 00:03:43.560
and I'm looking at parallelism in Python as one of the topics.

00:03:43.560 --> 00:03:46.680
So I've been quite involved in sub-interpreters,

00:03:46.680 --> 00:03:48.960
and the free threading project,

00:03:48.960 --> 00:03:50.520
and some other stuff as well.

00:03:50.520 --> 00:03:52.840
>> Awesome. Congratulations on the master's degree.

00:03:52.840 --> 00:03:53.600
That's really great.

00:03:53.600 --> 00:03:54.160
>> Thanks.

00:03:54.160 --> 00:03:57.440
>> I didn't realize you're going further. So Eric.

00:03:57.440 --> 00:04:00.280
>> Eric Snow. So I've been working on Python as

00:04:00.280 --> 00:04:03.560
a core developer for over 10 years now,

00:04:03.560 --> 00:04:07.520
but I've been participating for even longer than that,

00:04:07.520 --> 00:04:09.520
and it's been good.

00:04:09.520 --> 00:04:11.960
I've worked on a variety of things,

00:04:11.960 --> 00:04:16.080
a lot of stuff down in the core runtime of CPython.

00:04:16.080 --> 00:04:17.960
I've been working on this,

00:04:17.960 --> 00:04:23.080
trying to find a solution for multi-core Python really since 2014.

00:04:23.080 --> 00:04:23.440
>> Yeah.

00:04:23.440 --> 00:04:28.960
>> So I've been ever so slowly working towards that goal,

00:04:28.960 --> 00:04:31.000
and we've made it with 3.12,

00:04:31.000 --> 00:04:32.080
and there's more work to do.

00:04:32.080 --> 00:04:34.840
But that's a lot of the stuff that I've been working on.

00:04:34.840 --> 00:04:36.440
I'm at Microsoft,

00:04:36.440 --> 00:04:39.120
but don't work with Anthony a whole lot.

00:04:39.120 --> 00:04:43.440
I work on the Python performance team with Guido,

00:04:43.440 --> 00:04:46.640
and Brent Booger, and Mark Shannon here at

00:04:46.640 --> 00:04:50.400
CutTrail and we're just working generally to make Python faster.

00:04:50.400 --> 00:04:53.240
So my part of that has involved subinterpreters.

00:04:53.240 --> 00:04:53.640
>> Awesome.

00:04:53.640 --> 00:04:57.720
>> Interestingly enough, it's only really this year

00:04:57.720 --> 00:05:02.120
that I've been able to work on all the subinterpreter stuff full-time.

00:05:02.120 --> 00:05:04.600
Before that, I was working mostly on other stuff.

00:05:04.600 --> 00:05:07.760
So this year has been a good year for me.

00:05:07.760 --> 00:05:10.960
>> Yeah, I would say that must be really exciting to get the,

00:05:10.960 --> 00:05:12.960
"You know what, why don't you just do that?

00:05:12.960 --> 00:05:14.040
That'd be awesome for us."

00:05:14.040 --> 00:05:15.280
>> Yeah, it's been awesome.

00:05:15.280 --> 00:05:17.920
>> Well, maybe since you're on the team,

00:05:17.920 --> 00:05:20.440
let's do a quick check-in on Faster CPython.

00:05:20.440 --> 00:05:23.560
It's made a mega difference over the last couple of releases.

00:05:23.560 --> 00:05:25.720
>> Yeah, it's interesting.

00:05:25.720 --> 00:05:28.480
Mark Shannon definitely has a vision.

00:05:28.480 --> 00:05:32.080
He's developed a plan as of years ago,

00:05:32.080 --> 00:05:37.600
but we finally were able to put him in a position where he could do something about it,

00:05:37.600 --> 00:05:40.280
and we've all been pitching in.

00:05:40.280 --> 00:05:43.520
A lot of it has to do with just applying some of

00:05:43.520 --> 00:05:48.040
the general ideas that are out there regarding dynamic languages and optimization.

00:05:48.040 --> 00:05:50.800
Things have been applied to other things like

00:05:50.800 --> 00:05:55.600
HHVM or various of the JavaScript runtimes.

00:05:55.600 --> 00:06:00.480
A lot of adaptive specialization,

00:06:00.480 --> 00:06:03.480
a few other techniques.

00:06:04.000 --> 00:06:09.280
A lot of that stuff we're able to get in for 3.11.

00:06:09.280 --> 00:06:13.880
In 3.12, there wasn't quite as much impactful stuff.

00:06:13.880 --> 00:06:18.840
It was we're gearing up to effectively add a JIT into CPython,

00:06:18.840 --> 00:06:25.840
and that's required a lot of behind-the-scenes work to get things in the right places.

00:06:25.840 --> 00:06:30.160
We're somewhat targeting 3.13 for that.

00:06:30.160 --> 00:06:34.400
Right now, I think where things are at,

00:06:34.400 --> 00:06:37.120
we're break-even performance-wise,

00:06:37.120 --> 00:06:40.360
but there's a lot of stuff that we can do.

00:06:40.360 --> 00:06:44.280
A lot of optimization work that really hasn't even been done yet that

00:06:44.280 --> 00:06:48.440
will take that performance improvement up pretty drastically.

00:06:48.440 --> 00:06:50.880
It's hard to say where we're going to be,

00:06:50.880 --> 00:06:54.080
but for 3.13, it's looking pretty good for

00:06:54.080 --> 00:07:00.040
at least some performance improvement because of the JITing and optimization work.

00:07:00.040 --> 00:07:04.240
>> That's exciting. We have no real JIT at the moment, right?

00:07:04.240 --> 00:07:05.480
>> Not in CPython.

00:07:05.480 --> 00:07:08.360
>> Yeah. I mean, I know there's Numba and different things.

00:07:08.360 --> 00:07:09.600
>> Yeah, it is best.

00:07:09.600 --> 00:07:12.720
>> I know. Well, that's actually super exciting

00:07:12.720 --> 00:07:16.240
because I feel like that could be another big boost potentially.

00:07:16.240 --> 00:07:19.120
With the JIT, you come to all sorts of things like inlining of

00:07:19.120 --> 00:07:23.760
small methods and optimization based on type information.

00:07:23.760 --> 00:07:25.000
>> Yeah. All that stuff.

00:07:25.000 --> 00:07:29.120
One of the most exciting parts for me is that a lot of this work,

00:07:29.120 --> 00:07:31.400
not long after I joined the team,

00:07:31.400 --> 00:07:35.640
so two and a half years ago,

00:07:35.640 --> 00:07:37.400
somewhere in there, pretty early on,

00:07:37.400 --> 00:07:40.320
we started reaching out to other folks,

00:07:40.320 --> 00:07:42.440
other projects that were interested in

00:07:42.440 --> 00:07:46.320
performance and performance of Python code.

00:07:46.320 --> 00:07:50.040
We've worked pretty hard to collaborate with them.

00:07:50.040 --> 00:07:53.040
Like the team over at Meta,

00:07:53.040 --> 00:07:58.760
they have a lot of interest in making sure Python is very efficient.

00:07:58.760 --> 00:08:01.800
We've actually worked pretty closely with them,

00:08:01.800 --> 00:08:05.200
and they're able to take advantage of a lot of the work that we've done,

00:08:05.200 --> 00:08:05.880
which is great.

00:08:05.880 --> 00:08:07.840
>> Yeah. There seems to be some synergy between

00:08:07.840 --> 00:08:10.920
the Cinder team and the faster CPython team.

00:08:10.920 --> 00:08:15.560
Awesome. But let's focus on a part that is there,

00:08:15.560 --> 00:08:18.680
but not really utilized very much yet,

00:08:18.680 --> 00:08:20.880
which is the sub-interpreter.

00:08:20.880 --> 00:08:24.160
Back on, when is this, 2019?

00:08:24.160 --> 00:08:26.320
Eric, I had you on and we talked about,

00:08:26.320 --> 00:08:29.920
can sub-interpreters free us from Python's gill?

00:08:29.920 --> 00:08:33.720
Then since then, this has been accepted,

00:08:33.720 --> 00:08:36.160
but it's Anthony's fault that we're here.

00:08:36.160 --> 00:08:38.640
Because Anthony posted over on Mastodon,

00:08:38.640 --> 00:08:39.760
"Hey, here's a new blog post,

00:08:39.760 --> 00:08:43.080
me running Python parallel applications with sub-interpreters.

00:08:43.080 --> 00:08:48.680
How about we use Flask and FastAPI and sub-interpreters and make that go fast?"

00:08:48.680 --> 00:08:53.320
That sounded more available in the Python level

00:08:53.320 --> 00:08:56.560
than I realized the sub-interpreter stuff was.

00:08:56.560 --> 00:08:58.560
That's super exciting, both of you.

00:08:58.560 --> 00:09:00.960
>> Yeah. It's been fun to play with it and

00:09:00.960 --> 00:09:03.640
try and build applications on it and stuff like that.

00:09:03.640 --> 00:09:06.760
Working with Eric probably over the last couple of months

00:09:06.760 --> 00:09:10.720
on things that we've discovered in that process.

00:09:10.720 --> 00:09:13.160
Especially with C extensions.

00:09:13.160 --> 00:09:14.040
>> Daytime?

00:09:14.040 --> 00:09:16.760
>> Yeah, that's one.

00:09:16.760 --> 00:09:20.560
With C extensions, and I think that some of those challenges are

00:09:20.560 --> 00:09:24.840
going to be the same with free threading as well.

00:09:24.840 --> 00:09:28.880
It's how C extensions have state where they put it,

00:09:28.880 --> 00:09:31.160
whether that's thread safe.

00:09:31.160 --> 00:09:36.560
As soon as you open up the possibility of having multiple gills in one process,

00:09:36.560 --> 00:09:40.560
then what challenges does that create?

00:09:40.560 --> 00:09:43.960
>> Absolutely. Well, I guess maybe some nomenclature first,

00:09:43.960 --> 00:09:48.440
not no-gil Python or sub-interpreter free-threaded.

00:09:48.440 --> 00:09:50.320
Is that what we're calling it? What's the name?

00:09:50.320 --> 00:09:52.640
How do we speak about this?

00:09:52.640 --> 00:09:54.120
>> It's not quite settled,

00:09:54.120 --> 00:09:59.080
but I think a lot of people have taken to referring to it as free-threaded.

00:09:59.080 --> 00:10:00.400
>> I can go with that. It sounds pretty good.

00:10:00.400 --> 00:10:01.600
>> People still talk about no-gil,

00:10:01.600 --> 00:10:04.560
but free-threaded is probably the best bet.

00:10:04.560 --> 00:10:07.760
>> Are you describing what it does and why you care,

00:10:07.760 --> 00:10:09.440
or are you describing the implementation?

00:10:09.440 --> 00:10:11.560
The implementation is it has no-gil,

00:10:11.560 --> 00:10:12.680
so it can be free-threaded,

00:10:12.680 --> 00:10:13.920
or it has sub-interpreters,

00:10:13.920 --> 00:10:15.080
so it can be free-threaded.

00:10:15.080 --> 00:10:17.240
But really what you want is the free-threaded part.

00:10:17.240 --> 00:10:20.080
You don't care actually about the GIL too much.

00:10:20.080 --> 00:10:22.760
>> It's interesting with sub-interpreters,

00:10:22.760 --> 00:10:26.440
it really isn't necessarily a free-threaded model.

00:10:26.440 --> 00:10:34.040
It's free-threaded only in the part at which you're moving between interpreters.

00:10:34.040 --> 00:10:37.600
You only have to care about it when you're interacting between interpreters.

00:10:37.600 --> 00:10:39.480
The rest of the time, you don't have to worry about it.

00:10:39.480 --> 00:10:43.120
>> I think that with the no-gil,

00:10:43.120 --> 00:10:46.400
what we think of as free-threading work,

00:10:46.400 --> 00:10:48.400
everything is unsafe.

00:10:48.400 --> 00:10:50.600
>> For people who don't know, the no-gil stuff is what's coming out of

00:10:50.600 --> 00:10:52.360
the Sender team and from Sam Gross.

00:10:52.360 --> 00:10:54.040
That was also approved, but with

00:10:54.040 --> 00:10:56.720
the biggest caveat I've ever seen on an approved cup.

00:10:56.720 --> 00:10:57.520
>> Yeah.

00:10:57.520 --> 00:11:00.320
>> We approve this, we also reserve the right to

00:11:00.320 --> 00:11:02.800
completely undo it and not approve it anymore.

00:11:02.800 --> 00:11:07.760
But it's also a compiler flag that is an optional off by default situation.

00:11:07.760 --> 00:11:08.760
Should be interesting.

00:11:08.760 --> 00:11:12.040
>> Yeah, we can maybe compare and contrast them a bit later as well.

00:11:12.040 --> 00:11:16.560
>> Yeah, absolutely. Well, let's start with what is an interpreter?

00:11:16.560 --> 00:11:20.760
Then how do we get to sub-interpreters and then what work did you have to do?

00:11:20.760 --> 00:11:23.320
I heard there was a few global variables that are being shared.

00:11:23.320 --> 00:11:24.640
>> Yeah.

00:11:24.640 --> 00:11:29.000
>> Maybe let's give people a quick rundown of what is this and how is it,

00:11:29.000 --> 00:11:31.400
this new feature in 3.12 changing things.

00:11:31.400 --> 00:11:35.120
>> Yeah. Sub-interpreters in a Python process,

00:11:35.120 --> 00:11:37.000
when you run Python,

00:11:37.000 --> 00:11:38.920
all of everything that happens,

00:11:38.920 --> 00:11:42.200
all the machinery that's running your Python code

00:11:42.200 --> 00:11:45.880
is running with a certain amount of global state.

00:11:45.880 --> 00:11:50.760
Historically, you can think of it as across the whole process.

00:11:50.760 --> 00:11:52.480
You've got a bunch of global state.

00:11:52.480 --> 00:11:55.800
If you look at all the stuff like in the sys module,

00:11:55.800 --> 00:11:58.880
sys.modules or sys.whatever,

00:11:58.880 --> 00:12:03.160
all those things are shared across the whole runtime.

00:12:03.160 --> 00:12:05.920
If you have different threads, for instance, running it,

00:12:05.920 --> 00:12:07.760
they all share that stuff even though

00:12:07.760 --> 00:12:10.480
you're going to have different code running in each thread.

00:12:10.480 --> 00:12:16.080
All of that runtime state is everything that Python needs in order to run.

00:12:16.080 --> 00:12:19.240
But what's interesting is that the vast majority of it,

00:12:19.240 --> 00:12:23.840
you can think of as the actual interpreter.

00:12:23.840 --> 00:12:28.920
That state, if we treat it as isolated and we're very careful about it,

00:12:28.920 --> 00:12:30.560
then we can have multiple of them.

00:12:30.560 --> 00:12:33.200
That means that when your Python code runs,

00:12:33.200 --> 00:12:37.200
that it can run with a different set of this global state,

00:12:37.200 --> 00:12:38.880
different modules imported,

00:12:38.880 --> 00:12:40.880
different things going on,

00:12:40.880 --> 00:12:45.880
different threads that are unrelated and really don't affect each other at all.

00:12:45.880 --> 00:12:48.880
Then with that in mind,

00:12:48.880 --> 00:12:50.840
you can take it one step farther and say,

00:12:50.840 --> 00:12:55.880
well, let's completely isolate those and not even have them share a gill.

00:12:55.880 --> 00:12:59.520
Then at that point, that's where the magic happens.

00:12:59.520 --> 00:13:05.080
That's my first goal in this whole project was to get to that point.

00:13:05.080 --> 00:13:06.680
Because once you get there,

00:13:06.680 --> 00:13:12.520
then it opens up a lot of possibilities when it comes to concurrency and parallelism.

00:13:12.520 --> 00:13:16.760
>> Then Anthony can start running with his blog posts and showing off things.

00:13:16.760 --> 00:13:17.240
>> Yeah.

00:13:17.240 --> 00:13:21.280
>> Absolutely. This portion of

00:13:21.280 --> 00:13:27.120
Talk Python to Me is brought to you by the PyBytes Python Developer Mindset Program.

00:13:27.120 --> 00:13:30.120
It's run by my two friends and frequent guests,

00:13:30.120 --> 00:13:32.160
Bob Delderbos and Julian Sequeira.

00:13:32.160 --> 00:13:33.960
Instead of me telling you about it,

00:13:33.960 --> 00:13:36.160
let's hear them describe their program.

00:13:36.160 --> 00:13:38.040
>> In a world where AI,

00:13:38.040 --> 00:13:44.200
machine learning, and large language models are revolutionizing how we live and work,

00:13:44.200 --> 00:13:46.560
Python stands at the forefront.

00:13:46.560 --> 00:13:50.880
Don't get left behind in this technological evolution.

00:13:50.880 --> 00:13:54.240
>> Tutorial paralysis, that's a thing of the past.

00:13:54.240 --> 00:13:57.880
With PyBytes coaching, you move beyond endless tutorials to

00:13:57.880 --> 00:14:00.840
become an efficient, skilled Python developer.

00:14:00.840 --> 00:14:05.880
We focus on practical real-world skills that prepare you for the future of tech.

00:14:05.880 --> 00:14:11.040
>> Join us at PyBytes and step into a world where Python isn't just a language,

00:14:11.040 --> 00:14:15.600
but a key to unlocking endless possibilities in the tech landscape.

00:14:15.600 --> 00:14:20.960
Check out our 12-week PDM program and embark on a journey to Python mastery.

00:14:20.960 --> 00:14:25.360
The future is Python, and with PyBytes, you're one step ahead.

00:14:25.360 --> 00:14:28.680
>> Apply for the Python Developer Mindset today.

00:14:28.680 --> 00:14:31.120
It's quick and free to apply.

00:14:31.120 --> 00:14:33.520
The link is in your podcast player show notes.

00:14:33.520 --> 00:14:36.560
Thanks to PyBytes for sponsoring the show.

00:14:36.560 --> 00:14:39.120
One thing I don't know the answer to,

00:14:39.120 --> 00:14:42.280
but might be interesting is Python has

00:14:42.280 --> 00:14:46.040
a memory management story in front of the operating system,

00:14:46.040 --> 00:14:48.840
virtual memory that's assigned to the process with pools,

00:14:48.840 --> 00:14:51.440
arenas, blocks, those kinds of things.

00:14:51.440 --> 00:14:54.400
What's that look like with regard to sub-interpreters?

00:14:54.400 --> 00:14:59.800
So each sub-interpreter have its own chunk or set of those for the memory it allocates,

00:14:59.800 --> 00:15:03.000
or is it still a shared one thing per process?

00:15:03.000 --> 00:15:05.200
>> It's per interpreter.

00:15:05.200 --> 00:15:08.160
This is something that was very global.

00:15:08.160 --> 00:15:10.080
Like you pointed out earlier,

00:15:10.080 --> 00:15:13.280
this whole project was all about taking all sorts of

00:15:13.280 --> 00:15:18.120
global state that was actually stored in C global variables all over the place,

00:15:18.120 --> 00:15:22.360
and pulling those in together into one place and moving

00:15:22.360 --> 00:15:28.280
those down from the process global state down into each interpreter.

00:15:28.280 --> 00:15:34.920
So one of those things was all of the allocator state that we have for objects.

00:15:34.920 --> 00:15:39.360
Python has this idea of different levels of allocators.

00:15:39.360 --> 00:15:43.440
The object allocator is what's used heavily for Python objects,

00:15:43.440 --> 00:15:45.920
of course, but some other state as well.

00:15:45.920 --> 00:15:52.160
The object allocator is the part that has all the arenas and everything like you're saying.

00:15:52.160 --> 00:15:52.840
>> Yeah.

00:15:52.840 --> 00:15:56.720
>> So part of what I did before we could make the GIL per interpreter,

00:15:56.720 --> 00:16:00.160
we had to make the allocator state per interpreter.

00:16:00.160 --> 00:16:03.120
>> Well, the reason I think that it's interesting asking about it,

00:16:03.120 --> 00:16:05.040
it's one because of the GIL obviously,

00:16:05.040 --> 00:16:06.160
but the other one is,

00:16:06.160 --> 00:16:09.200
it seems to me like these sub-interpreters could be used for

00:16:09.200 --> 00:16:13.760
a little bit of stability or isolation to run some code.

00:16:13.760 --> 00:16:15.560
When that line exits,

00:16:15.560 --> 00:16:16.920
I want the memory free,

00:16:16.920 --> 00:16:18.440
I want modules unloaded,

00:16:18.440 --> 00:16:20.240
I want it to go back to the way it was.

00:16:20.240 --> 00:16:23.400
You know what I mean? Whereas normally in Python,

00:16:23.400 --> 00:16:24.960
even if the memory becomes free,

00:16:24.960 --> 00:16:26.280
it's still got some of that like, well,

00:16:26.280 --> 00:16:28.800
we allocate this stuff, now we're holding it to refill it,

00:16:28.800 --> 00:16:31.560
and then you don't un-import modules.

00:16:31.560 --> 00:16:34.400
But modules can be pretty intense actually,

00:16:34.400 --> 00:16:38.080
if they start allocating a bunch of stuff themselves and so on.

00:16:38.080 --> 00:16:41.160
What do you guys think about this as an idea, as an aspect of it?

00:16:41.160 --> 00:16:44.600
>> Yeah, there's one example I've been coming across recently,

00:16:44.600 --> 00:16:46.640
and this is a pattern.

00:16:46.640 --> 00:16:48.280
I think it's a bit of an anti-pattern actually,

00:16:48.280 --> 00:16:51.480
but some Python packages,

00:16:51.480 --> 00:16:56.480
they store some state information in the module level.

00:16:56.480 --> 00:17:00.600
So an example is a SDK that I've been working with,

00:17:00.600 --> 00:17:03.520
which has just been rewritten to stop it from doing this.

00:17:03.520 --> 00:17:07.680
But you would put the API key of the SDK,

00:17:07.680 --> 00:17:09.920
you would import it, so you'd import X,

00:17:09.920 --> 00:17:13.200
and then do like X.APIKey equals.

00:17:13.200 --> 00:17:18.080
So it basically stores the API key in the module object,

00:17:18.080 --> 00:17:24.800
which is fine if you've imported the module once and you're using it once.

00:17:24.800 --> 00:17:29.360
But what you see is that if you put that in a web application,

00:17:29.360 --> 00:17:32.000
it just assumes that everyone uses the same key.

00:17:32.000 --> 00:17:36.200
So you can't import that module,

00:17:36.200 --> 00:17:38.880
and then connect to it with different API keys,

00:17:38.880 --> 00:17:40.840
like you'd have different users or something.

00:17:40.840 --> 00:17:43.520
>> So you've got some kind of multi-tenancy,

00:17:43.520 --> 00:17:45.760
where they would say,

00:17:45.760 --> 00:17:47.880
enter their ChatGPT,

00:17:47.880 --> 00:17:51.000
open AI key, and then they could work on behalf of that.

00:17:51.000 --> 00:17:52.840
That potentially something like that, right?

00:17:52.840 --> 00:17:55.680
>> Yeah, exactly. So that's like an API example,

00:17:55.680 --> 00:17:57.280
but there are other examples where,

00:17:57.280 --> 00:17:59.480
let's say you're loading data or something,

00:17:59.480 --> 00:18:04.480
and it stores some temporary information somewhere in a class attribute,

00:18:04.480 --> 00:18:06.840
or even like a module attribute like that.

00:18:06.840 --> 00:18:10.640
Then if you've got one piece of code loading data,

00:18:10.640 --> 00:18:13.400
and then in another thread in a web app,

00:18:13.400 --> 00:18:15.200
or just in another thread generally,

00:18:15.200 --> 00:18:19.080
you're reading another piece of data and they're sharing state somehow,

00:18:19.080 --> 00:18:20.640
and you've got no isolation.

00:18:20.640 --> 00:18:25.040
Some of that is due to the way that people have written the Python code,

00:18:25.040 --> 00:18:29.800
or the extension code, has been built around,

00:18:29.800 --> 00:18:31.800
oh, we'll just put this information here,

00:18:31.800 --> 00:18:34.080
and they haven't really thought about the isolation.

00:18:34.080 --> 00:18:37.680
Sometimes it's because on the C level especially,

00:18:37.680 --> 00:18:40.480
that because the GIL was always there,

00:18:40.480 --> 00:18:42.280
they've never had to worry about it.

00:18:42.280 --> 00:18:45.920
So you could just have a counter for example,

00:18:45.920 --> 00:18:51.200
or there's an object which is a dictionary that is a cache of something.

00:18:51.200 --> 00:18:54.560
You just put that as a static variable,

00:18:54.560 --> 00:18:56.040
and you just read and write from it.

00:18:56.040 --> 00:18:57.880
You've never had to worry about thread safety

00:18:57.880 --> 00:19:00.200
because the GIL was there to protect you.

00:19:00.200 --> 00:19:02.000
You probably shouldn't have built it that way,

00:19:02.000 --> 00:19:05.040
but it didn't really matter because it worked.

00:19:05.040 --> 00:19:06.640
>> What about this, Anthony?

00:19:06.640 --> 00:19:08.960
What if we can write it on one line,

00:19:08.960 --> 00:19:10.040
it'll probably be safe.

00:19:10.040 --> 00:19:13.360
If we can fit it just one line of Python code, it'll be okay?

00:19:13.360 --> 00:19:15.120
>> Yeah.

00:19:15.120 --> 00:19:17.440
>> Dictionary.add, what's wrong there?

00:19:17.440 --> 00:19:19.040
Dictionary.get, it's fine.

00:19:19.040 --> 00:19:21.000
>> Yeah. So yeah, what we're saying,

00:19:21.000 --> 00:19:26.240
within sub-interpreters, I think what's the concept that people will need to

00:19:26.240 --> 00:19:29.800
understand is where the isolation is,

00:19:29.800 --> 00:19:33.280
because there are different models for running parallel code.

00:19:33.280 --> 00:19:36.520
At the moment, we've got coroutines,

00:19:36.520 --> 00:19:40.680
which is asynchronous, so it can run concurrently.

00:19:40.680 --> 00:19:45.000
So that's if you do async await or if you use the old coroutine decorator.

00:19:45.000 --> 00:19:47.040
You've also got things like generators,

00:19:47.040 --> 00:19:50.400
which are concurrent pattern.

00:19:50.400 --> 00:19:53.280
You've got threads that you can create.

00:19:53.280 --> 00:19:57.200
All of those live within the same interpreter,

00:19:57.200 --> 00:19:59.040
and they share the same information.

00:19:59.040 --> 00:20:02.400
So if you create a thread,

00:20:02.400 --> 00:20:06.720
inside that thread, you can read a variable from outside of that thread,

00:20:06.720 --> 00:20:08.480
and it doesn't complain.

00:20:08.480 --> 00:20:11.040
You don't need to create a lock at the moment,

00:20:11.040 --> 00:20:14.520
although in some situations, you probably should.

00:20:14.520 --> 00:20:18.360
You don't need to re-import modules and stuff like that,

00:20:18.360 --> 00:20:20.520
which can be fine.

00:20:20.520 --> 00:20:21.800
Then at the other extreme,

00:20:21.800 --> 00:20:23.200
you've got multiprocessing,

00:20:23.200 --> 00:20:26.760
which is a module in the standard library that allows you to

00:20:26.760 --> 00:20:29.360
create extra Python processes,

00:20:29.360 --> 00:20:33.200
and then gives you an API to talk to them and share information between them.

00:20:33.200 --> 00:20:35.400
That's the other extreme,

00:20:35.400 --> 00:20:40.080
which is the ultimate level of isolation.

00:20:40.080 --> 00:20:42.760
You've got a whole separate Python process.

00:20:42.760 --> 00:20:45.600
But instead of interacting with it via the command line,

00:20:45.600 --> 00:20:49.000
you've got this nicer API where you can

00:20:49.000 --> 00:20:53.440
almost treat it like it's in the same process as the one you're running from.

00:20:53.440 --> 00:20:56.200
>> It's kind of magical actually that you get

00:20:56.200 --> 00:20:59.240
a return value from a process, for example.

00:20:59.240 --> 00:21:02.640
>> But the thing is, if you pull back the covers a little bit,

00:21:02.640 --> 00:21:08.880
then how it sends information to the other Python process involves a lot of pickles,

00:21:08.880 --> 00:21:11.640
and it's not particularly efficient.

00:21:11.640 --> 00:21:14.560
Also, a Python process has a lot of

00:21:14.560 --> 00:21:18.400
extra stuff that you maybe necessarily didn't even need.

00:21:18.400 --> 00:21:20.680
You get all this isolation from having it,

00:21:20.680 --> 00:21:22.560
but you have to import all the modules again,

00:21:22.560 --> 00:21:25.240
you have to create the arenas again,

00:21:25.240 --> 00:21:26.360
all the memory allocation,

00:21:26.360 --> 00:21:28.600
you have to do all the startup process again,

00:21:28.600 --> 00:21:29.880
which takes a lot of times,

00:21:29.880 --> 00:21:31.200
like at least 200 milliseconds.

00:21:31.200 --> 00:21:32.600
>> Parsing the Python code again,

00:21:32.600 --> 00:21:34.080
at least the PYC.

00:21:34.080 --> 00:21:39.040
>> Yeah, exactly. You basically created a whole separate Python.

00:21:39.040 --> 00:21:42.840
If you do that just to run a small chunk of code,

00:21:42.840 --> 00:21:45.920
then it's not probably the best model at all.

00:21:45.920 --> 00:21:49.120
>> Yeah. You have a nice graph as well that shows

00:21:49.120 --> 00:21:52.880
the rate as you add more work and you need more parallelism.

00:21:52.880 --> 00:21:54.840
We'll get to that, I'm sure.

00:21:54.840 --> 00:22:00.360
One thing that struck me coming to Python from other languages like C, C++, C#,

00:22:00.360 --> 00:22:04.160
there's very little locks and events,

00:22:04.160 --> 00:22:06.800
threading, coordinating stuff in Python.

00:22:06.800 --> 00:22:13.200
I think that there's probably a ton of Python code that actually is not actually thread safe,

00:22:13.200 --> 00:22:17.920
but people get away with it because the context switching is so coarse grained.

00:22:17.920 --> 00:22:19.760
You say, well, the gills there,

00:22:19.760 --> 00:22:21.400
so you only run one instruction at a time,

00:22:21.400 --> 00:22:26.120
but this temporary invalid state you entered to as part of your code running,

00:22:26.120 --> 00:22:29.440
took money out of this account and then I'm going to put it into that account.

00:22:29.440 --> 00:22:32.360
Those are multiple Python lines and there's nothing saying they

00:22:32.360 --> 00:22:36.080
couldn't get interrupted between one to the other and then things are busted.

00:22:36.080 --> 00:22:39.960
I feel there's some concern about adding this concurrency,

00:22:39.960 --> 00:22:41.440
like, "Oh, we're going to have to worry about it."

00:22:41.440 --> 00:22:43.160
You probably should be worrying about it now.

00:22:43.160 --> 00:22:44.680
Not as much necessarily,

00:22:44.680 --> 00:22:48.680
but I feel like people are getting away with it because it's so rare,

00:22:48.680 --> 00:22:52.000
but it's a non-zero possibility. What do you guys think?

00:22:52.000 --> 00:22:55.040
>> Yeah. Those are real concerns.

00:22:55.040 --> 00:23:01.240
There's been lots of discussion with the no-gil work about really what matters,

00:23:01.240 --> 00:23:03.360
what we need to care about,

00:23:03.360 --> 00:23:06.240
really what impact it's going to have.

00:23:06.240 --> 00:23:11.480
It's probably going to have some impact on people with Python code,

00:23:11.480 --> 00:23:16.200
but it'll especially have impact on people that maintain extension modules.

00:23:16.200 --> 00:23:22.400
But it really is all the pain that comes with free threading.

00:23:22.400 --> 00:23:28.560
That's what it introduces with the benefits as well, of course.

00:23:28.560 --> 00:23:31.880
But what's interesting, I'd like to think of

00:23:31.880 --> 00:23:35.640
sub-interpreters provide the same facility,

00:23:35.640 --> 00:23:40.160
but they force you to be explicit about what gets shared,

00:23:40.160 --> 00:23:43.320
and they force you to do it in a thread-safe way.

00:23:43.320 --> 00:23:46.160
You can't do it without thread safety,

00:23:46.160 --> 00:23:48.840
and so it's not an issue.

00:23:48.840 --> 00:23:52.160
It doesn't hurt that people really haven't

00:23:52.160 --> 00:23:55.440
used sub-interpreters extensively up till now,

00:23:55.440 --> 00:23:58.720
whereas threads are something that's been around for quite a while.

00:23:58.720 --> 00:23:59.920
>> Yeah, it has been.

00:23:59.920 --> 00:24:05.040
Well, sub-interpreters have traditionally just been a thing you can do from C extensions,

00:24:05.040 --> 00:24:06.160
or the C API,

00:24:06.160 --> 00:24:09.840
which really limits them from being used in just a standard,

00:24:09.840 --> 00:24:11.400
I'm working on my web app,

00:24:11.400 --> 00:24:14.360
so let's just throw in a couple of sub-interpreters.

00:24:14.360 --> 00:24:20.760
3.13, is that when we're looking at having a Python-level API for creating and interacting with?

00:24:20.760 --> 00:24:23.280
>> Yeah, I've been working on a PEP for that,

00:24:23.280 --> 00:24:28.280
 PEP 554, recently created a new PEP to replace that one,

00:24:28.280 --> 00:24:31.040
which is PEP 734.

00:24:31.040 --> 00:24:36.040
That's the one. That's the one that I'm targeting for 3.13.

00:24:36.040 --> 00:24:38.200
It's pretty straightforward,

00:24:38.200 --> 00:24:43.960
create interpreters and look at them and with an interpreter,

00:24:43.960 --> 00:24:47.120
run some code, pretty basic stuff.

00:24:47.120 --> 00:24:50.640
Then also, because sub-interpreters aren't quite so useful,

00:24:50.640 --> 00:24:52.720
if you can't cooperate between them,

00:24:52.720 --> 00:25:00.960
there's also a queue type that you push stuff on and you pop stuff off and just pretty basic.

00:25:00.960 --> 00:25:02.680
>> So you could write something like a weight,

00:25:02.680 --> 00:25:06.360
q.popper or something like that. Excellent.

00:25:06.360 --> 00:25:06.760
>> Yeah.

00:25:06.760 --> 00:25:08.240
>> Yeah, this is really cool.

00:25:08.240 --> 00:25:10.880
The other thing that I wanted to talk about here,

00:25:10.880 --> 00:25:13.600
looks like you already have it in the pep, which is excellent.

00:25:13.600 --> 00:25:14.880
Somehow I missed that before,

00:25:14.880 --> 00:25:18.000
is that we have thread pool executors,

00:25:18.000 --> 00:25:20.640
we have multi-processing pool executors,

00:25:20.640 --> 00:25:23.600
and this would be an interpreter pool executor.

00:25:23.600 --> 00:25:25.120
What's the thinking there?

00:25:25.120 --> 00:25:28.440
>> People are already familiar with using concurrent futures.

00:25:28.440 --> 00:25:33.120
So if we can present the same API for sub-interpreters,

00:25:33.120 --> 00:25:36.360
it makes it really easy because you can set it up with

00:25:36.360 --> 00:25:39.000
multi-processing or threads and switch it

00:25:39.000 --> 00:25:41.760
over to one of the other pool types without a lot of fuss.

00:25:41.760 --> 00:25:44.440
>> Right. Basically, with a clever import statement,

00:25:44.440 --> 00:25:47.120
you could take it right from whatever import,

00:25:47.120 --> 00:25:50.400
like multi-processing pool executor as pool executor or

00:25:50.400 --> 00:25:52.440
interpreter pool executor as pool executor and

00:25:52.440 --> 00:25:54.640
then the rest of the code could stay potentially.

00:25:54.640 --> 00:25:55.120
>> Yeah.

00:25:55.120 --> 00:25:57.160
>> What about the communication?

00:25:57.160 --> 00:26:01.320
It's got to be a basic situation because there are assumptions.

00:26:01.320 --> 00:26:05.120
>> Yeah. It should work mostly the same way that you already

00:26:05.120 --> 00:26:09.040
use it with threads and multi-processing.

00:26:09.040 --> 00:26:11.960
But we'll see. There's some limitations with

00:26:11.960 --> 00:26:14.400
sub-interpreters currently that I'm sure

00:26:14.400 --> 00:26:17.600
we'll work on solving as we can.

00:26:17.600 --> 00:26:19.280
So we'll see.

00:26:19.280 --> 00:26:21.520
It may not be quite as efficient as I'd like

00:26:21.520 --> 00:26:24.040
at first with the interpreter pool executor,

00:26:24.040 --> 00:26:25.760
because we'll probably end up doing

00:26:25.760 --> 00:26:28.880
some pickling stuff like multi-processing does.

00:26:28.880 --> 00:26:31.840
Although I expect it'll be a little more efficient.

00:26:31.840 --> 00:26:35.320
>> This portion of Talk Python to me is brought to you by Sentry.

00:26:35.320 --> 00:26:37.600
You know Sentry for their air monitoring service,

00:26:37.600 --> 00:26:39.520
the one that we use right here at Talk Python.

00:26:39.520 --> 00:26:43.280
But this time, I want to tell you about a new and free workshop.

00:26:43.280 --> 00:26:47.600
Haming the Kraken, managing a Python monorepo with Sentry.

00:26:47.600 --> 00:26:49.800
Join Salma Alam Nayyar,

00:26:49.800 --> 00:26:51.840
Senior Developer Advocate at Sentry,

00:26:51.840 --> 00:26:53.120
and David Winterbottom,

00:26:53.120 --> 00:26:55.360
Head of Engineering at Kraken Technologies,

00:26:55.360 --> 00:26:58.280
for an inside look into how he and his team

00:26:58.280 --> 00:27:01.720
develop, deploy, and maintain a rapidly evolving

00:27:01.720 --> 00:27:05.240
Python monorepo with over 4 million lines of code

00:27:05.240 --> 00:27:08.200
that powers the Kraken utility platform.

00:27:08.200 --> 00:27:10.480
In this workshop, David will share how his department

00:27:10.480 --> 00:27:15.000
of 500 developers, who deploy around 200 times a day,

00:27:15.000 --> 00:27:17.400
use Sentry to reduce noise, prioritize issues,

00:27:17.400 --> 00:27:19.480
and maintain code quality without relying

00:27:19.480 --> 00:27:21.640
on a dedicated Q&A team.

00:27:21.640 --> 00:27:24.600
You'll learn how to find and fix root causes of crashes,

00:27:24.600 --> 00:27:27.920
ways to prioritize the most urgent crashes and errors,

00:27:27.920 --> 00:27:30.120
and tips to streamline your workflow.

00:27:30.120 --> 00:27:33.480
Join them for free on Tuesday, February 27th, 2024

00:27:33.480 --> 00:27:35.800
at 2 a.m. civic time.

00:27:35.800 --> 00:27:40.000
Just visit talkpython.fm/sentry-monorepo.

00:27:40.000 --> 00:27:42.800
That link is in your podcast player show notes.

00:27:42.800 --> 00:27:45.240
2 a.m. might be a little early here in the U.S.,

00:27:45.240 --> 00:27:47.760
but go ahead and sign up anyway if you're a U.S. listener,

00:27:47.760 --> 00:27:49.200
'cause I'm sure they'll email you

00:27:49.200 --> 00:27:52.320
about a follow-up recording as well.

00:27:52.320 --> 00:27:54.720
Thank you to Sentry for supporting this episode.

00:27:54.720 --> 00:27:57.240
I was gonna save this for later,

00:27:57.240 --> 00:27:59.040
but I think maybe it's worth talking about now.

00:27:59.040 --> 00:28:01.360
So first of all, Anthony, you wrote a lot about,

00:28:01.360 --> 00:28:03.760
and have actually had some recent influence on,

00:28:03.760 --> 00:28:07.160
what you can pass across, say, the starting code

00:28:07.160 --> 00:28:08.480
and then the running interpreter

00:28:08.480 --> 00:28:10.800
that's kind of like the sub-interpreter doing extra work.

00:28:10.800 --> 00:28:13.480
Wanna talk about what data exchange there is?

00:28:13.480 --> 00:28:17.200
- Yeah, so when you're using any of these models,

00:28:17.200 --> 00:28:20.440
multiprocessing, sub-interpreters, or threading,

00:28:20.440 --> 00:28:23.920
I guess you've got three things to worry about.

00:28:23.920 --> 00:28:26.160
One is how do you create it in the first place?

00:28:26.160 --> 00:28:28.440
So how do you create a process?

00:28:28.440 --> 00:28:29.720
How do you create an interpreter?

00:28:29.720 --> 00:28:31.120
How do you create a thread?

00:28:31.120 --> 00:28:33.960
The second thing is how do you send data to it?

00:28:33.960 --> 00:28:35.840
'Cause normally, the reason you've created them

00:28:35.840 --> 00:28:38.520
is 'cause you need it to do some work.

00:28:38.520 --> 00:28:41.880
So you've got the code, which is when you spawn it,

00:28:41.880 --> 00:28:43.040
when you create it.

00:28:43.040 --> 00:28:44.560
The code that you want it to run,

00:28:44.560 --> 00:28:46.360
but that code needs some sort of input,

00:28:46.360 --> 00:28:49.800
and that's probably gonna be Python objects.

00:28:49.800 --> 00:28:52.240
It might be reading files, for example,

00:28:52.240 --> 00:28:53.720
or listening to a network socket,

00:28:53.720 --> 00:28:57.560
so it might be getting its input from somewhere else.

00:28:57.560 --> 00:29:00.600
But typically, you need to give it parameters.

00:29:00.600 --> 00:29:03.400
Now, the way that works in multiprocessing

00:29:03.400 --> 00:29:05.680
is mostly reliant on pickle.

00:29:05.680 --> 00:29:10.640
So if you start a process and you give it some data,

00:29:10.640 --> 00:29:14.280
either as a parameter or you create a queue,

00:29:14.280 --> 00:29:19.160
and you send data down the queue or the pipe, for example,

00:29:19.160 --> 00:29:20.480
it pickles the data.

00:29:20.480 --> 00:29:22.840
So you can put a Python object in.

00:29:22.840 --> 00:29:24.280
It uses the pickle module.

00:29:24.280 --> 00:29:25.760
It converts that into a byte string,

00:29:25.760 --> 00:29:28.120
and then it basically converts the byte string

00:29:28.120 --> 00:29:30.520
on the other end back into objects.

00:29:30.520 --> 00:29:32.200
That's got its limitations

00:29:32.200 --> 00:29:34.160
because not everything can be pickled.

00:29:34.160 --> 00:29:37.600
And also, some objects,

00:29:37.600 --> 00:29:39.640
especially if you've got an object

00:29:39.640 --> 00:29:40.880
which has got objects in it,

00:29:40.880 --> 00:29:43.000
and it's deeply nested,

00:29:43.000 --> 00:29:46.000
or you've got a big, complicated dictionary or something

00:29:46.000 --> 00:29:47.720
that's got all these strange types in it

00:29:47.720 --> 00:29:50.400
which can't necessarily be rehydrated

00:29:50.400 --> 00:29:52.600
from just from a byte string.

00:29:52.600 --> 00:29:55.320
An alternative, actually, I do want to point out,

00:29:55.320 --> 00:29:58.560
'cause for people who come across this issue quite a lot,

00:29:58.560 --> 00:30:02.360
there's another package called dill on PyPI.

00:30:02.360 --> 00:30:06.080
So if you think of pickle, think of dill.

00:30:06.080 --> 00:30:08.320
Dill is very similar to pickle.

00:30:08.320 --> 00:30:10.400
It has the same interface,

00:30:10.400 --> 00:30:13.520
but it can pickle slightly more exotic objects

00:30:13.520 --> 00:30:14.880
than pickle can.

00:30:14.880 --> 00:30:19.240
So often, if you find that you've tried to pickle something,

00:30:19.240 --> 00:30:22.600
you try to share it with a process or a subinterpreter,

00:30:22.600 --> 00:30:25.240
and it comes back and says, "This can't be pickled,"

00:30:25.240 --> 00:30:27.640
you can try dill and see if that works.

00:30:27.640 --> 00:30:31.120
So yeah, that's the typical way of doing it,

00:30:31.120 --> 00:30:33.200
is that you would pickle an object,

00:30:33.200 --> 00:30:34.640
and then on the other end,

00:30:34.640 --> 00:30:38.320
you would basically unpickle it back into another object.

00:30:38.320 --> 00:30:40.640
The downside of that is that it's pretty slow.

00:30:40.640 --> 00:30:44.720
It's equivalent, like if you use the JSON module in Python,

00:30:44.720 --> 00:30:46.360
it's kind of similar, I guess,

00:30:46.360 --> 00:30:48.680
to converting something into JSON

00:30:48.680 --> 00:30:50.480
and then converting it from JSON

00:30:50.480 --> 00:30:52.880
back into a dictionary on the other end.

00:30:52.880 --> 00:30:56.000
Like, it's not a super efficient way of doing it.

00:30:56.000 --> 00:30:58.920
So subinterpreters have another mechanism,

00:30:58.920 --> 00:31:03.080
and I haven't read PEP 734 yet. (laughs)

00:31:03.080 --> 00:31:07.440
So I don't know how much of this is in the new PEP, Eric,

00:31:07.440 --> 00:31:10.040
or if it's in the queue, but there's a--

00:31:10.040 --> 00:31:11.880
- It's much the same.

00:31:11.880 --> 00:31:12.840
- Okay, it's much the same.

00:31:12.840 --> 00:31:16.160
So there's another mechanism with subinterpreters,

00:31:16.160 --> 00:31:18.520
because they share the same process,

00:31:18.520 --> 00:31:22.160
whereas multiprocessing doesn't, they're separate processes.

00:31:22.160 --> 00:31:23.560
Because they share the same process,

00:31:23.560 --> 00:31:27.480
you can basically put some data in a memory space,

00:31:27.480 --> 00:31:30.120
which can be read from a separate interpreter.

00:31:30.120 --> 00:31:33.160
Now you need to be, well, Python needs to be really careful.

00:31:33.160 --> 00:31:35.320
You don't need to worry too much about it,

00:31:35.320 --> 00:31:38.080
'cause that complexity's done for you.

00:31:38.080 --> 00:31:39.640
But there are certain types of objects

00:31:39.640 --> 00:31:41.320
that you can put in as parameters.

00:31:41.320 --> 00:31:44.520
You can send either as startup variables

00:31:44.520 --> 00:31:48.840
for your subinterpreter, or you can send via a pipe,

00:31:48.840 --> 00:31:52.280
basically, backwards and forwards between the interpreters.

00:31:52.280 --> 00:31:54.920
And these are essentially all the immutable types

00:31:54.920 --> 00:31:59.920
for Python, which is string, unicode strings, byte strings,

00:31:59.920 --> 00:32:05.400
bool, none, integer, float, and tuples.

00:32:05.400 --> 00:32:07.720
And you can do tuples of tuples as well.

00:32:07.720 --> 00:32:11.120
- And it seems like the tuple part

00:32:11.120 --> 00:32:13.400
had something that you added recently, right?

00:32:13.400 --> 00:32:16.160
It says, "I implemented tuple sharing just last week."

00:32:16.160 --> 00:32:18.040
- Yeah, that's in now.

00:32:18.040 --> 00:32:20.960
I really wanted to use it, so I thought, well,

00:32:20.960 --> 00:32:23.400
instead of keep, I kept complaining that it wasn't there,

00:32:23.400 --> 00:32:24.960
so I thought instead of complaining,

00:32:24.960 --> 00:32:26.920
I might as well talk to Eric

00:32:26.920 --> 00:32:29.000
and work out how to implement it.

00:32:29.000 --> 00:32:30.040
- Yeah, that's awesome.

00:32:30.040 --> 00:32:31.960
- But yeah, you can't share dictionaries, that's one thing.

00:32:31.960 --> 00:32:32.800
- Yeah, exactly.

00:32:32.800 --> 00:32:34.680
So one thing that I thought that might be awesome,

00:32:34.680 --> 00:32:36.600
are you familiar with message spec?

00:32:36.600 --> 00:32:37.880
You guys seen message spec?

00:32:37.880 --> 00:32:40.040
It's like Pydantic in the sense

00:32:40.040 --> 00:32:42.160
that you create a class with types,

00:32:42.160 --> 00:32:45.720
but the parsing performance is quite a bit,

00:32:45.720 --> 00:32:49.160
like much, much faster, 80 times faster than Pydantic,

00:32:49.160 --> 00:32:53.760
10 times faster than Marshiro and C-Adders and so on,

00:32:53.760 --> 00:32:58.040
and faster still even than say JSON or UJSON.

00:32:58.040 --> 00:33:00.360
So maybe it makes sense to use this,

00:33:00.360 --> 00:33:02.560
turn it into its serialization format bytes,

00:33:02.560 --> 00:33:04.960
send the bytes over and then pull it back, I don't know.

00:33:04.960 --> 00:33:05.800
Might give you a nice structured way.

00:33:05.800 --> 00:33:07.040
- Yeah, you can share byte strings.

00:33:07.040 --> 00:33:09.280
So you can stick something into pickle,

00:33:09.280 --> 00:33:13.400
you can use like msgspec or something like that

00:33:13.400 --> 00:33:15.520
to serialize something into a byte string

00:33:15.520 --> 00:33:18.400
and then receive it on the other end and rehydrate it.

00:33:18.400 --> 00:33:20.640
- Or even Pydantic, like Pydantic is awesome as well.

00:33:20.640 --> 00:33:22.760
Just, this is meant to be super fast

00:33:22.760 --> 00:33:24.800
with a little bit of less behavior, right?

00:33:24.800 --> 00:33:27.040
- Yeah, so this is a kind of a design thing.

00:33:27.040 --> 00:33:29.840
I think people need to consider when they're like,

00:33:29.840 --> 00:33:32.800
great, I can run everything in parallel now.

00:33:32.800 --> 00:33:34.440
But you have to kind of unwind

00:33:34.440 --> 00:33:36.720
and think about how you've designed your application.

00:33:36.720 --> 00:33:41.040
Like at which point do you fork off the work?

00:33:41.040 --> 00:33:44.400
And how do you split the data?

00:33:44.400 --> 00:33:46.480
You can't just kind of go into it assuming,

00:33:46.480 --> 00:33:48.280
oh, we'll just have a pool of workers

00:33:48.280 --> 00:33:52.000
and we've kind of got this shared area of data

00:33:52.000 --> 00:33:53.760
that everybody just reads from.

00:33:53.760 --> 00:33:56.440
- Yeah, I'll pass it a pointer to a million entry list

00:33:56.440 --> 00:33:57.480
and I'll just run with it.

00:33:57.480 --> 00:33:59.200
- Yeah, 'cause I mean, in any language,

00:33:59.200 --> 00:34:02.040
you're gonna get issues if you do that.

00:34:02.040 --> 00:34:03.440
Even if you've got shared memory

00:34:03.440 --> 00:34:06.680
and it's easier to read and write to different spaces,

00:34:06.680 --> 00:34:08.280
you're gonna get issues with locking.

00:34:08.280 --> 00:34:10.480
And I think it's also important with free threading.

00:34:10.480 --> 00:34:12.360
If you read the spec

00:34:12.360 --> 00:34:15.320
or kind of follow what's happening with free threading,

00:34:15.320 --> 00:34:18.280
it's not like the GILs disappeared.

00:34:18.280 --> 00:34:20.560
The GILs been replaced with other locks.

00:34:20.560 --> 00:34:24.040
So there are still going to be locks.

00:34:24.040 --> 00:34:26.200
You can't just have no locks.

00:34:26.200 --> 00:34:27.040
If you've got things running in parallel.

00:34:27.040 --> 00:34:27.880
- Especially cross threads, right?

00:34:27.880 --> 00:34:30.640
Like it moves some of the reference counting stuff

00:34:30.640 --> 00:34:33.680
into like, well, it's fast on the default thread,

00:34:33.680 --> 00:34:35.360
the same thread, but if it goes to another,

00:34:35.360 --> 00:34:37.760
it has to kick in another more thread safe case

00:34:37.760 --> 00:34:39.640
that potentially is slower and so on.

00:34:39.640 --> 00:34:40.480
- Yeah.

00:34:40.480 --> 00:34:42.280
So yeah, the really important thing with sub-interpreters

00:34:42.280 --> 00:34:43.840
is that they have their own,

00:34:43.840 --> 00:34:45.840
well, have their own GIL.

00:34:45.840 --> 00:34:48.600
So each one has its own lock.

00:34:48.600 --> 00:34:50.280
So they can run fully in parallel

00:34:50.280 --> 00:34:52.520
just as they could with multi-processing.

00:34:52.520 --> 00:34:55.160
So I feel like a closer comparison

00:34:55.160 --> 00:34:57.360
with sub-interpreters is multi-processing.

00:34:57.360 --> 00:34:58.320
- Yeah, absolutely.

00:34:58.320 --> 00:35:01.040
- 'Cause they basically run fully in parallel.

00:35:01.040 --> 00:35:03.400
If you start four of them and you have four cores,

00:35:03.400 --> 00:35:05.280
each core is gonna be busy doing work.

00:35:05.280 --> 00:35:08.440
You start them, you give them data,

00:35:08.440 --> 00:35:11.160
you can interact with them whilst they're running.

00:35:11.160 --> 00:35:13.400
And then when they're finished,

00:35:13.400 --> 00:35:17.160
they can close and they can be destroyed and cleaned up.

00:35:17.160 --> 00:35:20.160
So it's much closer to multi-processing,

00:35:20.160 --> 00:35:22.200
but the big, kind of the big difference

00:35:22.200 --> 00:35:25.360
is that the overhead both on the memory

00:35:25.360 --> 00:35:27.920
and CPU side of things is much smaller.

00:35:27.920 --> 00:35:30.560
Separate processes with multi-processing

00:35:30.560 --> 00:35:33.000
are pretty heavyweight, they're big workers.

00:35:33.880 --> 00:35:35.560
And then the other thing that's pretty significant

00:35:35.560 --> 00:35:38.120
is the time it takes to start one.

00:35:38.120 --> 00:35:41.560
So starting a process with multi-processing

00:35:41.560 --> 00:35:44.800
takes quite a lot of time and it's significantly,

00:35:44.800 --> 00:35:47.280
I think it's like 20 or 30 times faster

00:35:47.280 --> 00:35:48.960
to start a sub-interpreter.

00:35:48.960 --> 00:35:51.280
- You have a bunch of graphs for it somewhere.

00:35:51.280 --> 00:35:52.120
There we go.

00:35:52.120 --> 00:35:52.960
- Yeah.

00:35:52.960 --> 00:35:54.560
- So I scrolled past it, there we go.

00:35:54.560 --> 00:35:55.760
It's not exactly the same,

00:35:55.760 --> 00:35:59.000
but kind of captures a lot of it there.

00:35:59.000 --> 00:36:01.200
So one thing that I think is exciting, Eric,

00:36:01.200 --> 00:36:04.880
is the interpreter pool, sub-interpreter pool,

00:36:04.880 --> 00:36:08.240
because a lot of the difference between the threading

00:36:08.240 --> 00:36:10.240
and the sub-interpreter performance

00:36:10.240 --> 00:36:13.360
is that startup of the new arenas

00:36:13.360 --> 00:36:15.040
and like importing the standard library,

00:36:15.040 --> 00:36:17.000
all that kind of stuff that still is gonna happen.

00:36:17.000 --> 00:36:19.960
But once those things are loaded up in the process,

00:36:19.960 --> 00:36:21.960
they could be handed work easily, right?

00:36:21.960 --> 00:36:23.360
And so if you've got a pool of,

00:36:23.360 --> 00:36:24.760
you know, like say that you have 10 cores,

00:36:24.760 --> 00:36:26.200
you've got 10 of them just chilling

00:36:26.200 --> 00:36:27.400
or however many, you know,

00:36:27.400 --> 00:36:29.920
you've sort of done enough work to like do in parallel,

00:36:29.920 --> 00:36:32.160
then you could have them laying around

00:36:32.160 --> 00:36:33.320
and just send like, okay,

00:36:33.320 --> 00:36:34.520
now I want you to run this function.

00:36:34.520 --> 00:36:35.600
And now I want you to run this.

00:36:35.600 --> 00:36:38.280
And that one means go call that API and then process it.

00:36:38.280 --> 00:36:39.920
And I think you could get the difference

00:36:39.920 --> 00:36:42.800
between threading and sub-interpreters a lot lower

00:36:42.800 --> 00:36:45.360
by having them kind of reused basically.

00:36:45.360 --> 00:36:46.640
- Yep, absolutely.

00:36:46.640 --> 00:36:47.760
- Yeah.

00:36:47.760 --> 00:36:50.440
- There's some of the,

00:36:50.440 --> 00:36:53.200
the key difference I think is mostly that

00:36:53.200 --> 00:36:55.400
when you have mutable data,

00:36:55.400 --> 00:36:57.240
whereas with threads, you can share it.

00:36:57.240 --> 00:36:59.760
So threads can kind of talk to each other

00:36:59.760 --> 00:37:02.440
through the data that they share with each other.

00:37:02.440 --> 00:37:04.600
Whereas with sub-interpreters,

00:37:04.600 --> 00:37:06.320
there are a lot of restrictions

00:37:06.320 --> 00:37:09.080
and I expect we'll work on that to an extent,

00:37:09.080 --> 00:37:11.400
but it's also part of the programming model.

00:37:11.400 --> 00:37:13.400
And like Anthony was saying,

00:37:13.400 --> 00:37:15.920
if you really want to take advantage of parallelism,

00:37:15.920 --> 00:37:17.200
you need to think about it.

00:37:17.200 --> 00:37:20.440
You need to actually be careful about your data

00:37:20.440 --> 00:37:22.360
and how you're splitting up your work.

00:37:22.360 --> 00:37:24.080
- I think there's gonna be design patterns

00:37:24.080 --> 00:37:27.000
that we come to know or conventions we come to know,

00:37:27.000 --> 00:37:29.840
like, let's suppose I need some calculation

00:37:29.840 --> 00:37:31.560
and I'm gonna use it in a for loop.

00:37:31.560 --> 00:37:33.560
You don't run the calculation if it's the same

00:37:33.560 --> 00:37:35.200
over and over every time through the loop,

00:37:35.200 --> 00:37:37.320
you run it and then you use the result, right?

00:37:37.320 --> 00:37:39.720
So in this, a similar thing here would be like,

00:37:39.720 --> 00:37:42.080
well, if you're gonna process a bunch of data

00:37:42.080 --> 00:37:43.760
and the data comes from a say a database,

00:37:43.760 --> 00:37:46.480
don't do the query and hand it all the records,

00:37:46.480 --> 00:37:49.040
just tell it, go get that data from the database.

00:37:49.040 --> 00:37:51.400
That way it's already serialized in the right process

00:37:51.400 --> 00:37:54.400
and there's not this cross serialization

00:37:54.400 --> 00:37:56.960
through either pickling or whatever mechanism

00:37:56.960 --> 00:37:58.040
you come up with, right?

00:37:58.040 --> 00:38:00.640
But like, try to think about when you get the data,

00:38:00.640 --> 00:38:02.840
can you delay it until it's in the sub process

00:38:02.840 --> 00:38:06.120
and our sub interpreter rather and so on, right?

00:38:06.120 --> 00:38:07.440
- Yeah, definitely.

00:38:07.440 --> 00:38:11.480
One interesting thing is that PEP 734,

00:38:11.480 --> 00:38:13.440
I've included memory view

00:38:13.440 --> 00:38:16.240
as one of the types that's supported.

00:38:16.240 --> 00:38:18.880
So basically you can take a memory view

00:38:18.880 --> 00:38:23.360
of any kind of object that influence the buffer protocol.

00:38:23.360 --> 00:38:26.640
So like NumPy arrays and stuff like that

00:38:26.640 --> 00:38:29.760
and pass that memory view through to another interpreter

00:38:29.760 --> 00:38:32.360
and you can use it and it doesn't make a copy or anything.

00:38:32.360 --> 00:38:35.240
It actually uses the same underlying data.

00:38:35.240 --> 00:38:36.720
They actually get shared.

00:38:36.720 --> 00:38:37.680
- Oh, that's interesting.

00:38:37.680 --> 00:38:39.040
- Yeah, so there's,

00:38:39.040 --> 00:38:41.920
and I think there's even more room for that

00:38:41.920 --> 00:38:45.600
with other types, but we're starting small.

00:38:45.600 --> 00:38:49.800
But the key thing there is that, like you're saying,

00:38:49.800 --> 00:38:54.800
I mean, with coming up with different models and patterns

00:38:54.800 --> 00:38:59.680
and libraries, I'm sure they'll come up as people feel out

00:38:59.680 --> 00:39:01.480
really what's the easiest way

00:39:01.480 --> 00:39:03.000
to take advantage of these features.

00:39:03.000 --> 00:39:06.120
And that's the sort of thing that will apply

00:39:06.120 --> 00:39:09.120
not just to general free-threaded like no-gil,

00:39:09.120 --> 00:39:11.160
but also sub-interpreters.

00:39:11.160 --> 00:39:12.840
- Definitely, it's gonna be exciting.

00:39:12.840 --> 00:39:15.120
So I guess I wanna move on

00:39:15.120 --> 00:39:17.160
and talk about working with this in Python

00:39:17.160 --> 00:39:19.040
and the stuff that you've done, Anthony,

00:39:19.040 --> 00:39:22.400
but maybe a quick comment from the audience is Jazzy asked,

00:39:22.400 --> 00:39:23.800
"Is this built on top of a queue,

00:39:23.800 --> 00:39:25.080
which is built on top of linked lists?"

00:39:25.080 --> 00:39:26.240
Because I'm building this

00:39:26.240 --> 00:39:29.000
and my research led me to these data structures.

00:39:29.000 --> 00:39:30.720
I guess that's the communication

00:39:30.720 --> 00:39:34.600
across sub-interpreter, cross-interpreter communication.

00:39:34.600 --> 00:39:37.480
- Yeah, with sub-interpreters, like in PEPS 734,

00:39:37.480 --> 00:39:41.160
it's a queue implements the same interfaces

00:39:41.160 --> 00:39:42.960
as the queue from the queue module.

00:39:42.960 --> 00:39:46.360
But there's no reason why people couldn't implement

00:39:46.360 --> 00:39:48.440
whatever data structure they want

00:39:48.440 --> 00:39:50.560
for communicating between sub-interpreters.

00:39:50.560 --> 00:39:52.440
And then that data structure is in charge

00:39:52.440 --> 00:39:55.800
of preserving thread safety and so forth.

00:39:55.800 --> 00:39:56.640
- Yep, excellent.

00:39:56.640 --> 00:39:57.600
Yeah, it's not a standard queue.

00:39:57.600 --> 00:39:58.880
It's like a concurrent queue

00:39:58.880 --> 00:40:00.640
or something along those lines.

00:40:00.640 --> 00:40:01.520
- Yeah. - Yeah.

00:40:01.520 --> 00:40:04.320
All right, so all of this we've been talking about here

00:40:04.320 --> 00:40:08.960
is we're looking at this cool interpreter pool executor stuff

00:40:08.960 --> 00:40:12.760
that's in draft format, Anthony, for 3.13.

00:40:12.760 --> 00:40:14.880
And somehow I'm looking at this

00:40:14.880 --> 00:40:16.440
running Python parallel applications

00:40:16.440 --> 00:40:18.320
and sub-interpreters that you wrote.

00:40:18.320 --> 00:40:19.320
(Anthony laughs)

00:40:19.320 --> 00:40:20.360
What's going on here?

00:40:20.360 --> 00:40:21.880
How do you do this magic?

00:40:21.880 --> 00:40:24.200
- You need to know the secret password.

00:40:24.200 --> 00:40:27.160
So in Python-- - Right there, yeah.

00:40:27.160 --> 00:40:32.160
- In Python 3.12, the C API

00:40:32.160 --> 00:40:36.920
for creating sub-interpreters was included.

00:40:36.920 --> 00:40:39.600
And a lot of the mechanism

00:40:39.600 --> 00:40:42.520
for creating sub-interpreters was included.

00:40:42.520 --> 00:40:47.080
So there's also a, in CPython,

00:40:47.080 --> 00:40:48.080
there's a standard library,

00:40:48.080 --> 00:40:50.800
which I think everybody kind of knows.

00:40:50.800 --> 00:40:54.000
And then there are some hidden modules,

00:40:54.000 --> 00:40:56.120
which are mostly used for testing.

00:40:56.120 --> 00:41:00.080
So not all of them get bundled, I think, in the distribution.

00:41:00.080 --> 00:41:03.600
I think a lot of the test modules get taken out.

00:41:03.600 --> 00:41:07.600
But there are some hidden modules you can use for testing,

00:41:07.600 --> 00:41:09.400
'cause a lot of the test suite for CPython

00:41:09.400 --> 00:41:12.920
has to test C APIs, and nobody really wants

00:41:12.920 --> 00:41:14.280
to write unit tests in C.

00:41:14.280 --> 00:41:16.640
So they write the tests in Python,

00:41:16.640 --> 00:41:19.280
and then they kind of create these modules

00:41:19.280 --> 00:41:21.200
that basically just call the C functions.

00:41:21.200 --> 00:41:22.760
And so you can get the test coverage

00:41:22.760 --> 00:41:25.360
and do the testing from Python code.

00:41:25.360 --> 00:41:28.040
So I guess, what was from PEP6,

00:41:28.040 --> 00:41:31.920
I can't remember, I look at too many PEPs,

00:41:31.920 --> 00:41:34.360
six, six, Eric will probably know.

00:41:34.360 --> 00:41:37.240
What is now PEP734?

00:41:38.080 --> 00:41:42.640
- But the Python interface to create subinterpreters,

00:41:42.640 --> 00:41:45.240
a version of that was included in 3.12.

00:41:45.240 --> 00:41:46.680
So you can import this module

00:41:46.680 --> 00:41:49.520
called _xx_subinterpreters.

00:41:49.520 --> 00:41:53.240
And it's called _xx 'cause it kind of indicates

00:41:53.240 --> 00:41:55.800
that it's experimental and it's underscore

00:41:55.800 --> 00:41:58.120
'cause you probably shouldn't be using it.

00:41:58.120 --> 00:41:59.600
- It's not safe for work to me.

00:41:59.600 --> 00:42:01.760
I mean, I don't know.

00:42:01.760 --> 00:42:06.720
- But it provides a good way of people

00:42:06.720 --> 00:42:08.520
actually testing this stuff

00:42:08.520 --> 00:42:12.080
and seeing what happens if I import my C extension

00:42:12.080 --> 00:42:13.400
from a subinterpreter.

00:42:13.400 --> 00:42:16.040
So that's kind of some of what I've been doing

00:42:16.040 --> 00:42:20.880
is looking at, okay, what can we try and do in parallel?

00:42:20.880 --> 00:42:26.760
And this blog post, I wanted to try a WSGI

00:42:26.760 --> 00:42:28.520
or an ASGI web app.

00:42:28.520 --> 00:42:32.480
And the typical pattern that you have at the moment,

00:42:32.480 --> 00:42:33.920
and I guess how a lot of people

00:42:33.920 --> 00:42:35.760
would be using parallel code,

00:42:35.760 --> 00:42:38.160
but without really realizing it,

00:42:38.160 --> 00:42:43.080
is when you deploy a web app for Django Flask or FastAPI,

00:42:43.080 --> 00:42:46.440
you can't have one GIL per web server

00:42:46.440 --> 00:42:49.040
because if you've got one GIL per web server,

00:42:49.040 --> 00:42:54.040
you can only have one user per website, which is not great.

00:42:54.040 --> 00:42:57.200
So the way that most web servers implement this

00:42:57.200 --> 00:42:59.680
is that they have a pool of workers.

00:42:59.680 --> 00:43:04.840
G-Unicorn does that by spawning Python processes

00:43:04.840 --> 00:43:07.160
and then using the multiprocessing module.

00:43:07.160 --> 00:43:10.400
So it basically creates multiple Python processes

00:43:10.400 --> 00:43:12.800
all listening to the same socket.

00:43:12.800 --> 00:43:15.000
And then when a web request comes in,

00:43:15.000 --> 00:43:17.080
one of them takes that request.

00:43:17.080 --> 00:43:19.800
It also then inside that has a thread pool.

00:43:19.800 --> 00:43:23.360
So even basically a thread pool

00:43:23.360 --> 00:43:25.880
is better for concurrent code.

00:43:25.880 --> 00:43:27.880
So G-Unicorn normally is used

00:43:27.880 --> 00:43:30.680
in a multi-worker, multi-thread model.

00:43:30.680 --> 00:43:32.440
That's how we kind of talk about it.

00:43:32.440 --> 00:43:35.920
So you'd have the number of workers that you have CPU cores,

00:43:35.920 --> 00:43:40.160
and then inside that you'd have multiple threads.

00:43:40.160 --> 00:43:42.960
So it kind of means you can handle more requests

00:43:42.960 --> 00:43:43.840
at a time.

00:43:43.840 --> 00:43:44.880
If you've got eight cores,

00:43:44.880 --> 00:43:48.200
you can handle at least eight requests at a time.

00:43:48.200 --> 00:43:50.440
However, because most web code

00:43:50.440 --> 00:43:52.480
can be concurrent on the backend,

00:43:52.480 --> 00:43:55.080
like you're making a database query

00:43:55.080 --> 00:43:58.000
or you're reading some stuff from a file like that,

00:43:58.000 --> 00:44:00.640
that doesn't necessarily need to hold the GIL.

00:44:00.640 --> 00:44:02.760
So you can run it concurrently,

00:44:02.760 --> 00:44:05.040
which is why you have multiple threads.

00:44:05.040 --> 00:44:07.000
So even if you've only got eight CPU cores,

00:44:07.000 --> 00:44:12.000
you can actually handle 16 or 32 web requests at once

00:44:12.000 --> 00:44:14.320
because some of them will be waiting

00:44:14.320 --> 00:44:17.120
for the database server to finish running at SQL query

00:44:17.120 --> 00:44:20.640
or the API that it called to actually reply.

00:44:20.640 --> 00:44:23.520
So what I wanted to do with this experiment

00:44:23.520 --> 00:44:26.840
was to look at the multi-worker, multi-thread model

00:44:26.840 --> 00:44:28.920
for web apps and say, okay,

00:44:28.920 --> 00:44:31.880
could the worker be a sub-interpreter

00:44:31.880 --> 00:44:35.400
and what difference would that make?

00:44:35.400 --> 00:44:38.360
So instead of using multi-processing for the workers,

00:44:38.360 --> 00:44:41.520
could I use sub-interpreters for the workers?

00:44:41.520 --> 00:44:44.440
So even though the Python interface in 3.12

00:44:44.440 --> 00:44:46.360
was experimental,

00:44:46.360 --> 00:44:48.520
it basically wanted to adapt Hypercorn,

00:44:48.520 --> 00:44:53.520
which is a web server for ASCII and WSGI apps in Python,

00:44:53.520 --> 00:44:55.800
wanted to adapt Hypercorn

00:44:55.800 --> 00:44:57.920
and basically start Hypercorn workers

00:44:57.920 --> 00:44:59.280
from a sub-interpreter pool

00:44:59.280 --> 00:45:02.800
and then seeing if I can run Django, Flask,

00:45:02.800 --> 00:45:04.880
and FastAPI in a sub-interpreter.

00:45:04.880 --> 00:45:07.560
So a single process, single Python process,

00:45:07.560 --> 00:45:09.720
but running across multiple cores

00:45:09.720 --> 00:45:11.680
and listening to web requests

00:45:11.680 --> 00:45:14.360
and basically running and serving web requests

00:45:14.360 --> 00:45:15.600
with multiple gills.

00:45:15.600 --> 00:45:16.440
So that was the task.

00:45:16.440 --> 00:45:18.360
- So in the article, you said you had started

00:45:18.360 --> 00:45:21.440
with a G-Unicorn and they just made too many assumptions

00:45:21.440 --> 00:45:24.280
about the web workers

00:45:24.280 --> 00:45:26.600
being truly sub-processes,

00:45:26.600 --> 00:45:28.480
but Hypercorn was a better fit, you said.

00:45:28.480 --> 00:45:31.720
- Yeah, it was easier to implement this experiment

00:45:31.720 --> 00:45:33.200
in Hypercorn.

00:45:33.200 --> 00:45:36.240
It had like a single entry point

00:45:36.240 --> 00:45:38.000
because when you start an interpreter,

00:45:38.000 --> 00:45:39.600
when you start a sub-interpreter,

00:45:39.600 --> 00:45:42.280
you need to import the modules that you want to use.

00:45:42.280 --> 00:45:46.600
You can't just say, run this function over here.

00:45:46.600 --> 00:45:50.440
You can, but if that function relies on something else

00:45:50.440 --> 00:45:51.280
that you've imported,

00:45:51.280 --> 00:45:53.960
you need to import that from the new sub-interpreter.

00:45:53.960 --> 00:45:56.480
So what I did with this experiment

00:45:56.480 --> 00:45:58.800
was basically start a sub-interpreter

00:45:58.800 --> 00:46:01.920
that imports Hypercorn, listens to the sockets,

00:46:01.920 --> 00:46:04.200
and then is ready to serve web requests.

00:46:04.200 --> 00:46:05.120
- Interesting, okay.

00:46:05.120 --> 00:46:07.600
And at a minimum, you got it working, right?

00:46:07.600 --> 00:46:09.960
- Yeah, it did a hello world.

00:46:09.960 --> 00:46:13.360
So we got that working.

00:46:13.360 --> 00:46:15.040
So I was, yeah, pleased with that.

00:46:15.040 --> 00:46:19.120
And then kind of started doing some more testing of it.

00:46:19.120 --> 00:46:21.560
So, you know, how many concurrent requests

00:46:21.560 --> 00:46:22.800
can I make at once?

00:46:22.800 --> 00:46:23.880
How does it handle that?

00:46:23.880 --> 00:46:26.040
What does my CPU core load look like?

00:46:26.040 --> 00:46:27.560
Is it distributing it well?

00:46:27.560 --> 00:46:31.440
And then kind of some of the questions are, you know,

00:46:31.440 --> 00:46:35.240
how do you share data between the sub-interpreters?

00:46:35.240 --> 00:46:39.480
So the minimum I had to do was each sub-interpreter

00:46:39.480 --> 00:46:42.520
needs to know which web socket should I be listening to?

00:46:42.520 --> 00:46:45.240
So like which network socket, once I've started,

00:46:45.240 --> 00:46:46.560
what port is it running on?

00:46:46.560 --> 00:46:49.120
And is it running on multiple ports?

00:46:49.120 --> 00:46:50.440
And which one should I listen to?

00:46:50.440 --> 00:46:52.520
So yeah, that's the first thing I had to do.

00:46:52.520 --> 00:46:53.360
- Nice.

00:46:53.360 --> 00:46:55.360
Can we just tell people real quick about just like,

00:46:55.360 --> 00:46:58.720
what are the commands like at the Python level

00:46:58.720 --> 00:47:01.560
that you look at in order to create an interpreter,

00:47:01.560 --> 00:47:02.800
run some code on it and so on?

00:47:02.800 --> 00:47:04.960
What's this weird world look like?

00:47:04.960 --> 00:47:06.040
- Derek, do you wanna cover that?

00:47:06.040 --> 00:47:08.200
- Yeah, there is a whole lot.

00:47:08.200 --> 00:47:12.040
I mean, if we talk about PEP 734,

00:47:12.040 --> 00:47:14.000
you have an interpreters module

00:47:14.000 --> 00:47:15.960
with a create function in it

00:47:15.960 --> 00:47:18.040
that returns you an interpreter object.

00:47:18.040 --> 00:47:20.000
And then once you have the interpreter object,

00:47:20.000 --> 00:47:23.600
you'll have, it has a function called run,

00:47:23.600 --> 00:47:26.200
or a method, interpreter object

00:47:26.200 --> 00:47:28.440
also has a method called exec.

00:47:28.440 --> 00:47:30.560
I'm trying to remember what it is.

00:47:30.560 --> 00:47:34.600
Exec sync, because it's synchronous with the current thread.

00:47:34.600 --> 00:47:38.320
And whereas exec run will create a new thread for you

00:47:38.320 --> 00:47:40.080
and run things in that there.

00:47:40.080 --> 00:47:42.200
So there's kind of different use cases.

00:47:42.200 --> 00:47:43.800
But it's basically the same thing.

00:47:43.800 --> 00:47:46.760
You have some code, currently supports,

00:47:48.440 --> 00:47:51.560
you give it a string with all your code on it,

00:47:51.560 --> 00:47:53.680
like you load it from a file or something.

00:47:53.680 --> 00:47:55.120
Basically, it's a script.

00:47:55.120 --> 00:47:57.840
It's gonna run in that subinterpreter.

00:47:57.840 --> 00:48:00.360
Alternately, you can give it a function.

00:48:00.360 --> 00:48:03.200
And as long as that function isn't a closure,

00:48:03.200 --> 00:48:05.440
doesn't have any arguments and stuff like that.

00:48:05.440 --> 00:48:09.680
So it's just like really basic, basically a script.

00:48:09.680 --> 00:48:11.360
If you got something like that,

00:48:11.360 --> 00:48:14.560
you can also pass that through, and then it runs it.

00:48:14.560 --> 00:48:16.680
And that's just about it.

00:48:16.680 --> 00:48:18.120
If you wanna get some results back,

00:48:18.120 --> 00:48:20.080
you're gonna have to manually pass them back

00:48:20.080 --> 00:48:21.920
kind of like you do with threads.

00:48:21.920 --> 00:48:24.400
But that's something people already understand pretty well.

00:48:24.400 --> 00:48:26.120
- Right, and create one of those channels,

00:48:26.120 --> 00:48:27.760
and then you just wait for it to exit

00:48:27.760 --> 00:48:29.960
and then read from the channel, something like that.

00:48:29.960 --> 00:48:33.680
Yeah, and so there's a way to say things like just run.

00:48:33.680 --> 00:48:36.360
And there's also a way to say, create an interpreter,

00:48:36.360 --> 00:48:38.960
and then you could use the interpreter to do things.

00:48:38.960 --> 00:48:41.760
And that lets you only pay the process

00:48:41.760 --> 00:48:44.040
like startup cost ones, right?

00:48:44.040 --> 00:48:45.800
- Yeah, yeah, and you can also,

00:48:45.800 --> 00:48:49.520
you can call that the run multiple times.

00:48:49.520 --> 00:48:52.800
And each time it kind of adds on to what ran before.

00:48:52.800 --> 00:48:57.080
So if you run some code that modifies things

00:48:57.080 --> 00:48:59.560
or import some modules and that sort of thing,

00:48:59.560 --> 00:49:01.400
those will still be there the next time

00:49:01.400 --> 00:49:03.280
you run some code in that interpreter,

00:49:03.280 --> 00:49:06.040
which is nice 'cause then if you got some startup stuff

00:49:06.040 --> 00:49:07.800
that you need to do one time,

00:49:07.800 --> 00:49:09.120
you can do that ahead of time

00:49:09.120 --> 00:49:10.840
right after you create the interpreter.

00:49:10.840 --> 00:49:13.240
But then in kind of your loop in your worker,

00:49:13.240 --> 00:49:16.720
then you run again and all that stuff is ready to go.

00:49:16.720 --> 00:49:17.560
- Oh, that's interesting.

00:49:17.560 --> 00:49:20.120
'Cause when I think about say my web apps,

00:49:20.120 --> 00:49:22.680
a lot of them talk to MongoDB and use Beanie

00:49:22.680 --> 00:49:24.760
and you go to Beanie and you tell it to like

00:49:24.760 --> 00:49:28.680
create a connection or a MongoDB client pool

00:49:28.680 --> 00:49:30.160
and it does all that stuff

00:49:30.160 --> 00:49:31.840
and then you just ambiently talk to it.

00:49:31.840 --> 00:49:33.680
Like go to that, you know, kind of like Django or whatever.

00:49:33.680 --> 00:49:35.880
Go to that class and do a query on it.

00:49:35.880 --> 00:49:38.440
You could run that startup code like once potentially

00:49:38.440 --> 00:49:41.520
and have that pool just hanging around for subsequent work.

00:49:41.520 --> 00:49:42.360
Nice.

00:49:42.360 --> 00:49:44.920
All right, let's see some more stuff.

00:49:44.920 --> 00:49:48.320
So you said you got it working pretty well, Anthony.

00:49:48.320 --> 00:49:49.920
And you said one of the challenges

00:49:49.920 --> 00:49:51.960
was trying to get it to shut down, right?

00:49:51.960 --> 00:49:52.800
- Mm.

00:49:52.800 --> 00:49:53.640
- Yeah.

00:49:53.640 --> 00:49:56.320
- Yeah, so in Python, when you start a Python process,

00:49:56.320 --> 00:50:00.160
you can press Control + C to quit,

00:50:00.160 --> 00:50:01.760
which is a keyboard interrupt.

00:50:01.760 --> 00:50:06.200
That kind of sends the interrupt in that process.

00:50:06.200 --> 00:50:10.920
All of these web servers have got like a mechanism

00:50:10.920 --> 00:50:12.680
for cleanly shutting down.

00:50:12.680 --> 00:50:14.520
'Cause you don't wanna just, if you press Control + C,

00:50:14.520 --> 00:50:17.560
you don't wanna just terminate the processes.

00:50:17.560 --> 00:50:20.360
'Cause when you write an ASCII app in particular,

00:50:20.360 --> 00:50:22.720
you can have like events that you can do.

00:50:22.720 --> 00:50:24.960
So people who've done FastAPI probably know

00:50:24.960 --> 00:50:28.280
the like the on events decorator that you can put

00:50:28.280 --> 00:50:30.560
and say when my app starts up,

00:50:30.560 --> 00:50:32.760
create a database connection pool

00:50:32.760 --> 00:50:35.680
and when it shuts down, then go and clean up all this stuff.

00:50:35.680 --> 00:50:39.680
So if the web servers decided to shut down

00:50:39.680 --> 00:50:41.920
for whatever reason, whether you've pressed Control + C

00:50:41.920 --> 00:50:45.680
or it just decided to close for whatever reason,

00:50:45.680 --> 00:50:48.640
it needs to tell all the workers to shut down cleanly.

00:50:48.640 --> 00:50:53.480
So signals, like the signals module

00:50:53.480 --> 00:50:56.280
doesn't work between sub-interpreters

00:50:56.280 --> 00:50:59.560
because it kind of, it sits in the interpreter state

00:50:59.560 --> 00:51:01.080
from what I understand.

00:51:01.080 --> 00:51:03.840
So what I did was basically use a channel

00:51:03.840 --> 00:51:07.760
so that the main worker, like the coordinator,

00:51:07.760 --> 00:51:09.920
when that had a shutdown request,

00:51:09.920 --> 00:51:13.000
it would send a message to all of the sub-interpreters

00:51:13.000 --> 00:51:15.280
to say, okay, can you stop now?

00:51:15.280 --> 00:51:18.440
And then it would kick off a job,

00:51:18.440 --> 00:51:21.080
basically tell Hypercorn in this case,

00:51:21.080 --> 00:51:22.280
to shut down cleanly,

00:51:22.280 --> 00:51:25.280
call any shutdown functions that you might have

00:51:25.280 --> 00:51:27.920
and then log a message to say that it's shutting down

00:51:27.920 --> 00:51:30.800
as well because the other thing is with web servers,

00:51:30.800 --> 00:51:32.760
if it just terminated immediately

00:51:32.760 --> 00:51:35.040
and then you looked at your logs

00:51:35.040 --> 00:51:37.040
and you were like, okay, why did the website

00:51:37.040 --> 00:51:38.520
suddenly stop working?

00:51:38.520 --> 00:51:39.960
And there was no log entries.

00:51:39.960 --> 00:51:43.000
It just went from I'm handling requests

00:51:43.000 --> 00:51:45.680
to just absolute silence.

00:51:45.680 --> 00:51:47.120
That also wouldn't be very helpful.

00:51:47.120 --> 00:51:48.600
So it needs to write log messages,

00:51:48.600 --> 00:51:51.680
it needs to call like shutdown functions and stuff.

00:51:51.680 --> 00:51:55.440
So what I did was, and this is, I guess,

00:51:55.440 --> 00:51:58.680
where it's kind of a bit of a turtles all the way down,

00:51:58.680 --> 00:52:02.040
but inside the sub-interpreter, I start another thread

00:52:02.040 --> 00:52:04.640
because if you have a polar,

00:52:04.640 --> 00:52:07.480
which listens to a signal on a channel,

00:52:07.480 --> 00:52:09.480
that's a blocking operation.

00:52:09.480 --> 00:52:13.880
So at the bottom of my sub-interpreter code,

00:52:13.880 --> 00:52:15.920
I've got, okay, run Hypercorn.

00:52:15.920 --> 00:52:17.800
So it's gonna run, it's gonna listen to Socket's

00:52:17.800 --> 00:52:20.560
web requests, but I need to also be able to run

00:52:20.560 --> 00:52:23.000
concurrently in the sub-interpreter,

00:52:23.000 --> 00:52:26.320
a loop which listens to the communication channel

00:52:26.320 --> 00:52:30.880
and sees if a shutdown request has been sent.

00:52:30.880 --> 00:52:33.880
So this is kind of maybe an implementation detail

00:52:33.880 --> 00:52:36.480
of how interpreters work in Python,

00:52:36.480 --> 00:52:39.160
but interpreters have threads as well.

00:52:39.160 --> 00:52:42.760
So you can start threads inside interpreters.

00:52:42.760 --> 00:52:46.680
So similar to what I said with g-unicorn and Hypercorn,

00:52:46.680 --> 00:52:48.360
how you got multi-worker, multi-thread,

00:52:48.360 --> 00:52:50.520
like each worker has its own threads.

00:52:50.520 --> 00:52:53.400
In Python, interpreters have the threads.

00:52:53.400 --> 00:52:56.200
So you can start a sub-interpreter

00:52:56.200 --> 00:52:57.640
and then inside that sub-interpreter,

00:52:57.640 --> 00:52:59.440
you can also start multiple threads

00:52:59.440 --> 00:53:01.840
and you can do coroutines

00:53:01.840 --> 00:53:04.040
and all that kind of stuff as well.

00:53:04.040 --> 00:53:06.600
So basically what I did is to start a sub-interpreter

00:53:06.600 --> 00:53:08.400
which also starts a thread

00:53:08.400 --> 00:53:10.760
and that thread listens to the communication channel

00:53:10.760 --> 00:53:12.840
and then waits for a shutdown request.

00:53:12.840 --> 00:53:16.320
- Right, tells Hypercorn, all right, you're done.

00:53:16.320 --> 00:53:17.520
We're out of here.

00:53:17.520 --> 00:53:18.840
Yeah, okay, interesting.

00:53:18.840 --> 00:53:20.920
Here's a interesting question from the audience,

00:53:20.920 --> 00:53:22.120
from Chris as well.

00:53:22.120 --> 00:53:25.320
It says, "We talked about the global kind of startup,

00:53:25.320 --> 00:53:27.200
"like if you run that once, it'll already be set.

00:53:27.200 --> 00:53:30.760
"And does that make code somewhat non-deterministic

00:53:30.760 --> 00:53:32.280
"in the sub-interpreter?"

00:53:32.280 --> 00:53:34.120
I mean, if you explicitly work with it, no.

00:53:34.120 --> 00:53:36.360
But if you're doing the pool, like which one do you get?

00:53:36.360 --> 00:53:38.120
Is it initialized or not?

00:53:38.120 --> 00:53:41.440
Eric, do you have an idea of a startup function

00:53:41.440 --> 00:53:44.760
that runs in the interpreter pool executor type thing

00:53:44.760 --> 00:53:46.600
or is it just they get doled out

00:53:46.600 --> 00:53:48.000
and they run what they run?

00:53:48.000 --> 00:53:52.200
- With concurrent features,

00:53:52.200 --> 00:53:54.320
it's already kind of a pattern.

00:53:54.320 --> 00:53:57.440
You have an initialized function that you can call

00:53:57.440 --> 00:53:58.680
that'll do the right thing

00:53:59.640 --> 00:54:03.080
and then you have your task

00:54:03.080 --> 00:54:04.880
that the worker's actually running.

00:54:04.880 --> 00:54:09.000
I don't know.

00:54:09.000 --> 00:54:11.680
I wouldn't say it's non-deterministic

00:54:11.680 --> 00:54:14.320
unless you have no control over it.

00:54:14.320 --> 00:54:18.640
I mean, if you wanna make sure that state progresses

00:54:18.640 --> 00:54:20.400
in an expected way,

00:54:20.400 --> 00:54:23.840
then you're gonna run your own sub-interpreters, right?

00:54:23.840 --> 00:54:25.840
But if you have no control over the sub-interpreters,

00:54:25.840 --> 00:54:27.440
you're just like handing off to some library

00:54:27.440 --> 00:54:28.960
that's using sub-interpreters.

00:54:28.960 --> 00:54:32.920
I would think it'd be somewhat not quite so important

00:54:32.920 --> 00:54:35.360
about whether it's deterministic or not.

00:54:35.360 --> 00:54:38.800
I mean, each time it runs,

00:54:38.800 --> 00:54:41.760
there are a variety of things.

00:54:41.760 --> 00:54:44.240
The whole thing could be kind of reset

00:54:44.240 --> 00:54:48.720
or you could make sure that anything that runs it,

00:54:48.720 --> 00:54:50.480
any part of your code that runs

00:54:50.480 --> 00:54:54.920
is careful to keep its state self-contained

00:54:54.920 --> 00:54:58.880
and therefore you preserve determinist behavior that way.

00:54:58.880 --> 00:55:02.000
- What I do a lot is I'll write code that'll say,

00:55:02.000 --> 00:55:05.000
if this is already initialized, don't do it again.

00:55:05.000 --> 00:55:07.200
So I talked about the database connection thing.

00:55:07.200 --> 00:55:08.680
If somebody were to call it twice,

00:55:08.680 --> 00:55:09.560
it'll say, well,

00:55:09.560 --> 00:55:13.840
looks like the connection's already not none, so we're good.

00:55:13.840 --> 00:55:15.560
You could just always run the startup code

00:55:15.560 --> 00:55:18.440
with one of these short circuit things that says,

00:55:18.440 --> 00:55:20.080
hey, it looks like on this interpreter,

00:55:20.080 --> 00:55:21.680
this is already done, we're good.

00:55:21.680 --> 00:55:26.360
But that would probably handle a good chunk of it right there.

00:55:26.360 --> 00:55:28.400
But we're back to this thing that Anthony said, right?

00:55:28.400 --> 00:55:31.240
Like we're gonna learn some new programming patterns

00:55:31.240 --> 00:55:33.400
potentially, yeah, quite interesting.

00:55:33.400 --> 00:55:34.880
So we talked at the beginning

00:55:34.880 --> 00:55:37.240
about how sub-interpreters have their own memory

00:55:37.240 --> 00:55:40.040
and their own module loads and all those kinds of things.

00:55:40.040 --> 00:55:42.680
And that might be potentially interesting for isolation.

00:55:42.680 --> 00:55:45.480
Also kind of tying back to Chris's comment here,

00:55:45.480 --> 00:55:49.400
this isolation is pretty interesting for testing, right,

00:55:49.400 --> 00:55:51.360
Anthony, like my test?

00:55:51.360 --> 00:55:52.360
So another thing you've been up to

00:55:52.360 --> 00:55:55.480
is working with trying to run pytest sessions

00:55:55.480 --> 00:55:56.760
in sub-interpreters.

00:55:56.760 --> 00:55:57.720
Tell people about that.

00:55:57.720 --> 00:56:01.200
- Yeah, so I started off with a web worker.

00:56:01.200 --> 00:56:03.920
One of the things I hit with a web worker

00:56:03.920 --> 00:56:06.480
was that I couldn't start Django applications

00:56:06.480 --> 00:56:10.960
and realized the reason was the,

00:56:10.960 --> 00:56:13.400
so the date/time module.

00:56:13.400 --> 00:56:16.560
So the C, the Python standard library,

00:56:16.560 --> 00:56:18.800
some of the modules are implemented in Python,

00:56:18.800 --> 00:56:21.040
some of them are implemented in C,

00:56:21.040 --> 00:56:23.720
but some of them are a combination of both.

00:56:23.720 --> 00:56:26.400
So some modules you import in the standard library

00:56:26.400 --> 00:56:29.720
have like a C part that's been implemented in C

00:56:29.720 --> 00:56:31.280
for performance reasons typically,

00:56:31.280 --> 00:56:34.560
or 'cause it needs some special operating system API

00:56:34.560 --> 00:56:36.520
that you can't access from Python.

00:56:36.520 --> 00:56:38.560
And then the front end is Python.

00:56:38.560 --> 00:56:44.520
So there is a list basically of standard library modules

00:56:44.520 --> 00:56:48.600
that are written in C that had some sort of global state.

00:56:48.600 --> 00:56:52.560
And then the core developers have been going down that list

00:56:52.560 --> 00:56:55.560
and fixing them up so that they can be imported

00:56:55.560 --> 00:56:57.360
from a sub-interpreter,

00:56:57.360 --> 00:57:01.760
or just marking them as not compatible with sub-interpreters.

00:57:01.760 --> 00:57:04.320
One such example was the Readline module

00:57:04.320 --> 00:57:08.360
that Eric and I were kind of working on last week

00:57:08.360 --> 00:57:09.360
and the week before.

00:57:09.360 --> 00:57:13.520
Readline is used for, I guess, listening to user input.

00:57:13.520 --> 00:57:15.120
So if you run the input built-in,

00:57:15.120 --> 00:57:18.240
Readline is one of the utilities it uses

00:57:18.240 --> 00:57:21.080
to listen to keyboard input.

00:57:21.080 --> 00:57:24.120
If you start, let's say you started five sub-interpreters

00:57:24.120 --> 00:57:26.520
at the same time and all of them did a Readline

00:57:26.520 --> 00:57:28.840
listen for input, like what would you expect

00:57:28.840 --> 00:57:30.680
the behavior to be?

00:57:30.680 --> 00:57:32.440
Which when you type in the keyboard,

00:57:32.440 --> 00:57:33.280
which-- - Yeah, exactly.

00:57:33.280 --> 00:57:35.600
- Yeah, where would you expect the letters to come out?

00:57:35.600 --> 00:57:38.320
So it kind of poses an interesting question.

00:57:38.320 --> 00:57:43.120
So Readline is not compatible with sub-interpreters,

00:57:43.120 --> 00:57:45.680
but we discovered like it was actually sharing

00:57:45.680 --> 00:57:46.520
a global state.

00:57:46.520 --> 00:57:50.480
So when it initialized, it would install like a callback.

00:57:50.480 --> 00:57:53.200
And what that meant was that even though it said

00:57:53.200 --> 00:57:55.960
it's not compatible, if you started multiple,

00:57:55.960 --> 00:57:58.400
it's in sub-interpreters that imported Readline,

00:57:58.400 --> 00:58:00.160
it would crash Python itself.

00:58:00.160 --> 00:58:05.880
The DateTime module is another one that needs fixing.

00:58:05.880 --> 00:58:08.640
It installs a bunch of global state.

00:58:08.640 --> 00:58:10.480
So yeah, DateTime was another one.

00:58:10.480 --> 00:58:12.880
So what I wanted to do is to try and test

00:58:12.880 --> 00:58:16.480
some other C extensions that I had.

00:58:16.480 --> 00:58:21.000
And just basically write a pytest extension,

00:58:21.000 --> 00:58:22.280
or pytest plugin, I guess,

00:58:22.280 --> 00:58:25.760
which you've got an existing pytest suite,

00:58:25.760 --> 00:58:28.800
but you want to run all of that in a sub-interpreter.

00:58:28.800 --> 00:58:31.000
And the goal of this is really that

00:58:31.000 --> 00:58:34.240
you're developing a C extension,

00:58:34.240 --> 00:58:37.560
you've written a test suite already for pytest,

00:58:37.560 --> 00:58:41.160
and you want to run that inside a sub-interpreter.

00:58:41.160 --> 00:58:43.680
So I'm looking at this from a couple of different angles,

00:58:43.680 --> 00:58:47.160
but I want to really try and use sub-interpreters

00:58:47.160 --> 00:58:49.600
in other ways, import some C extensions

00:58:49.600 --> 00:58:52.720
that have never even considered the idea of sub-interpreters

00:58:52.720 --> 00:58:54.520
and just see how they respond to it.

00:58:54.520 --> 00:58:57.800
Like Readline was a good example.

00:58:57.800 --> 00:59:00.560
Like, I think it was a, this won't work,

00:59:00.560 --> 00:59:02.720
but the fact that it crashed is bad.

00:59:02.720 --> 00:59:03.920
- How is it going to crash, right?

00:59:03.920 --> 00:59:05.280
Like what's happening there?

00:59:05.280 --> 00:59:08.400
- Yeah, so it should have kind of just said,

00:59:08.400 --> 00:59:09.480
this is not compatible.

00:59:09.480 --> 00:59:11.680
And then I was kind of uncovered a,

00:59:12.560 --> 00:59:14.160
and this is all super experimental as well.

00:59:14.160 --> 00:59:16.480
So like, this is not, you know,

00:59:16.480 --> 00:59:19.600
you've had to import the underscore XX module

00:59:19.600 --> 00:59:21.560
to even try this.

00:59:21.560 --> 00:59:26.320
So yeah, there's Readline, DateTime was another one.

00:59:26.320 --> 00:59:29.520
And so I put this sort of pytest extension together

00:59:29.520 --> 00:59:32.120
so that I could run some existing test suites

00:59:32.120 --> 00:59:33.760
inside sub-interpreters.

00:59:33.760 --> 00:59:38.280
And then the next thing that I looked at doing was,

00:59:38.280 --> 00:59:41.000
CPython has a huge test suite.

00:59:42.040 --> 00:59:46.840
So basically how all of Python itself is tested,

00:59:46.840 --> 00:59:51.840
the parser, the compiler, the evaluation loop,

00:59:51.840 --> 00:59:53.480
all of the standard library modules

00:59:53.480 --> 00:59:55.280
have got pretty good test coverage.

00:59:55.280 --> 00:59:58.600
So like when you compile Python from source

00:59:58.600 --> 01:00:00.360
or you make changes on GitHub,

01:00:00.360 --> 01:00:02.040
like it runs the test suite to make sure

01:00:02.040 --> 01:00:04.800
that your changes didn't break anything.

01:00:04.800 --> 01:00:08.280
Now, the next thing I kind of wanted to look at was,

01:00:08.280 --> 01:00:12.240
okay, can we, to try and kind of get ahead of the curve

01:00:12.240 --> 01:00:15.120
really on sub-interpreter adoption.

01:00:15.120 --> 01:00:18.440
So in 3.13, when PEP 734 lands,

01:00:18.440 --> 01:00:21.720
can we try and test all of the standard library

01:00:21.720 --> 01:00:23.280
inside a sub-interpreter

01:00:23.280 --> 01:00:26.440
and see if it has any other weird behaviors?

01:00:26.440 --> 01:00:31.720
And this test will probably apply to free threading as well,

01:00:31.720 --> 01:00:36.200
to be honest, because I think anything that you're doing

01:00:36.200 --> 01:00:38.160
like this, you're importing these C extensions,

01:00:38.160 --> 01:00:41.280
which always assumed that there was a big GIL in place.

01:00:41.280 --> 01:00:42.640
If you take away that assumption,

01:00:42.640 --> 01:00:44.920
then you get these strange behaviors.

01:00:44.920 --> 01:00:47.360
So yeah, the next thing I've been working on

01:00:47.360 --> 01:00:50.120
is basically running the CPython test suite

01:00:50.120 --> 01:00:52.160
inside sub-interpreters and then seeing

01:00:52.160 --> 01:00:55.040
what kind of weird behaviors pop up.

01:00:55.040 --> 01:00:57.560
- I think it's a great idea 'cause obviously CPython

01:00:57.560 --> 01:00:59.840
is gonna need to run code in a sub-interpreter,

01:00:59.840 --> 01:01:00.760
run our code, right?

01:01:00.760 --> 01:01:03.640
So at a minimum, the framework,

01:01:03.640 --> 01:01:05.240
interpreter, all the runtime bits,

01:01:05.240 --> 01:01:06.840
that should all hang together, right?

01:01:06.840 --> 01:01:08.920
- Yeah, there are some modules that doesn't make sense

01:01:08.920 --> 01:01:11.680
to run in sub-interpreters, Readline was an example.

01:01:11.680 --> 01:01:14.040
- Some TKinterp maybe.

01:01:14.040 --> 01:01:15.800
- Yeah, yeah, possibly.

01:01:15.800 --> 01:01:18.000
- Maybe not actually, I don't know.

01:01:18.000 --> 01:01:20.280
- Yeah, if you think about like,

01:01:20.280 --> 01:01:23.040
if you got, when you're doing GUI programming, right,

01:01:23.040 --> 01:01:26.240
you're gonna have kind of your core stuff

01:01:26.240 --> 01:01:27.880
running the main thread, right?

01:01:27.880 --> 01:01:31.160
And then you hand off, you may have sub-threads

01:01:31.160 --> 01:01:34.320
doing some other work, but the core of the application,

01:01:34.320 --> 01:01:36.720
think of it as running in the main thread.

01:01:36.720 --> 01:01:37.560
- Yeah, absolutely.

01:01:37.560 --> 01:01:38.600
- I think of applications in that way.

01:01:38.600 --> 01:01:41.520
And there are certain things that you do in Python,

01:01:41.520 --> 01:01:44.720
standard library modules that really only make sense

01:01:44.720 --> 01:01:46.520
with that main thread.

01:01:46.520 --> 01:01:49.360
So supporting those in sub-interpreters

01:01:49.360 --> 01:01:51.640
isn't quite as meaningful.

01:01:51.640 --> 01:01:54.360
- Yeah, I can't remember all the details,

01:01:54.360 --> 01:01:57.360
but I feel like there's some parts of Windows itself,

01:01:57.360 --> 01:01:59.640
some UI frameworks there that required

01:01:59.640 --> 01:02:01.480
that you access them on the main program thread,

01:02:01.480 --> 01:02:03.640
not on some background thread as well,

01:02:03.640 --> 01:02:05.480
'cause it'd freak things out.

01:02:05.480 --> 01:02:07.360
So it seems like not unusable.

01:02:07.360 --> 01:02:08.280
- Yeah, same is true.

01:02:08.280 --> 01:02:12.840
Like the signal module, I remember at exit, a few others.

01:02:12.840 --> 01:02:13.680
- Excellent.

01:02:13.680 --> 01:02:15.840
All right, well, I guess let's, we're getting short on time.

01:02:15.840 --> 01:02:17.560
Let's wrap it up with this.

01:02:17.560 --> 01:02:20.840
So the big thing to keep an eye on really here

01:02:20.840 --> 01:02:24.720
is PEP 734, because that's when this would land,

01:02:24.720 --> 01:02:28.800
this, you're no longer with the underscore XX sub-interpreter

01:02:28.800 --> 01:02:32.400
and you're just working with the interpreter's sub-module.

01:02:32.400 --> 01:02:33.560
- Yeah, 313.

01:02:33.600 --> 01:02:35.600
- Yeah, so right now it's in draft.

01:02:35.600 --> 01:02:37.480
Like what's it looking like?

01:02:37.480 --> 01:02:41.440
If it'll be in 313, it'll be in 313 alpha something,

01:02:41.440 --> 01:02:42.520
some beta something.

01:02:42.520 --> 01:02:44.440
Like when is this gonna start looking like a thing

01:02:44.440 --> 01:02:46.920
that is ready for people to play with?

01:02:46.920 --> 01:02:50.520
- So I, yeah, this PEP, I went through

01:02:50.520 --> 01:02:53.600
and did a massive cleanup of PEP 554,

01:02:53.600 --> 01:02:55.280
which is why I made a new PEP for it,

01:02:55.280 --> 01:02:59.280
and simplified a lot of things, clarified a lot of points,

01:02:59.280 --> 01:03:01.400
had lots of good feedback from people,

01:03:01.400 --> 01:03:04.920
and ended up with what I think is a good API,

01:03:04.920 --> 01:03:07.040
but it was a little different in some ways.

01:03:07.040 --> 01:03:10.120
So I've had the implementation for PEP 554

01:03:10.120 --> 01:03:13.160
mostly done and ready to go for years.

01:03:13.160 --> 01:03:16.320
And so it was a matter, it's been a matter of

01:03:16.320 --> 01:03:19.160
now that I have this updated PEP up,

01:03:19.160 --> 01:03:20.800
going back to the implementation,

01:03:20.800 --> 01:03:22.440
tweaking it to match,

01:03:22.440 --> 01:03:26.320
and then making sure everything still feels right.

01:03:26.320 --> 01:03:28.520
Try and use it in a few cases,

01:03:28.520 --> 01:03:30.160
and if everything looks good,

01:03:30.160 --> 01:03:33.480
then go ahead and I'll start a discussion on that.

01:03:33.480 --> 01:03:35.320
I'm hoping within the next week or two

01:03:35.320 --> 01:03:38.520
to start up a round of discussion about this PEP,

01:03:38.520 --> 01:03:42.520
and hopefully we won't have a whole lot of back and forth

01:03:42.520 --> 01:03:44.960
so I can get this over to the steering councils

01:03:44.960 --> 01:03:46.120
in the near future.

01:03:46.120 --> 01:03:48.880
- Well, the hard work has been done already, right?

01:03:48.880 --> 01:03:53.040
The C layer is there and it's accepted and it's in there.

01:03:53.040 --> 01:03:55.080
Now it's just a matter of what's the right way

01:03:55.080 --> 01:03:56.960
to look at it from Python, right?

01:03:56.960 --> 01:03:59.280
- And one thing to keep in mind

01:03:59.280 --> 01:04:04.280
is that I'm planning on backporting the module to Python 3.12

01:04:04.280 --> 01:04:07.880
just so that we have a printer per gallon 3.12,

01:04:07.880 --> 01:04:10.800
so it'd be nice if people could really take advantage of it.

01:04:10.800 --> 01:04:13.120
- I see, so for that one, we'd have to pip install it

01:04:13.120 --> 01:04:14.560
or would it be added as?

01:04:14.560 --> 01:04:15.840
- Yeah, pip install.

01:04:15.840 --> 01:04:16.680
- Okay.

01:04:16.680 --> 01:04:19.520
- I probably won't support before 3.12.

01:04:19.520 --> 01:04:21.960
Subinterpreters have been around for decades,

01:04:21.960 --> 01:04:23.480
but only through the C API.

01:04:23.480 --> 01:04:28.480
But that said, I doubt I'll backport this module past 3.12.

01:04:29.000 --> 01:04:30.880
So just 3.12 and up.

01:04:30.880 --> 01:04:32.960
- And that's more than I expected anyway,

01:04:32.960 --> 01:04:33.840
so that's pretty cool.

01:04:33.840 --> 01:04:36.120
All right, final thoughts, you guys,

01:04:36.120 --> 01:04:38.360
what do you wanna tell people about this stuff?

01:04:38.360 --> 01:04:41.040
- Personally, I'm excited for where everything's going.

01:04:41.040 --> 01:04:42.640
It's taken a while,

01:04:42.640 --> 01:04:46.120
but I think we're getting to a good place.

01:04:46.120 --> 01:04:49.000
It's interesting with all the discussion about no-gil,

01:04:49.000 --> 01:04:51.880
it's easy to think, oh, then why do we need subinterpreters?

01:04:51.880 --> 01:04:54.960
Or if we have subinterpreters, why do we need no-gil?

01:04:54.960 --> 01:04:57.920
But they're kind of different needs, they're meeting.

01:04:57.920 --> 01:05:00.760
The most interesting thing for me is that

01:05:00.760 --> 01:05:03.800
what's good for no-gil is good for subinterpreters

01:05:03.800 --> 01:05:04.800
and vice versa.

01:05:04.800 --> 01:05:08.640
That no-gil probably really wouldn't be possible

01:05:08.640 --> 01:05:10.480
without a lot of the work that we've done

01:05:10.480 --> 01:05:13.040
to make a per-interpreter GIL possible.

01:05:13.040 --> 01:05:16.200
So I think that's one of the neat things,

01:05:16.200 --> 01:05:20.160
that the future's looking bright for Python multi-core.

01:05:20.160 --> 01:05:22.800
And I'm excited to see where people go

01:05:22.800 --> 01:05:24.600
with all these things that we're adding.

01:05:24.600 --> 01:05:27.960
- Anthony, when's the subinterpreter programming design

01:05:27.960 --> 01:05:29.240
patterns book coming out?

01:05:29.240 --> 01:05:33.560
- Yeah, my thoughts are,

01:05:33.560 --> 01:05:36.160
I will, there's a, subinterpreters are mentioned

01:05:36.160 --> 01:05:40.480
in my book actually, when it was like Python 3.9, I think.

01:05:40.480 --> 01:05:43.880
'Cause it was possible then,

01:05:43.880 --> 01:05:45.960
but it's changed quite a lot since.

01:05:45.960 --> 01:05:50.600
I guess some thoughts to leave people with,

01:05:50.600 --> 01:05:55.400
I think if you're a maintainer of a Python package

01:05:55.400 --> 01:05:58.040
or a C extension module in a Python package,

01:05:58.040 --> 01:06:01.240
there's gonna be a lot more exotic scenarios

01:06:01.240 --> 01:06:03.760
for you to test coming in the next year or so.

01:06:03.760 --> 01:06:09.480
And some of those uncover things that you might've done

01:06:09.480 --> 01:06:12.800
or just kind of relied on the GIL with global state,

01:06:12.800 --> 01:06:15.840
where that's not really desirable anymore

01:06:15.840 --> 01:06:18.080
and you're gonna get bugs down the line.

01:06:18.080 --> 01:06:20.560
So I think with any of that stuff as a package maintainer,

01:06:20.560 --> 01:06:22.880
you want to test as many scenarios as you can

01:06:22.880 --> 01:06:24.800
so that you can catch bugs and fix them

01:06:24.800 --> 01:06:27.120
before your users find them.

01:06:27.120 --> 01:06:28.640
So if you are a package maintainer,

01:06:28.640 --> 01:06:29.720
there's definitely some things

01:06:29.720 --> 01:06:32.320
that you can start to look at now to test

01:06:32.320 --> 01:06:37.320
in that's available in 3.12, 3.13 alpha two

01:06:37.320 --> 01:06:40.440
is at least probably the one I've tried to be honest.

01:06:40.440 --> 01:06:44.800
And if you're a developer, not necessarily a maintainer,

01:06:44.800 --> 01:06:48.480
then I think this is a good time to start reading up

01:06:48.480 --> 01:06:51.320
on like parallel programming

01:06:51.320 --> 01:06:54.960
and how you need to design parallel programs.

01:06:54.960 --> 01:06:57.040
And that those kinds of concepts

01:06:57.040 --> 01:06:59.360
are the same across all languages

01:06:59.360 --> 01:07:01.320
and Python would be no different.

01:07:01.320 --> 01:07:04.520
We just have different mechanisms for starting parallel work

01:07:04.520 --> 01:07:06.720
and joining it back together.

01:07:06.720 --> 01:07:08.040
But if you're interested in this

01:07:08.040 --> 01:07:10.880
and you want to run more code in parallel,

01:07:10.880 --> 01:07:13.200
and there's definitely some stuff to read

01:07:13.200 --> 01:07:18.200
and some stuff to learn about in terms of signals, pipes,

01:07:18.320 --> 01:07:22.920
queues, sharing data, how you have locks

01:07:22.920 --> 01:07:24.600
and where you should put them,

01:07:24.600 --> 01:07:27.240
how deadlocks can occur, things like that.

01:07:27.240 --> 01:07:28.960
So all of that stuff is the same in Python

01:07:28.960 --> 01:07:29.800
as anywhere else.

01:07:29.800 --> 01:07:32.200
We just have different mechanisms for doing it.

01:07:32.200 --> 01:07:35.320
- All right, well, people have some research work

01:07:35.320 --> 01:07:37.080
and I guess a really, really quick final question,

01:07:37.080 --> 01:07:39.040
Eric, and then we'll wrap this up.

01:07:39.040 --> 01:07:40.600
Following up on what Anthony said,

01:07:40.600 --> 01:07:41.600
like test your stuff,

01:07:41.600 --> 01:07:43.000
make sure it works in a sub-interpreter.

01:07:43.000 --> 01:07:44.240
If for some reason you're like,

01:07:44.240 --> 01:07:46.160
my code will not work in a sub-interpreter

01:07:46.160 --> 01:07:47.720
and I'm not ready yet,

01:07:47.720 --> 01:07:49.520
is there a way to determine that your code

01:07:49.520 --> 01:07:51.240
is being run in a sub-interpreter

01:07:51.240 --> 01:07:54.360
rather than regularly from your Python code?

01:07:54.360 --> 01:07:57.440
- Yeah, if you have an extension module

01:07:57.440 --> 01:07:59.720
that supports sub-interpreters,

01:07:59.720 --> 01:08:02.760
then you will have updated your module

01:08:02.760 --> 01:08:05.240
to use what's called multi-phase init.

01:08:05.240 --> 01:08:09.200
And that's something that shouldn't be too hard to look up.

01:08:09.200 --> 01:08:11.680
I think I talked about it in the pep.

01:08:11.680 --> 01:08:13.720
If you implement multi-phase init,

01:08:13.720 --> 01:08:16.040
then you've already done most of the work

01:08:16.040 --> 01:08:17.600
to support a sub-interpreter.

01:08:17.600 --> 01:08:20.520
The, if you haven't,

01:08:20.520 --> 01:08:24.000
then your module can't be imported in a sub-interpreter.

01:08:24.000 --> 01:08:26.440
It'll actually fail with an import error

01:08:26.440 --> 01:08:28.520
if you try and import it in a sub-interpreter

01:08:28.520 --> 01:08:31.840
or at least a sub-interpreter that has its own GIL.

01:08:31.840 --> 01:08:33.440
There are ways to create sub-interpreters

01:08:33.440 --> 01:08:35.800
that still share a GIL and that sort of thing.

01:08:35.800 --> 01:08:39.400
But you just won't be able to import it at all.

01:08:39.400 --> 01:08:41.840
So like the readline module

01:08:41.840 --> 01:08:44.200
can't be imported in sub-interpreters.

01:08:44.200 --> 01:08:46.800
The issue that Anthony ran into

01:08:46.800 --> 01:08:50.440
is kind of a subtle side effect

01:08:50.440 --> 01:08:52.280
of the check that we're doing.

01:08:52.280 --> 01:08:55.680
So, but really it boils down to

01:08:55.680 --> 01:08:58.200
if you don't implement multi-phase init,

01:08:58.200 --> 01:09:00.920
then you won't be able to import the module.

01:09:00.920 --> 01:09:02.560
You'll just get an importer.

01:09:02.560 --> 01:09:05.440
So that's, I mean, it makes it kind of straightforward.

01:09:05.440 --> 01:09:06.320
- Yeah, sounds good.

01:09:06.320 --> 01:09:08.000
More opt-in than opt-out.

01:09:08.000 --> 01:09:08.920
- Yep. - Right on.

01:09:08.920 --> 01:09:11.200
All right, guys, thank you both for coming back on the show

01:09:11.200 --> 01:09:13.400
and awesome work.

01:09:13.400 --> 01:09:16.040
This is looking close to the finish line and exciting.

01:09:16.040 --> 01:09:17.880
- Thanks, Michael. - Yep, see y'all.

01:09:18.720 --> 01:09:22.000
- This has been another episode of "Talk Python to Me."

01:09:22.000 --> 01:09:23.440
Thank you to our sponsors.

01:09:23.440 --> 01:09:24.800
Be sure to check out what they're offering.

01:09:24.800 --> 01:09:26.920
It really helps support the show.

01:09:26.920 --> 01:09:29.800
Are you ready to level up your Python career?

01:09:29.800 --> 01:09:32.200
And could you use a little bit of personal

01:09:32.200 --> 01:09:34.800
and individualized guidance to do so?

01:09:34.800 --> 01:09:38.320
Check out the PyBytes Python Developer Mindset program

01:09:38.320 --> 01:09:42.080
at talkpython.fm/pdm.

01:09:42.080 --> 01:09:43.680
Take some stress out of your life.

01:09:43.680 --> 01:09:45.880
Get notified immediately about errors

01:09:45.880 --> 01:09:47.840
and performance issues in your web

01:09:47.840 --> 01:09:49.760
or mobile applications with Sentry.

01:09:49.760 --> 01:09:54.640
Just visit talkpython.fm/sentry and get started for free.

01:09:54.640 --> 01:09:58.680
And be sure to use the promo code talkpython, all one word.

01:09:58.680 --> 01:10:00.000
Want to level up your Python?

01:10:00.000 --> 01:10:01.760
We have one of the largest catalogs

01:10:01.760 --> 01:10:04.120
of Python video courses over at Talk Python.

01:10:04.120 --> 01:10:06.200
Our content ranges from true beginners

01:10:06.200 --> 01:10:09.160
to deeply advanced topics like memory and async.

01:10:09.160 --> 01:10:11.840
And best of all, there's not a subscription in sight.

01:10:11.840 --> 01:10:15.080
Check it out for yourself at training.talkpython.fm.

01:10:15.080 --> 01:10:16.840
Be sure to subscribe to the show.

01:10:16.840 --> 01:10:19.760
Open your favorite podcast app and search for Python.

01:10:19.760 --> 01:10:21.000
We should be right at the top.

01:10:21.000 --> 01:10:24.040
You can also find the iTunes feed at /itunes,

01:10:24.040 --> 01:10:26.200
the Google Play feed at /play,

01:10:26.200 --> 01:10:30.480
and the Direct RSS feed at /rss on talkpython.fm.

01:10:30.480 --> 01:10:33.280
We're live streaming most of our recordings these days.

01:10:33.280 --> 01:10:34.520
If you want to be part of the show

01:10:34.520 --> 01:10:36.720
and have your comments featured on the air,

01:10:36.720 --> 01:10:38.640
be sure to subscribe to our YouTube channel

01:10:38.640 --> 01:10:41.440
at talkpython.fm/youtube.

01:10:41.440 --> 01:10:43.200
This is your host, Michael Kennedy.

01:10:43.200 --> 01:10:44.400
Thanks so much for listening.

01:10:44.400 --> 01:10:45.640
I really appreciate it.

01:10:45.640 --> 01:10:48.200
Now get out there and write some Python code.

01:10:48.200 --> 01:10:50.800
(upbeat music)

01:11:04.280 --> 01:11:08.280
[MUSIC]

