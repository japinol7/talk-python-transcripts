WEBVTT

00:00:00.001 --> 00:00:06.320
I have a special episode for you this time around. We're coming to you live from PyCon 2024. I had

00:00:06.320 --> 00:00:11.600
the chance to sit down with some amazing people from the data science side of things. Jody Burchell,

00:00:11.600 --> 00:00:18.480
Maria Jose Molina Contreras, and Jessica Green. We cover a whole set of recent topics from a data

00:00:18.480 --> 00:00:23.360
science perspective. Though we did have to cut the conversation a bit short as they were coming

00:00:23.360 --> 00:00:28.080
from and going to talks they were all giving, but it's still a pretty deep conversation. I know you'll

00:00:28.080 --> 00:00:36.000
enjoy it. This is Talk Python to Me, episode 467 recorded on location in Pittsburgh on May 18th, 2024.

00:00:36.000 --> 00:00:42.800
Are you ready for your host, Darius? You're listening to Michael Kennedy on Talk Python to Me.

00:00:42.800 --> 00:00:46.160
Live from Portland, Oregon, and this segment was made with Python.

00:00:46.160 --> 00:00:54.800
Welcome to Talk Python to Me, a weekly podcast on Python. This is your host, Michael Kennedy.

00:00:54.800 --> 00:01:00.000
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,

00:01:00.000 --> 00:01:06.000
both on fosstodon.org. Keep up with the show and listen to over seven years of past episodes at

00:01:06.000 --> 00:01:11.840
talkpython.fm. We've started streaming most of our episodes live on YouTube. Subscribe to our

00:01:11.840 --> 00:01:17.840
YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be part of

00:01:17.840 --> 00:01:22.960
that episode. This episode is brought to you by Sentry. Don't let those errors go unnoticed. Use

00:01:22.960 --> 00:01:29.680
Sentry like we do here at Talk Python. Sign up at talkpython.fm/sentry. And it's brought to you by

00:01:29.680 --> 00:01:34.880
Code Comments, an original podcast from Red Hat. This podcast covers stories from technologists

00:01:34.880 --> 00:01:40.960
who've been through tough tech transitions and share how their teams survived the journey.

00:01:40.960 --> 00:01:46.960
Episodes are available everywhere you listen to your podcast and at talkpython.fm/code-comments.

00:01:47.680 --> 00:01:53.360
Hello from PyCon. Hello, Jessica, Jody, Maria. Welcome to Talk Python to Me. It's awesome to

00:01:53.360 --> 00:01:58.560
have you all here. And I'm looking forward to talking about data science, some fun LLM questions

00:01:58.560 --> 00:02:03.440
maybe, some controversial questions, some data science tools, all sorts of good things. Of course,

00:02:03.440 --> 00:02:09.520
before we get to that, Jody, you've been on the show a time or two, and people may know you, but

00:02:09.520 --> 00:02:13.520
maybe not. So how about a quick introduction, what you all are into? Maria, you want to start?

00:02:13.520 --> 00:02:20.880
Oh, okay. Well, my name is Maria. I am originally from Barcelona, but I am based in Berlin. I work

00:02:20.880 --> 00:02:29.600
as a data scientist in a small startup that we are trying to solve some sustainability problems.

00:02:29.600 --> 00:02:31.680
And yeah, that is me. Excellent.

00:02:31.680 --> 00:02:36.320
Yeah. So my name's Jody and I am a data science developer advocate. I've been working in data

00:02:36.320 --> 00:02:39.920
science for about eight years. And yeah, I'm currently working at JetBrains, as you can see

00:02:39.920 --> 00:02:48.080
from the shirt. And the background. And so I'd say my interest at the moment is natural language

00:02:48.080 --> 00:02:52.720
processing, because I worked in that a big chunk of my career, but core statistics will always be

00:02:52.720 --> 00:02:56.800
my love. So tabular data, I'm there for you always. Beautiful.

00:02:56.800 --> 00:03:02.720
Yeah. My name's Jessica. So I'm an ML engineer at Coja, which is the search engine for a better

00:03:02.720 --> 00:03:08.720
planet. I am actually a career changer. So I used to roast coffee for a living, and I really just

00:03:08.720 --> 00:03:14.960
got into this field in the last six years. So I don't have like any formal training. I'm a community

00:03:14.960 --> 00:03:20.800
slash self-taught engineer. And I went through more of a like a backend focused path. And now

00:03:20.800 --> 00:03:24.080
I've started to work in the ML realm. So really exciting.

00:03:24.080 --> 00:03:27.760
Yeah. Very, very interesting. Another thing I absolutely love is coffee.

00:03:27.760 --> 00:03:32.560
I think we're running on it at PyCon.

00:03:32.560 --> 00:03:38.080
Pretty much. We are. Yeah. We're getting farther into the show and more coffee is needed.

00:03:38.640 --> 00:03:44.080
But I do want to ask you, what do you think about being in the data science space? That's a really

00:03:44.080 --> 00:03:49.200
different world that interacting with people all day and working with your hands more or whatever.

00:03:49.200 --> 00:03:54.400
Yeah. How has it been with this switch? There is a lot of synergies actually, when you're

00:03:54.400 --> 00:03:57.600
stood behind the Espresso machine and you're getting all the orders in and then you need to

00:03:57.600 --> 00:04:03.280
like problem solve to like how you get everyone their correct order to the way that they like it.

00:04:04.000 --> 00:04:09.040
So there was a lot of transferable skills, I will say. But I think what I found really powerful,

00:04:09.040 --> 00:04:16.080
especially maybe learning at this specific period of time, is how accessible a lot of the tools are

00:04:16.080 --> 00:04:22.400
today. So like how, I wouldn't say easy because I put a lot of hard work into it, but like how

00:04:22.400 --> 00:04:26.400
possible it is, even with a background like mine to get into the field.

00:04:26.400 --> 00:04:31.920
Awesome. I switched, I didn't have a formal education either. I took two computer college

00:04:31.920 --> 00:04:37.360
courses just because they match, you know, I needed for something else. And yeah, I thought,

00:04:37.360 --> 00:04:42.880
I think you can completely succeed here teaching yourself. There's so many resources. Honestly,

00:04:42.880 --> 00:04:46.800
the problem is what resources do you choose to learn these days, right? You can spend all your

00:04:46.800 --> 00:04:50.560
time while I'm doing another tutorial, I'm doing another class, like some point you got to start

00:04:50.560 --> 00:04:55.840
doing something, right? Yeah. And I think actually it felt like that probably when we all started.

00:04:55.840 --> 00:05:01.200
So data science was just getting hot when I started and oh my God, back when I started,

00:05:01.200 --> 00:05:05.440
this is how long ago it was. There were actually like those articles like R versus Python,

00:05:05.440 --> 00:05:08.960
like there's no conversations anyone's having anymore, but they have similar conversations.

00:05:08.960 --> 00:05:13.600
And I think it makes it super difficult for beginners because the field felt inaccessible,

00:05:13.600 --> 00:05:18.640
I think, eight years ago. The field feels very hostile to beginners right now, I think,

00:05:18.640 --> 00:05:24.720
because of the AI hype. I don't actually think the field has changed that much in fundamentals.

00:05:24.720 --> 00:05:29.680
It's just NLP has become a bigger thing and computer vision recently, but we can get into that.

00:05:30.400 --> 00:05:36.880
Yeah, I completely agree with half of you. To be honest, for me, data science is super broad

00:05:36.880 --> 00:05:44.640
world full of a lot of things that are kind of popping up, doing different evolution during time.

00:05:44.640 --> 00:05:52.880
And it's so interesting to see the evolution in the last eight years. I started eight years ago in

00:05:52.880 --> 00:05:59.680
data science. And I remember when how I was doing things eight years ago, and how I'm doing things

00:05:59.680 --> 00:06:07.440
now. And I love it. I love to see this progression. And I am pretty sure that in eight more years,

00:06:07.440 --> 00:06:13.600
we're gonna be in something completely different. And so I totally agree with that.

00:06:13.600 --> 00:06:19.440
I do. And I also think data science is interesting because coming into it, you can be a data

00:06:19.440 --> 00:06:24.720
scientist, but because some other reason, right, I could be a data science because I'm interested in

00:06:24.720 --> 00:06:29.760
biology or sustainability or something. Whereas you're if you're a web developer, or you build

00:06:29.760 --> 00:06:35.760
API's, or you optimize, you know, whatever, you, you're more focused on I care about the thing at

00:06:35.760 --> 00:06:40.400
the code itself, rather than I'm trying to I care about that. And this is a tool to address that.

00:06:40.400 --> 00:06:41.280
Yeah, yeah.

00:06:41.280 --> 00:06:44.640
Yeah, actually, I was gonna say, I met a bioinformatician yesterday,

00:06:44.640 --> 00:06:48.560
like, that's also a data scientist, like someone who works in genetic data.

00:06:48.560 --> 00:06:54.560
Yeah, absolutely. I had a comment from I did a show recently from about how Python's used in

00:06:54.560 --> 00:06:58.640
neurology labs, right? And somebody wrote me, this is my favorite episode. It speaks to me.

00:06:58.640 --> 00:07:02.400
I'm also a neurologist. Like, it's really cool. All right, we're looking out,

00:07:02.400 --> 00:07:08.240
kind of the backside of the little bit. We're looking at the expo hall here at PyCon. So

00:07:08.240 --> 00:07:13.840
I don't know about you all feel but for me, this is like my geek holiday, I get to come here. And

00:07:13.840 --> 00:07:19.120
it's really special to me because I get to see my friends who I've collaborated with projects on,

00:07:19.120 --> 00:07:24.880
and I admire and I've worked with, but I might never see them outside of this week, you know,

00:07:24.880 --> 00:07:31.040
maybe they live in Australia or Europe or some oddly just down the street. And yet still,

00:07:31.040 --> 00:07:36.080
I don't see them except here. So maybe what are your thoughts on PyCon here?

00:07:36.080 --> 00:07:41.600
It's my first time attending. So I'm super stoked, I have to say like, it's slightly

00:07:41.600 --> 00:07:46.000
overwhelming, because there's so many things going on. And like you mentioned, the opportunity to

00:07:46.000 --> 00:07:50.880
meet so many folks that I either already knew in some capacity, but had never met or didn't meet

00:07:50.880 --> 00:07:55.520
before, but have heard of their work. So yeah, it's been a real honor to be here, right? And get

00:07:55.520 --> 00:08:00.480
to I mean, we are all based in Berlin. So we do actually know each other. But it's also a pleasure

00:08:00.480 --> 00:08:06.560
just to come away on a geek holiday with friends. Yeah, and we were actually all just at PyCon DE

00:08:06.560 --> 00:08:12.000
just before this, like a month ago. Yeah, well, yeah, it's it's a different scale. Let's put it

00:08:12.000 --> 00:08:16.160
that way. But I think it's a similar feel like one thing that I value so much about the Python

00:08:16.160 --> 00:08:22.400
community is that it's community. And I'm very lucky to have gotten involved in a program called

00:08:22.400 --> 00:08:28.720
Hatchery, which you two have also been involved in. It's a Hatchery we're running is Humble Data.

00:08:28.720 --> 00:08:34.400
And what I like is this program got accepted at a Python conference, which is designed for people

00:08:34.400 --> 00:08:39.280
who have never coded and who are career changers, because I'm also a career changer from academia.

00:08:39.280 --> 00:08:44.800
And this is what makes I think Python special, the community and I think the PyCons are an

00:08:44.800 --> 00:08:51.680
absolute representation of that. Yeah, absolutely. For me, it's the same feeling I love to go to

00:08:51.680 --> 00:08:59.680
different conferences of PyCon. And because we have a lot of things in common. But also,

00:08:59.680 --> 00:09:07.360
we have differences and the different conferences bring a different point of value. And I think it's

00:09:07.360 --> 00:09:13.520
awesome. I came here and meet friends that this is my third time in here. And I'm super, super

00:09:13.520 --> 00:09:20.000
excited and happy. And I'm super eager to next year. And also the Python in Espanol. Yeah,

00:09:20.000 --> 00:09:26.640
of course. And also we have even here we have a track that is PyCon charlas to be even more

00:09:26.640 --> 00:09:32.000
welcoming to different people from different communities. And it's just amazing. It's super

00:09:32.000 --> 00:09:36.640
nice, to be honest. Awesome. Yeah, I definitely want to encourage people out there listening

00:09:36.640 --> 00:09:43.680
who feel like oh, I'm not high enough of a level of Python. I know to come. I'm not ready for PyCon.

00:09:43.680 --> 00:09:47.520
I believe last year, I haven't heard any numbers this year. I believe last year 50% of the

00:09:47.520 --> 00:09:53.040
attendees were first time attendees. And I think that's generally true. A lot of times people are

00:09:53.040 --> 00:09:57.280
it's their first time coming and yeah, it's I think you can get a lot out of it even if you're

00:09:57.280 --> 00:10:02.240
not super advanced, maybe even more so than if you are super advanced. I definitely have had

00:10:02.240 --> 00:10:07.520
the opportunity like the honor, I would actually say to like listen into conversations around

00:10:07.520 --> 00:10:12.640
topics that I find interesting, but aren't part of my day to day work. And it's just like general

00:10:12.640 --> 00:10:18.000
vibe that whether it's at lunch or during the breaks or after a talk, you get to partake in

00:10:18.000 --> 00:10:23.680
these conversations, which ultimately will advance you. So if you also want to get sponsoring,

00:10:23.680 --> 00:10:28.160
right, like a lot of people need their work to sponsor them. I think there's a lot of reasoning

00:10:28.160 --> 00:10:33.280
behind asking for PyCon as a conference because there's so much value. Jessica, that's a great

00:10:33.280 --> 00:10:38.640
point. And I think also, I was talking to someone earlier about how much more affordable this is

00:10:38.640 --> 00:10:43.360
than a lot of tech conferences. A lot of them are like, how many thousand dollars is just the ticket?

00:10:43.360 --> 00:10:50.000
And this is not that cheap, but it's relatively cheap compared. So I was going to say you could

00:10:50.000 --> 00:10:55.760
do a plug for EuroPython while you're here. We have also the option to have grants. There is a

00:10:56.400 --> 00:11:02.880
different programs, PyLadiesGrants or the conference organizers grants. Also, this is

00:11:02.880 --> 00:11:10.000
something that could help people to try to apply or come here. Yeah, they mentioned that at the

00:11:10.000 --> 00:11:16.240
opening keynote or the introductions before the keynote. It's some significant number of grants

00:11:16.240 --> 00:11:19.680
that were given. I can't remember the number, but it's like half a million dollars or something

00:11:19.680 --> 00:11:24.880
in grants. Was that what it was? I think it was around that scale. Yeah. Yeah. Yeah. It's a really

00:11:24.880 --> 00:11:29.760
big deal. And I suppose all three of you being from Berlin, we should say generally the same

00:11:29.760 --> 00:11:34.720
stuff applies to EuroPython as well, I imagine. Right? Yeah. So if you're in Europe, the biggest

00:11:34.720 --> 00:11:39.840
deal is to get all the way to the US, maybe go to EuroPython as well, which would be fun. Yeah.

00:11:39.840 --> 00:11:44.320
Or something more local. This portion of Talk Python to Me is brought to you by

00:11:44.320 --> 00:11:50.880
OpenTelemetry support at Sentry. In the previous two episodes, you heard how we use Sentry's error

00:11:50.880 --> 00:11:56.480
monitoring at Talk Python, and how distributed tracing connects errors, performance and slowdowns

00:11:56.480 --> 00:12:03.200
and more across services and tiers. But you may be thinking, our company uses OpenTelemetry. So

00:12:03.200 --> 00:12:08.960
it doesn't make sense for us to switch to Sentry. After all, OpenTelemetry is a standard, and you've

00:12:08.960 --> 00:12:14.320
already adopted it, right? Did you know, with just a couple of lines of code, you can connect

00:12:14.320 --> 00:12:20.080
OpenTelemetry's monitoring and reporting to Sentry's backend. OpenTelemetry does not come

00:12:20.080 --> 00:12:26.000
with a backend to store your data, analytics on top of that data, a UI or error monitoring. And

00:12:26.000 --> 00:12:31.760
that's exactly what you get when you integrate Sentry with your OpenTelemetry setup. Don't fly

00:12:31.760 --> 00:12:38.160
blind, fix and monitor code faster with Sentry. Integrate your OpenTelemetry systems with Sentry

00:12:38.160 --> 00:12:44.640
and see what you've been missing. Create your Sentry account at talkpython.fm/sentry-telemetry.

00:12:44.640 --> 00:12:50.320
And when you sign up, use the code TALKPYTHON, all caps, no spaces. It's good for two free months of

00:12:50.320 --> 00:12:55.360
Sentry's business plan, which will give you 20 times as many monthly events as well as other

00:12:55.360 --> 00:13:01.040
features. My thanks to Sentry for supporting Talk Python and me. - Jody, you have been on the

00:13:01.040 --> 00:13:07.520
receiving end of many, many questions and you've been, let's see here, doing demos, sworn with

00:13:07.520 --> 00:13:12.080
people for a day and a half. I'm surprised you still have your voice. - I've got to give a talk

00:13:12.080 --> 00:13:20.000
in two hours too, so I hope I have a voice. - Speak quietly. Save a little bit for that. One

00:13:20.000 --> 00:13:24.640
of the questions you said was that people are still just have core data science questions.

00:13:24.640 --> 00:13:29.280
They're not necessarily trying to figure out how LLMs are going to change the world, but how do

00:13:29.280 --> 00:13:34.080
you do that with pandas or whatever? What are your thoughts on this? What are your takeaways? - So I

00:13:34.080 --> 00:13:37.520
alluded to the fact I have an academic background. I've probably talked about this on the last

00:13:37.520 --> 00:13:44.560
podcast, but basically my background is in behavioral sciences, so a lot of core statistics

00:13:44.560 --> 00:13:50.800
and working with what's called tabular data, data in tables. And pretty much I would say, look, this

00:13:50.800 --> 00:13:56.400
is a guesstimate. This is not scientific, but my kind of gut feeling, PyCon after PyCon, conference

00:13:56.400 --> 00:14:01.600
after conference that I do, I think like 80% of people are probably still doing this stuff because

00:14:01.600 --> 00:14:05.840
business questions are not necessarily solved with the cutting edge. Business questions are

00:14:05.840 --> 00:14:11.760
solved with the simplest possible models that will address your needs. I think we talked about this in

00:14:11.760 --> 00:14:17.600
the last podcast. So like for an example, my last job, we had to deal with low latency systems, like

00:14:17.600 --> 00:14:23.200
very low latency. So we used a decision tree to solve the problem. Decision tree is a very old

00:14:23.200 --> 00:14:28.480
algorithm. It's not sexy anymore, but everyone's secretly still using it. And so yeah, some people

00:14:28.480 --> 00:14:35.920
are doing cutting edge LLM stuff. But my feeling is this is a technology that maybe has more interest

00:14:35.920 --> 00:14:43.280
than real profitable applications, because these are expensive models to run and deploy and to set

00:14:43.280 --> 00:14:48.000
up reliable pipelines for. Yeah. My feeling is, gut feeling is a lot of people are still just

00:14:48.000 --> 00:14:53.440
doing boring linear regression, which I will defend until the day I die. My favorite algorithm.

00:14:53.440 --> 00:14:54.400
Amazing. Yeah.

00:14:54.400 --> 00:15:00.160
Yeah. And I mean, I think we've seen that in our work as well is we don't per se need the biggest,

00:15:00.160 --> 00:15:06.240
fanciest thing. We need something that works and provides users with useful information. I think

00:15:06.240 --> 00:15:11.040
there's also still a lot of problems with large language models, like Simon alluded to in the

00:15:11.040 --> 00:15:17.360
keynote today around security. So if you want to put this into a product, it's still kind of early

00:15:17.360 --> 00:15:23.600
days, but I don't think those base kind of NLP techniques are going to go away anytime soon.

00:15:23.600 --> 00:15:28.000
And I think like we spoke about learners earlier and people coming into the field,

00:15:28.000 --> 00:15:33.840
there's still a huge amount of value just to go and learn this core aspects that will serve

00:15:33.840 --> 00:15:38.720
you really well. Absolutely. Way more than LLMs and AIs and all that stuff.

00:15:38.720 --> 00:15:43.600
You can use a LLM to learn it. That's what we just saw in the keynote.

00:15:43.600 --> 00:15:49.280
Yeah, absolutely. And I also think what people are going to do with LLMs and stuff like that,

00:15:49.280 --> 00:15:53.200
ask it to help give me this little bit of code or that bit of code, but you're going to need to be

00:15:53.200 --> 00:15:56.880
able to look at it and say, yeah, that does make sense. Yeah, that does fit in. And so you need to

00:15:56.880 --> 00:16:03.040
know that's a reasonable use of pandas. What do you think, Maria? I completely agree. The LLM's

00:16:03.040 --> 00:16:09.200
role is kind of complex. I think that it has a lot of potential. And I think that a lot of people

00:16:09.200 --> 00:16:15.360
could see this potential and everyone is getting very excited and even a bit in a hype because of

00:16:15.360 --> 00:16:22.880
that. However, it has a lot of limitations still nowadays, I can tell you, because I am currently

00:16:22.880 --> 00:16:30.960
working with LLMs for solving the real world problems that we were mentioning about the

00:16:30.960 --> 00:16:37.680
sustainable packaging. And it's very challenging, to be honest. It's more challenging that people

00:16:37.680 --> 00:16:42.640
are mentioning. It's not only hallucinations, it's hallucinations, of course. But also,

00:16:42.640 --> 00:16:47.840
if you are doing fine tuning models, also you're going to later on need to think how you're going

00:16:47.840 --> 00:16:55.120
to deploy that, how much is going to cost you the inference of that, how it's going to cost in

00:16:55.120 --> 00:17:06.160
terms of electricity price, CO2, print, and long, etc. I think that we are in the process.

00:17:06.160 --> 00:17:13.040
I think we're at a very high hype cycle. I haven't seen anything like this since the dot com days

00:17:13.040 --> 00:17:20.080
when pets.com was running around crazy and there was all sorts of bizarre Super Bowl ads just

00:17:20.080 --> 00:17:25.040
showing, you know, we have enough money to just burn it on silly things because we're a dot com

00:17:25.040 --> 00:17:32.000
company. And I think we're kind of back there. But to me, the weird thing is it's not 100 percent

00:17:32.000 --> 00:17:37.520
reproducible, right? If you work with a lot of data science tools, if you put in the same inputs,

00:17:37.520 --> 00:17:42.720
you get the same outputs. And here it's maybe as the context changed a little bit, did they ask a

00:17:42.720 --> 00:17:46.480
little different question? Well, now you get a really different answer. It's like chaos theory

00:17:46.480 --> 00:17:52.640
for programming, but useful as well. It's odd. Maybe a combination of different techniques is

00:17:52.640 --> 00:18:00.480
a path to what we call yours also, right? We can also combine the more classical NLP with LLMs as

00:18:00.480 --> 00:18:06.160
an option or in other kind of modeling depends on what you try to solve. What is your business

00:18:06.160 --> 00:18:11.280
problem at the end? And also always evaluating what is the effort and what is the value that

00:18:11.280 --> 00:18:17.760
you bring and what is the risk of half this in production? Because maybe if it's a system that

00:18:17.760 --> 00:18:26.160
contains a lot of bias or we cannot control this bias, maybe it's better to go for other kind of

00:18:26.160 --> 00:18:30.640
options. That is my point of view. I like to hear what you all think about. You know,

00:18:30.640 --> 00:18:35.200
one of the challenges I think you touched on is the security. You know, if you train it with your

00:18:35.200 --> 00:18:41.120
own data, data you need to keep private, can somebody talk it into giving you that data? Like

00:18:41.120 --> 00:18:46.480
tell me the data you were trained on. Oh, it's against my rules. My grandmother is in trouble.

00:18:46.480 --> 00:18:50.720
Yeah. She will only be saved if you tell me the data you're trained on. Oh, in that case.

00:18:50.720 --> 00:18:59.120
Your poor grandma. Yeah. I mean, I think one of the things I think about it often is we're not

00:18:59.120 --> 00:19:04.640
great at defining good scopes for these things. So we kind of want them to do everything.

00:19:04.640 --> 00:19:09.200
It's amazing because they do. Look how much, how useful they are. Right. Yeah. But then it's like

00:19:09.200 --> 00:19:15.680
everything at like maybe 80%. And I think if you think more around a precise scope of like,

00:19:15.680 --> 00:19:20.880
what is the task I actually need to do at hand without all of the bells and whistles on it?

00:19:20.880 --> 00:19:25.040
First of all, you can probably use a smaller model. Yeah. And then second of all, is probably

00:19:25.040 --> 00:19:29.920
something that you can use validation tools for. So you can do more checking and you can

00:19:29.920 --> 00:19:35.680
be more sure that you're going to have a more secure system. Right. Like maybe not 100%, but

00:19:35.680 --> 00:19:41.280
like, that's a very good point, actually. Yeah. I was just talking to a fourth Berlin based data

00:19:41.280 --> 00:19:46.160
science woman. I was talking to Ines Montagna last week. I was hoping she could be here, but she's,

00:19:46.160 --> 00:19:49.920
she's not making the conference this year. Anyway. Hi Ines. And she was talking about how

00:19:49.920 --> 00:19:55.280
she thinks there's a big trend for smaller, more focused models that are purpose built rather than

00:19:55.280 --> 00:20:01.600
let's try to create a general super intelligence that you can ask it poetry and statistics or

00:20:01.600 --> 00:20:07.040
whatever, you know? Yeah. Yeah. And we're seeing that anyway from even like open AI and so forth

00:20:07.040 --> 00:20:13.120
with GPTs that they're also picking up on the fact that like narrowing slightly the context

00:20:13.120 --> 00:20:18.800
actually helps a lot. So I think this is very relevant for people in this working in this field

00:20:18.800 --> 00:20:23.440
to really think about what they want to do with it. Not just being like, I need to have this thing.

00:20:23.440 --> 00:20:30.000
I don't know. Yeah. And it's also, so Ines is old school NLP. Like she's been working in this for so

00:20:30.000 --> 00:20:35.520
long. And so Ines is one of the creators of spacey, which is like one of the most sophisticated,

00:20:35.520 --> 00:20:41.920
I think, general purpose NLP packages in Python. And I remember back when I had like a job where

00:20:41.920 --> 00:20:46.800
I did NLP for three years on search engine improvements, like this was the sort of stuff

00:20:46.800 --> 00:20:50.960
you were doing, like things about like, okay, it seems kind of quaint now, but it's still

00:20:50.960 --> 00:20:55.440
really important. Like how can you clean your data effectively? And it's very complex when it

00:20:55.440 --> 00:21:01.520
comes to tech stuff. And so, yeah, like Ines, of course she's completely right, but she's seen all

00:21:01.520 --> 00:21:05.760
of this. She knows where this is going. Yeah, absolutely. Absolutely. Let's touch on some

00:21:05.760 --> 00:21:12.240
tools. I know Maria, you had some interesting ones, just general data science tools that while

00:21:12.240 --> 00:21:17.760
people are listening, you should be like, let's check the LLM or as Jody puts it old school,

00:21:17.760 --> 00:21:24.880
just core data science. Yeah, yeah. It's going to depend on what kind of problem you want to solve.

00:21:24.880 --> 00:21:31.920
Again, it's like, it's not the tool. This is my perspective. It's not only one tool or 10 tools.

00:21:31.920 --> 00:21:37.840
It depends on your problem. And depends on your problem, we have tools that are going to help us

00:21:37.840 --> 00:21:45.680
more or easier than others. For instance, some tools that I'm using currently, just for giving

00:21:45.680 --> 00:21:55.840
you an example, this LangChain or Discord and yeah, and they are two open source libraries.

00:21:55.840 --> 00:22:03.920
LangChain is more focusing in that chat system in case that you want to develop a chat system or

00:22:03.920 --> 00:22:10.800
of course has a lot of more applications because LangChain is super useful also for handling all the

00:22:11.440 --> 00:22:16.640
large language models. Yeah. There's some cool boosts here that are boosted with cool products

00:22:16.640 --> 00:22:20.720
based on LangChain as well. Oh, really? I'm going to take a look.

00:22:20.720 --> 00:22:27.440
That then you export as a Python application. It's very neat. Anyway. Very good. Yeah. But you

00:22:27.440 --> 00:22:34.880
also said Discord. Yeah. G I S K R D. Exactly. Okay. It's the one that has a turtle, the logo,

00:22:34.880 --> 00:22:43.600
very cute. These people is developing a library for evaluating the models. Try to take a look

00:22:43.600 --> 00:22:53.280
in the bias of the system, has tests, test your models and generate metrics to help you understand

00:22:53.280 --> 00:22:59.760
if the model that you are using or training or fine tuning is something that you can trust or

00:22:59.760 --> 00:23:06.000
not or you need to re-evaluate or restart the system or whatever you need to do. I think these

00:23:06.000 --> 00:23:13.760
kind of libraries are super necessary, especially right now that the still it's very young, the

00:23:13.760 --> 00:23:19.040
field. And I think that they are very, very important. This portion of Talk Python to Me

00:23:19.040 --> 00:23:23.920
is brought to you by Code Comments, an original podcast from Red Hat. You know, when you're

00:23:23.920 --> 00:23:29.040
working on a project and you leave behind a small comment in the code, maybe you're hoping to help

00:23:29.040 --> 00:23:35.200
others learn what isn't clear at first. Sometimes that code comment tells a story of a challenging

00:23:35.200 --> 00:23:40.880
journey to the current state of the project. Code Comments, the podcast features technologists who

00:23:40.880 --> 00:23:46.160
have been through tough tech transitions, and they share how their teams survived that journey.

00:23:46.160 --> 00:23:51.920
The host, Jamie Parker is a Red Hatter and an experienced engineer. In each episode, Jamie

00:23:51.920 --> 00:23:57.680
recounts the stories of technologists from across the industry who've been on a journey implementing

00:23:57.680 --> 00:24:02.480
new technologies. I recently listened to an episode about DevOps from the folks at Worldwide

00:24:02.480 --> 00:24:08.320
Technology. The hardest challenge turned out to be getting buy in on the new tech stack rather than

00:24:08.320 --> 00:24:13.280
using that tech stack directly. It's a message that we can all relate to. And I'm sure you can

00:24:13.280 --> 00:24:19.040
take some hard one lessons back to your own team. Give Code Comments a listen. Search for Code

00:24:19.040 --> 00:24:26.080
Comments in your podcast player or just use our link, talkpython.fm/code-comments. The link is in

00:24:26.080 --> 00:24:31.120
your podcast players show notes. Thank you to Code Comments and Red Hat for supporting Talk Python to

00:24:31.120 --> 00:24:38.480
me. -Jerry? -Yeah, so maybe I'm going to do a little plug for my talk. So when I was doing psychology,

00:24:38.480 --> 00:24:43.280
I was fascinated by psychometrics. And what you learn when you learn psychometrics is

00:24:43.280 --> 00:24:49.920
measurement captures one specific thing, and you need to be very clear about what it captures. And

00:24:49.920 --> 00:24:55.760
so at the moment, we're seeing a lot of leaderboards to help people evaluate LLM performance, but also

00:24:56.080 --> 00:25:00.480
things like hallucination rates or things like bias and toxicity. What we need to understand is

00:25:00.480 --> 00:25:05.760
these things have extremely specific definitions. So in my talk, I'm going to be delving into a

00:25:05.760 --> 00:25:10.480
package which I do, a package, sorry, a measurement that I love called Truthful QA. But Truthful QA is

00:25:10.480 --> 00:25:15.920
designed to measure a specific type of hallucinations in English-speaking communities,

00:25:15.920 --> 00:25:22.000
because it assesses incorrect facts, things like misconceptions, misinformation, conspiracies.

00:25:22.000 --> 00:25:26.160
They're not going to be present in other languages. And so it's not as easy as looking at,

00:25:26.160 --> 00:25:31.200
okay, this model has a low hallucination rate. What does that mean? Or this model has good

00:25:31.200 --> 00:25:36.480
performance. Does it have that performance in your domain? How did they assess that? So it's very

00:25:36.480 --> 00:25:41.600
boring, but actually it's not because measurement is super sexy. You need to think about this stuff.

00:25:41.600 --> 00:25:46.320
It's really interesting, but it's challenging and it requires a lot of hard graph from you.

00:25:46.320 --> 00:25:52.720
Awesome. And while people will be watching this in the future, after your talk is out, that talk

00:25:52.720 --> 00:25:56.320
will be on YouTube, right? Yes, it'll be recorded. Yeah. So people can check out your talk. What's

00:25:56.320 --> 00:26:01.200
the title? Lies, Damn Lies and Large Language Models. Oh, I love it. It's the best title I've

00:26:01.200 --> 00:26:08.160
ever come up with. That is a good title. I love it. Jessica, tools, libraries, packages? Maybe

00:26:08.160 --> 00:26:14.720
I'll plug my tutorial that was two days ago and we'll also be recording somewhere at some point.

00:26:14.720 --> 00:26:20.080
We were working on looking at monitoring and observability of Python applications,

00:26:20.080 --> 00:26:27.600
which could well be your AI, LLM kind of thing. And we're using a package called Code Carbon.

00:26:27.600 --> 00:26:35.920
So it measures the carbon emissions of your code, of your workload. So this is one way that we can

00:26:35.920 --> 00:26:41.840
start to kind of get an idea of the impact that we're having with these things. So I think it's

00:26:41.840 --> 00:26:46.960
a really great library. It's open source. They're looking for contributors. And it's not the full

00:26:46.960 --> 00:26:51.360
picture, of course, because if you're using like a cloud provider, you also need to ask and follow

00:26:51.360 --> 00:26:56.720
up with them to get further information. How much of there is renewable versus non-renewable

00:26:56.720 --> 00:27:01.360
energy? Yeah, exactly. Is it a coal plant? Please say it's not a coal plant. Yeah. We live in

00:27:01.360 --> 00:27:07.520
Germany. Germany is not too bad, but yeah, there is a lot of coal in there. So I think this is a

00:27:07.520 --> 00:27:11.760
great way to start to think about it as technologists, because often it's easy to see

00:27:11.760 --> 00:27:18.640
these problems as something out of our control or beyond the scope of the work that we do every day.

00:27:18.640 --> 00:27:23.360
But I think there's still a lot that we actually can do. Make a huge difference. And just as simple

00:27:23.360 --> 00:27:29.440
as could we cache this output and then reuse it or let it run for five minutes on the cluster and,

00:27:29.440 --> 00:27:33.360
oh, we're not that big of a hurry. Let's let it run over and over and over and then let it run

00:27:33.360 --> 00:27:38.000
in continuous integration. And exactly. Yeah, exactly. And I mean, the good thing that also is

00:27:38.000 --> 00:27:42.160
those things cost money, too. So, yeah, you don't just need to save the planet. You can also save

00:27:42.160 --> 00:27:48.080
yourself some money. It's not 100 percent the same, but usually, yeah, you're you have this

00:27:48.080 --> 00:27:54.080
benefit that other people care more about money or time, money and time. Right. But it's easier

00:27:54.080 --> 00:27:58.880
to sell. Yeah, absolutely. You know, I've had a couple of episodes on this previously, but just

00:27:58.880 --> 00:28:04.160
give people a sense of how much energy is in training some of these large models. And since

00:28:04.160 --> 00:28:09.040
it's one of the shows that I talked to, there was some research done to say training one of

00:28:09.040 --> 00:28:14.560
these large models just one time is as much as, say, a person driving a car for a year type of

00:28:14.560 --> 00:28:20.640
energy. And you're like, oh, that's no joke. And so that that might encourage you to run

00:28:20.640 --> 00:28:25.760
smaller models or things like that, which I think for a long time we were thinking like, oh, it's

00:28:25.760 --> 00:28:30.160
the training that's everything. And then it's kind of like fine once the training's done. But

00:28:30.160 --> 00:28:35.760
actually, the inference is also just as compute heavy when you see the slow words coming out.

00:28:35.760 --> 00:28:41.440
That's yeah. And it's because it's autoregressive. It moves. Yeah, I think it's you have to look at

00:28:41.440 --> 00:28:46.400
it holistically. I think it's very useful to have these metrics that we compare to other things,

00:28:46.400 --> 00:28:52.320
because then we get a sense of like how daunting that is. I think like comparing it to like air

00:28:52.320 --> 00:28:58.880
travel or like to cars and so forth is good. But we tend to focus a little bit on like, oh,

00:28:58.880 --> 00:29:03.920
it's just this part of the system and not the system as a whole. Well, I think the training

00:29:03.920 --> 00:29:09.120
was done a lot previously and the usage was done less. And now the usage has just gone out of

00:29:09.120 --> 00:29:14.320
control. Like if you don't have a eye in your menu ordering app, it's a useless thing, right?

00:29:14.320 --> 00:29:18.640
It's like everybody needs it. Yeah, they don't really need it, I think. But they think they

00:29:18.640 --> 00:29:23.040
need it or the VCs think they need it or something. I think also like a lot of people might think,

00:29:23.040 --> 00:29:27.360
oh, we need to train our own models. But with things like rag, like retrieval, augmentation,

00:29:27.360 --> 00:29:32.960
generation, that now a lot of vector database services are promoting and educating people

00:29:32.960 --> 00:29:38.800
around how to do. That's not true. So you can take like a base model and start to give it your data

00:29:38.800 --> 00:29:44.160
without the need to like tune something yourself, like train something yourself. Yeah, we are very,

00:29:44.160 --> 00:29:49.680
very nearly out of time here, ladies. We all have different things we got to run off and do. But let

00:29:49.680 --> 00:29:54.400
me just close out with some quick thoughts. And really, this deserves maybe two hours, but we've

00:29:54.400 --> 00:30:00.800
got two minutes for data scientists out there listening who are concerned that things like

00:30:00.800 --> 00:30:06.480
Copilot and Devon and all these weird, I'll write code for you things are going to make learning

00:30:06.480 --> 00:30:11.680
data science not relevant. What do you think? I think it's still going to be super relevant, but

00:30:11.680 --> 00:30:21.120
I think that going to help a lot. And I think that could be seen as a potential useful tool

00:30:21.120 --> 00:30:29.200
that could help to a lot of people. It's even for beginners for learning. I think for people who

00:30:29.200 --> 00:30:37.440
are starting to code could be super useful to try to take a look with Copilot or with LLMs and say,

00:30:37.440 --> 00:30:41.920
hey, I don't understand the code. Can you explain to me what is happening in this function and

00:30:41.920 --> 00:30:49.680
something like that. From here to be able to introduce an idea and have a production ready

00:30:49.680 --> 00:30:56.960
code, we are very far away, to be honest, right now. We need more work and the field needs to

00:30:56.960 --> 00:31:04.880
improve a bit of that. But I truly believe that's going to help us a lot at some point in time.

00:31:04.880 --> 00:31:09.840
I think maybe I'll take a different perspective and say that I think for data scientists,

00:31:09.840 --> 00:31:14.080
the core concern for us is not really code. It's more data, I guess.

00:31:14.080 --> 00:31:15.440
Oh, yeah, absolutely.

00:31:15.440 --> 00:31:20.160
Yeah. So I think I'm seeing some potential, even with our own tools at JetBrains,

00:31:20.160 --> 00:31:26.000
to potentially help introduce people to the idea of how to work with data. But there's not really

00:31:26.000 --> 00:31:30.640
necessarily huge shortcuts here because you're still going to learn how to clean a data set

00:31:30.640 --> 00:31:36.480
and evaluate for quality. And so the science part of data science, I don't think it's ever

00:31:36.480 --> 00:31:39.280
going to go away. You still need to be able to think about business problems. You still need

00:31:39.280 --> 00:31:40.480
to be able to think about data.

00:31:40.480 --> 00:31:41.680
We'll be there forever.

00:31:41.680 --> 00:31:44.000
It'll be there forever. Thank God. It's so good.

00:31:44.000 --> 00:31:46.160
That's fun.

00:31:46.160 --> 00:31:51.280
Maybe as not a data scientist, I can give a slightly different perspective. I feel like

00:31:51.280 --> 00:31:56.800
because it comes up just for general programming all the time as well, right? And I think one of

00:31:56.800 --> 00:32:02.960
the things that is at the moment most hurting our industry is the lack of getting people into

00:32:02.960 --> 00:32:09.920
junior level jobs and not AI or any technology itself. It's a very human problem. As are pretty

00:32:09.920 --> 00:32:17.200
much all of the problems with AI itself. So I think, to be honest, what we need to do is really

00:32:17.200 --> 00:32:23.200
hire more juniors, make more entry level programs, get people into these positions and get them

00:32:23.200 --> 00:32:28.320
trained up on using the tools. We don't need to gatekeep. There's going to be plenty of work for

00:32:28.320 --> 00:32:33.600
the rest of us for the next foreseeable future, considering all the big social problems that we

00:32:33.600 --> 00:32:37.600
have to solve. So I just think we should do that.

00:32:37.600 --> 00:32:42.080
All right. Well, let's leave it there. Maria, Jody, Jessica, thank you so much for being on

00:32:42.080 --> 00:32:42.400
the show.

00:32:42.400 --> 00:32:42.880
Thank you.

00:32:42.880 --> 00:32:44.720
Thank you very much. It was amazing.

00:32:44.720 --> 00:32:45.040
Bye.

00:32:45.040 --> 00:32:45.520
Bye.

00:32:45.520 --> 00:32:46.020
Bye.

00:32:46.020 --> 00:32:52.320
This has been another episode of Talk Python to Me. Thank you to our sponsors. Be sure to

00:32:52.320 --> 00:32:56.480
check out what they're offering. It really helps support the show. Take some stress out of your

00:32:56.480 --> 00:33:01.520
life. Get notified immediately about errors and performance issues in your web or mobile

00:33:01.520 --> 00:33:08.160
applications with Sentry. Just visit talkpython.fm/sentry and get started for free. And be

00:33:08.160 --> 00:33:11.520
sure to use the promo code TALKPYTHON, all one word.

00:33:11.520 --> 00:33:17.280
Code comments and original podcast from Red Hat. This podcast covers stories from technologists

00:33:17.280 --> 00:33:23.280
who've been through tough tech transitions and share how their teams survived the journey.

00:33:23.280 --> 00:33:29.440
Episodes are available everywhere you listen to your podcast and at talkpython.fm/code-comments.

00:33:29.440 --> 00:33:34.080
Want to level up your Python? We have one of the largest catalogs of Python video courses over at

00:33:34.080 --> 00:33:40.000
Talk Python. Our content ranges from true beginners to deeply advanced topics like memory and async.

00:33:40.000 --> 00:33:43.760
And best of all, there's not a subscription in sight. Check it out for yourself at

00:33:43.760 --> 00:33:45.360
training.talkpython.fm.

00:33:45.920 --> 00:33:50.560
Be sure to subscribe to the show. Open your favorite podcast app and search for Python.

00:33:50.560 --> 00:33:54.800
We should be right at the top. You can also find the iTunes feed at /itunes,

00:33:54.800 --> 00:34:00.880
the Google Play feed at /play, and the Direct RSS feed at /rss on talkpython.fm.

00:34:00.880 --> 00:34:05.520
We're live streaming most of our recordings these days. If you want to be part of the show and have

00:34:05.520 --> 00:34:10.240
your comments featured on the air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube.

00:34:12.480 --> 00:34:16.320
This is your host, Michael Kennedy. Thanks so much for listening. I really appreciate it.

00:34:16.320 --> 00:34:18.400
Now get out there and write some Python code.

00:34:18.960 --> 00:34:38.960
[Music]

