WEBVTT

00:00:00.001 --> 00:00:05.000
There hasn't been a boom like the AI boom since the dot com days, and it may look like

00:00:05.000 --> 00:00:08.940
a space destined to be controlled by a couple of tech giants.

00:00:08.940 --> 00:00:13.320
But Ines Montani thinks open source will play an important role in the future of AI.

00:00:13.320 --> 00:00:19.060
I hope you join us for this excellent conversation about the future of AI and open source.

00:00:19.060 --> 00:00:24.000
This is Talk Python to Me, episode 465, recorded May 8th, 2024.

00:00:24.000 --> 00:00:27.240
Are you ready for your host, Darius?

00:00:27.240 --> 00:00:30.200
You're listening to Michael Kennedy on Talk Python to Me.

00:00:30.200 --> 00:00:37.560
Live from Portland, Oregon, and this segment was made with Python.

00:00:37.560 --> 00:00:40.960
Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:40.960 --> 00:00:42.820
This is your host, Michael Kennedy.

00:00:42.820 --> 00:00:47.720
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,

00:00:47.720 --> 00:00:50.360
both on fosstodon.org.

00:00:50.360 --> 00:00:55.600
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:00:55.600 --> 00:00:59.360
We've started streaming most of our episodes live on YouTube.

00:00:59.360 --> 00:01:05.080
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and

00:01:05.080 --> 00:01:06.720
be part of that episode.

00:01:06.720 --> 00:01:08.800
This episode is brought to you by Sentry.

00:01:08.800 --> 00:01:10.240
Don't let those errors go unnoticed.

00:01:10.240 --> 00:01:12.520
Use Sentry like we do here at Talk Python.

00:01:12.520 --> 00:01:14.000
Sign up at talkpython.fm/sentry.

00:01:14.000 --> 00:01:18.040
And it's brought to you by Pork Bun.

00:01:18.040 --> 00:01:22.320
Launching a successful project involves many decisions, not the least of which is choosing

00:01:22.320 --> 00:01:23.320
a domain name.

00:01:23.320 --> 00:01:29.040
Get a .app, .dev, or .food domain name at Pork Bun for just $1 for the first year at

00:01:29.040 --> 00:01:32.520
talkpython.fm/porkbun.

00:01:32.520 --> 00:01:34.960
Before we jump into the show, a quick announcement.

00:01:34.960 --> 00:01:39.400
Over at Talk Python, we just launched a new course, and it's super relevant to today's

00:01:39.400 --> 00:01:40.620
topic.

00:01:40.620 --> 00:01:44.040
The course is called Getting Started with NLP and spaCy.

00:01:44.040 --> 00:01:49.120
It was created by Vincent Vormerdam, who has spent time working directly on spaCy at Explosion

00:01:49.120 --> 00:01:50.120
AI.

00:01:50.120 --> 00:01:53.480
The course is a really fun exploration of what you can do with spaCy for processing

00:01:53.480 --> 00:01:55.880
and understanding text data.

00:01:55.880 --> 00:02:00.600
And Vincent uses the past nine years of Talk Python transcripts as the core data for the

00:02:00.600 --> 00:02:01.600
course.

00:02:01.600 --> 00:02:07.200
If you have text data you need to understand, check out the course today at talkpython.fm/spacy.

00:02:07.200 --> 00:02:09.480
The link is in your podcast player show notes.

00:02:09.480 --> 00:02:12.560
Now on to more AI and spaCy with Ines.

00:02:12.560 --> 00:02:14.920
Ines, welcome back to Talk Python with me.

00:02:14.920 --> 00:02:16.520
Yeah, thanks for having me back again.

00:02:16.520 --> 00:02:17.880
You're one of my favorite guests.

00:02:17.880 --> 00:02:19.240
It's always awesome to have you.

00:02:19.240 --> 00:02:20.240
Thanks.

00:02:20.240 --> 00:02:21.240
You're my favorite podcast.

00:02:21.240 --> 00:02:22.240
Thank you.

00:02:22.240 --> 00:02:24.960
We have some really cool things to talk about.

00:02:24.960 --> 00:02:32.440
spaCy, some of course, but also more broadly, we're going to talk about just LLMs and AIs

00:02:32.440 --> 00:02:35.600
and open source and business models and even monopolies.

00:02:35.600 --> 00:02:37.960
We're going to cover a lot of things.

00:02:37.960 --> 00:02:44.760
You've been kind of making a bit of a roadshow, a tour around much of Europe and talking about

00:02:44.760 --> 00:02:46.120
some of these ideas, right?

00:02:46.120 --> 00:02:47.120
Yeah.

00:02:47.120 --> 00:02:51.160
I've been invited to quite a few conferences and I feel like this is like after COVID,

00:02:51.160 --> 00:02:55.760
the first proper, proper year again that I'm traveling for conferences.

00:02:55.760 --> 00:02:56.760
And I was like, why not?

00:02:56.760 --> 00:03:00.680
And then I think especially now that so much is happening in the AI space, I think it's

00:03:00.680 --> 00:03:05.800
actually really nice to go to these conferences and connect with actual developers.

00:03:05.800 --> 00:03:09.880
Because if you're just sitting on the internet and you're scrolling, I don't know, LinkedIn

00:03:09.880 --> 00:03:14.560
and sometimes it can be really hard to tell what people are really thinking and what's...

00:03:14.560 --> 00:03:19.680
Do people really believe some of these hot, weird takes that people are putting out there?

00:03:19.680 --> 00:03:25.320
So yeah, it was very, very nice to talk about some of these ideas, get them checked against

00:03:25.320 --> 00:03:26.320
what developers think.

00:03:26.320 --> 00:03:28.320
So yeah, it's been really cool.

00:03:28.320 --> 00:03:29.320
And there's more to come.

00:03:29.320 --> 00:03:30.320
Yeah, I know.

00:03:30.320 --> 00:03:35.800
I'll be traveling again later this month to Italy for PyCon for my first time, then Pineda

00:03:35.800 --> 00:03:37.760
London and who knows what else.

00:03:37.760 --> 00:03:42.000
If you must go to Italy and London, what terrible places to spend time in, huh?

00:03:42.000 --> 00:03:45.760
I'm definitely very open for tips, especially for Italy, for Florence.

00:03:45.760 --> 00:03:48.800
I've never been to Italy ever.

00:03:48.800 --> 00:03:49.800
Oh, awesome.

00:03:49.800 --> 00:03:51.720
I've been to Rome, but that's it.

00:03:51.720 --> 00:03:55.120
And so I don't have any tips, but London is also fantastic.

00:03:55.120 --> 00:03:56.120
Yeah.

00:03:56.120 --> 00:03:57.120
Cool.

00:03:57.120 --> 00:03:58.120
So people can check you out.

00:03:58.120 --> 00:04:02.200
Maybe, I think, do you have a list of that publicly where people can see some of your

00:04:02.200 --> 00:04:03.200
talks?

00:04:03.200 --> 00:04:04.200
We can put that in the show notes.

00:04:04.200 --> 00:04:05.200
Yeah.

00:04:06.200 --> 00:04:08.280
It's on my website and then also on the Explosion site of our company, we've actually added

00:04:08.280 --> 00:04:14.080
an events page because it came up a lot that like either me, Matt, people from our team

00:04:14.080 --> 00:04:15.080
giving talks.

00:04:15.080 --> 00:04:17.000
And so we thought like, hey, let's... and podcasts as well.

00:04:17.000 --> 00:04:20.920
So we're collecting everything on one page, all the stuff we're doing, which is kind of

00:04:20.920 --> 00:04:21.920
fun.

00:04:21.920 --> 00:04:22.920
Which is quite a bit, actually, for sure.

00:04:22.920 --> 00:04:23.920
Yeah.

00:04:24.920 --> 00:04:31.280
Well, I know many people know you, but let's just talk a little bit about Spacey, Explosion,

00:04:31.280 --> 00:04:34.920
Prodigy, the stuff that you guys are doing to give people a sense of where you're coming

00:04:34.920 --> 00:04:35.920
from.

00:04:35.920 --> 00:04:36.920
Yeah.

00:04:36.920 --> 00:04:43.200
So we're an open source company and we build developer tools for AI, natural language processing

00:04:43.200 --> 00:04:44.200
specifically.

00:04:44.200 --> 00:04:47.960
So, you know, you're working with lots of text, you want to analyze it beyond just looking

00:04:47.960 --> 00:04:49.240
for keywords.

00:04:49.240 --> 00:04:53.040
That's kind of where we started and what we've always been focusing on.

00:04:53.040 --> 00:04:58.160
So Spacey is probably what we're mostly known for, which is a popular open source library

00:04:58.160 --> 00:05:01.680
for really what we call industrial strength NLP.

00:05:01.680 --> 00:05:05.120
So built for production, it's fast, it's efficient.

00:05:05.120 --> 00:05:11.000
We've put a lot of work into having good usable, user-friendly, developer-friendly APIs.

00:05:11.000 --> 00:05:13.280
Actually, yeah, I always set an example.

00:05:13.280 --> 00:05:17.280
I always like to show in my talks a nice side effect that we never anticipated like that

00:05:17.280 --> 00:05:23.080
is that ChatGPT and similar models are actually pretty good at writing Spacey code because

00:05:23.080 --> 00:05:27.800
we put a lot of work into all of this stuff like backwards compatibility, not breaking

00:05:27.800 --> 00:05:30.520
people's code all the time, stuff like that.

00:05:30.520 --> 00:05:34.680
But that happens to really help, at least for now, with these models.

00:05:34.680 --> 00:05:35.680
It's really nice.

00:05:35.680 --> 00:05:40.080
It's a good thing you've done to make it a really stable API that people can trust.

00:05:40.080 --> 00:05:44.800
But is it weird to see LLMs talking about stuff you all created?

00:05:44.800 --> 00:05:46.800
It's kind of, it's funny in some way.

00:05:46.800 --> 00:05:54.240
I mean, it's also, there is this whole other side to, you know, doing user support for

00:05:54.240 --> 00:05:58.440
detecting clearly auto-generated code, because for Spacey, these models are pretty good for

00:05:58.440 --> 00:06:02.240
Prodigy, which is our annotation tool, which is also scriptable in Python.

00:06:02.240 --> 00:06:06.960
It's a bit less precise and it hallucinates a lot because there's just less code online

00:06:06.960 --> 00:06:07.960
and on GitHub.

00:06:07.960 --> 00:06:12.040
So we sometimes get like support requests where like users post their code and we're

00:06:12.040 --> 00:06:13.040
like, this is so strange.

00:06:13.040 --> 00:06:14.640
How did you find these APIs?

00:06:14.640 --> 00:06:15.640
They don't exist.

00:06:15.640 --> 00:06:17.640
And then we're like, ah, this was auto-generated.

00:06:17.640 --> 00:06:19.320
Oh, okay.

00:06:19.320 --> 00:06:21.360
So that was a very new experience.

00:06:21.360 --> 00:06:25.040
And also it's, you know, I think everyone who publishes online deals with that, but

00:06:25.040 --> 00:06:30.440
like, it's very frustrating to see all these like auto-generated posts that like look like

00:06:30.440 --> 00:06:34.320
tech posts up, but are completely bullshit and completely wrong.

00:06:34.320 --> 00:06:40.440
Like I saw with something on Spacey LLM, which is our extension for integrating large language

00:06:40.440 --> 00:06:41.720
models into Spacey.

00:06:41.720 --> 00:06:46.280
And they're like some blog posts that look like they're tech blog posts, but they're

00:06:46.280 --> 00:06:48.200
like completely hallucinated.

00:06:48.200 --> 00:06:52.220
And it's very, very strange to see that about like your own software.

00:06:52.220 --> 00:06:55.640
And also it frustrates me because that stuff is going to feed into the next generation

00:06:55.640 --> 00:06:56.640
of these models.

00:06:56.640 --> 00:06:57.640
Right.

00:06:57.640 --> 00:07:02.680
And I think people stop being so good at this because they're full of stuff that they've

00:07:02.680 --> 00:07:06.600
generated themselves on like APIs and things that don't even exist.

00:07:06.600 --> 00:07:07.600
Yeah.

00:07:07.600 --> 00:07:11.800
It's just going to cycle around and around and around until it just gets worse every

00:07:11.800 --> 00:07:12.800
time.

00:07:12.800 --> 00:07:13.800
And then.

00:07:13.800 --> 00:07:14.800
That's interesting.

00:07:14.800 --> 00:07:17.120
Like it's very interesting to see what's going on and where these things lead.

00:07:17.120 --> 00:07:18.120
It is.

00:07:18.120 --> 00:07:19.440
You know, I just had a thought.

00:07:19.440 --> 00:07:25.600
I was, you know, OpenAI and some of these different companies are doing work to try

00:07:25.600 --> 00:07:28.840
to detect AI generated images.

00:07:28.840 --> 00:07:31.200
And I imagine AI generated content.

00:07:31.200 --> 00:07:33.920
When I heard that, my thought was like, well, that's just because they kind of want to be

00:07:33.920 --> 00:07:38.120
good citizens and they want to put little labels and say, what if it's just so they

00:07:38.120 --> 00:07:40.040
don't ingest it twice?

00:07:40.040 --> 00:07:44.520
I think that's definitely, and I mean, in a way it's also good because, you know, it

00:07:44.520 --> 00:07:46.260
would make these models worse.

00:07:46.260 --> 00:07:50.760
And so from like, you know, from a product perspective for a company like OpenAI, that's

00:07:50.760 --> 00:07:53.160
definitely very useful.

00:07:53.160 --> 00:07:57.640
And I think also, you know, commercially, I think there's definitely, you know, a big

00:07:57.640 --> 00:08:04.200
market in that also for like social networks and stuff to detect are these real images,

00:08:04.200 --> 00:08:07.060
are these deep fakes, is there money in that too?

00:08:07.060 --> 00:08:11.200
So it's not, I don't think it's just, yeah, being good citizens, but like there's a clear

00:08:11.200 --> 00:08:14.640
product motivated thing, which is fine, you know, for a company.

00:08:14.640 --> 00:08:15.640
Yeah, it is fine.

00:08:15.640 --> 00:08:17.480
I just, I never really thought about it.

00:08:17.480 --> 00:08:18.480
Of course.

00:08:19.480 --> 00:08:25.000
I mean, I think we're getting to some point where in food or art, you hear about artisanal

00:08:25.000 --> 00:08:28.080
handcrafted pizza or, you know, whatever.

00:08:28.080 --> 00:08:33.760
Will there be artisanal human created tech that has got a special, special flavor to

00:08:33.760 --> 00:08:34.760
it?

00:08:34.760 --> 00:08:35.760
Like this was created with no AI.

00:08:35.760 --> 00:08:37.360
Look at how cool this site is or whatever.

00:08:37.360 --> 00:08:41.040
I think it's already something that like, you see, like, I don't know which product

00:08:41.040 --> 00:08:43.160
this was, but I saw there was some ad campaign.

00:08:43.160 --> 00:08:48.200
I think it might've been a language learning app or something else where they really like

00:08:48.200 --> 00:08:53.520
put that into one of their like marketing claims, like, Hey, it's not AI generated.

00:08:53.520 --> 00:08:54.640
We don't use AI.

00:08:54.640 --> 00:08:58.320
It's actually real in humans because it seems to be, you know, what people want.

00:08:58.320 --> 00:09:00.840
They want, you know, they want to have at least that feeling.

00:09:00.840 --> 00:09:04.880
So I definitely think there's an appeal of that also going forward.

00:09:04.880 --> 00:09:09.280
The whole LLM and AI stuff, it's just, it's permeated culture so much.

00:09:09.280 --> 00:09:13.560
I was at the motorcycle shop yesterday talking to a guy who was a motorcycle salesman.

00:09:13.560 --> 00:09:18.720
And he was like, do you think that AI is going to change how software developers work?

00:09:18.720 --> 00:09:19.720
Do you think there's still going to be relevant?

00:09:19.720 --> 00:09:20.720
I'm like, you're a sales guy.

00:09:20.720 --> 00:09:21.720
You're a motorcycle sales guy.

00:09:21.720 --> 00:09:22.720
This is amazing.

00:09:22.720 --> 00:09:25.800
How did you're like really this tuned into it, right?

00:09:25.800 --> 00:09:30.080
That you're like, but you know, you think it's maybe just a little echo chamber of us

00:09:30.080 --> 00:09:33.320
talking about, but it seems to be like these kinds of conversations are more broad than

00:09:33.320 --> 00:09:34.320
maybe you would have guessed.

00:09:34.320 --> 00:09:38.720
ChatGPT definitely, you know, brought the conversation into the mainstream, but on the

00:09:38.720 --> 00:09:42.720
other hand, on the plus side, it also means it makes it a lot easier for us kind of to

00:09:42.720 --> 00:09:46.160
explain our work because people have at least heard of this.

00:09:46.160 --> 00:09:50.780
And I think it's also for developers working in teams, like on the one hand, it can maybe

00:09:50.780 --> 00:09:54.800
be frustrating to do this expectation management because you know, you have management who

00:09:54.800 --> 00:10:00.600
just came back from some fancy conference and got sold on like, Ooh, we need like some

00:10:00.600 --> 00:10:01.880
chat bot or LLM.

00:10:01.880 --> 00:10:07.040
It's kind of the chat bot hype all over again that we already had in 2015 or so.

00:10:07.040 --> 00:10:08.040
That can be frustrating.

00:10:08.040 --> 00:10:09.600
Not that those were going to be so important.

00:10:09.600 --> 00:10:10.600
And now what are they doing?

00:10:10.600 --> 00:10:13.800
Yeah, but yeah, but it's like, yeah, I see a lot of parallels.

00:10:13.800 --> 00:10:18.120
It's like, if you look at kind of the hype cycle and people's expectations and expectation

00:10:18.120 --> 00:10:22.440
management, it's kind of the same thing in a lot of ways, only that like it actually

00:10:22.440 --> 00:10:26.240
cuts a lot of parts actually kind of work now, which we didn't really have before.

00:10:26.240 --> 00:10:27.240
Yeah.

00:10:27.240 --> 00:10:30.520
But yeah, it also means for teams and developers that they at least have some more funding

00:10:30.520 --> 00:10:33.320
available and resources that they can work with.

00:10:33.320 --> 00:10:37.080
Because I felt like before that happened, it looked like that, you know, companies are

00:10:37.080 --> 00:10:42.280
really cutting their budgets, all these exploratory AI projects, they all got cut.

00:10:42.280 --> 00:10:44.280
It was quite frustrating for a lot of developers.

00:10:44.280 --> 00:10:48.960
And now at least, you know, it means they can actually work again, even though they

00:10:48.960 --> 00:10:55.400
also have to kind of manage the expectations and like work around some of the Baywild ideas

00:10:55.400 --> 00:10:57.640
that companies might have at the moment.

00:10:57.640 --> 00:10:58.640
Absolutely.

00:10:58.640 --> 00:11:02.840
Now, one of the things that's really important and we're going to get to here, give you a

00:11:02.840 --> 00:11:06.800
chance to give a shout out to the other thing that you all have is, is how do you teach

00:11:06.800 --> 00:11:09.000
these things?

00:11:09.000 --> 00:11:12.320
Information and how do you get them to know things and so on?

00:11:12.320 --> 00:11:16.120
And you know, for the spaCy world, you have Prodigy and maybe give a shout out to Prodigy

00:11:16.120 --> 00:11:17.120
Teams.

00:11:17.120 --> 00:11:19.320
That's something you just are just announcing, right?

00:11:19.320 --> 00:11:20.320
Yeah.

00:11:20.320 --> 00:11:21.320
So that's currently in beta.

00:11:21.320 --> 00:11:22.320
It's something we've been working on.

00:11:22.320 --> 00:11:27.240
So the idea of Prodigy has always been, hey, you know, support spaCy, also other libraries.

00:11:27.240 --> 00:11:31.600
And how can we, yeah, how can we make the training and data collection process more

00:11:31.600 --> 00:11:36.480
efficient or so efficient that companies can in-house that process?

00:11:36.480 --> 00:11:41.000
Like whether it's creating training data, creating evaluation data, like even if what

00:11:41.000 --> 00:11:44.840
you're doing is completely generative and you have a model that does it well, you need

00:11:44.840 --> 00:11:47.520
some examples and some data where you know the answer.

00:11:47.520 --> 00:11:49.920
And often that's a structured data format.

00:11:49.920 --> 00:11:54.240
So you need to create that with, you know, really seeing that like outsourcing that doesn't

00:11:54.240 --> 00:11:55.240
work very well.

00:11:55.240 --> 00:12:00.000
And also now with the newer technologies, you transfer learning, you don't need millions

00:12:00.000 --> 00:12:01.760
of examples anymore.

00:12:01.760 --> 00:12:07.600
So like this big, big data idea for task specific stuff is really dead in a lot of ways.

00:12:07.600 --> 00:12:13.280
So Prodigy is a developer tool that you can script in Python and that makes it easy to

00:12:13.280 --> 00:12:19.760
really collect this kind of structured data on text images and so on.

00:12:19.760 --> 00:12:22.880
And then Prodigy Teams, that has been a very ambitious project.

00:12:22.880 --> 00:12:27.360
We've really been, we've wanted to ship this a long time ago already, but it's been very

00:12:27.360 --> 00:12:31.360
challenging because we basically want to bring also a lot of these ideas that probably we're

00:12:31.360 --> 00:12:36.520
going to talk about today a bit into the cloud while retaining the data privacy.

00:12:36.520 --> 00:12:41.760
And so you'll be able to run your own cluster on your own infrastructure that has the data

00:12:41.760 --> 00:12:43.200
that's scriptable in Python.

00:12:43.200 --> 00:12:47.240
So you can kind of script the SaaS app in Python, which is very cool, which you normally

00:12:47.240 --> 00:12:48.580
can't do.

00:12:48.580 --> 00:12:51.400
Your data never leaves our service.

00:12:51.400 --> 00:12:56.320
And you can basically also use these workflows like distillation, where you start out with

00:12:56.320 --> 00:13:03.600
a super easy prototype that might use Lama or some other models to add GPT, GPT-4.

00:13:03.600 --> 00:13:06.160
Then you benchmark that, see how it does.

00:13:06.160 --> 00:13:11.280
And then you collect some data until you can beat that inaccuracy and have a task specific

00:13:11.280 --> 00:13:14.520
model that really only does the one extraction you're interested in.

00:13:14.520 --> 00:13:15.960
And that model can be tiny.

00:13:15.960 --> 00:13:19.800
Like we've had users build models that are under 10 megabytes.

00:13:19.800 --> 00:13:23.640
Like that's, that is pretty crazy to think about these days.

00:13:23.640 --> 00:13:27.840
And that run like 20 times faster, they're entirely private.

00:13:27.840 --> 00:13:31.120
You can, you know, you don't need like tons of compute to run them.

00:13:31.120 --> 00:13:35.640
And that's kind of really one of the workflows of the future that we see as very promising.

00:13:35.640 --> 00:13:40.380
And it's also, people are often surprised how little task specific data you actually

00:13:40.380 --> 00:13:43.940
need to say beat GPT-4 inaccuracy.

00:13:43.940 --> 00:13:45.600
It's not as much as people think.

00:13:45.600 --> 00:13:50.080
And it's totally, you know, in a single workday, you could often do it.

00:13:50.080 --> 00:13:53.280
The main idea we've been thinking about a lot is basically how can we make that workflow

00:13:53.280 --> 00:13:57.960
better and more user-friendly, even for people who don't have an extensive machine learning

00:13:57.960 --> 00:13:59.080
background.

00:13:59.080 --> 00:14:03.480
Because one thing that like prompting an LLM or prompting a generative model has is that

00:14:03.480 --> 00:14:05.580
it's a very low barrier to entry.

00:14:05.580 --> 00:14:08.780
And it's very, very, the UX is very good.

00:14:08.780 --> 00:14:11.960
You just type in a question, you talk to it the way you would talk to a human.

00:14:11.960 --> 00:14:13.920
And that's easy to get started with.

00:14:13.920 --> 00:14:15.960
The workflow that's a bit more involved.

00:14:15.960 --> 00:14:20.600
Yes, machine learning developers know how to do that, and they know when to do it.

00:14:20.600 --> 00:14:24.640
But it's not as accessible to people who don't have all of that experience.

00:14:24.640 --> 00:14:30.120
And so that's kind of the underlying thing that we're trying to solve.

00:14:30.120 --> 00:14:32.780
This portion of Talk Python to Me is brought to you by Sentry.

00:14:32.780 --> 00:14:37.500
In the last episode, I told you about how we use Sentry to solve a tricky problem.

00:14:37.500 --> 00:14:41.680
This time, I want to talk about making your front end and back end code work more tightly

00:14:41.680 --> 00:14:42.940
together.

00:14:42.940 --> 00:14:47.440
If you're having a hard time getting a complete picture of how your app is working, and how

00:14:47.440 --> 00:14:52.120
requests flow from the front end JavaScript app, back to your Python services down into

00:14:52.120 --> 00:14:57.380
database calls for errors and performance, you should definitely check out Sentry's distributed

00:14:57.380 --> 00:14:58.700
tracing.

00:14:58.700 --> 00:15:02.240
With distributed tracing, you'll be able to track your software's performance, measure

00:15:02.240 --> 00:15:09.020
metrics like throughput and latency, and display the impact of errors across multiple systems.

00:15:09.020 --> 00:15:13.160
Distributed tracing makes Sentry a more complete performance monitoring solution, helping you

00:15:13.160 --> 00:15:18.220
diagnose problems and measure your application's overall health more quickly.

00:15:18.220 --> 00:15:23.540
Tracing in Sentry provides insights such as what occurred for a specific event or issue,

00:15:23.540 --> 00:15:28.160
the conditions that cause bottlenecks or latency issues, and the endpoints and operations that

00:15:28.160 --> 00:15:29.760
consume the most time.

00:15:29.760 --> 00:15:33.420
Help your front end and back end teams work seamlessly together.

00:15:33.420 --> 00:15:39.160
Check out Sentry's distributed tracing at talkpython.fm/sentry-trace.

00:15:39.160 --> 00:15:40.160
That's talkpython.fm/sentry-trace.

00:15:40.160 --> 00:15:48.320
And when you sign up, please use our code TALKPYTHON, all caps, no spaces, to get more

00:15:48.320 --> 00:15:51.220
features and let them know that you came from us.

00:15:51.220 --> 00:15:54.300
Thank you to Sentry for supporting the show.

00:15:54.300 --> 00:15:59.740
Talked about transfer learning and using relatively small amounts of data to specialize models.

00:15:59.740 --> 00:16:00.740
Tell people about what that is.

00:16:00.740 --> 00:16:02.440
How do you actually do that?

00:16:02.440 --> 00:16:07.340
It's actually the same idea that has ultimately really led to these large generative models

00:16:07.340 --> 00:16:08.340
that we see.

00:16:08.340 --> 00:16:14.580
And that's essentially realizing that we can learn a lot about the language and the world

00:16:14.580 --> 00:16:17.860
and a lot of general stuff from raw.

00:16:17.860 --> 00:16:24.220
If we just train a model with a language modeling objective on a bunch of text on all internet

00:16:24.220 --> 00:16:30.180
or parts of the internet or whatever, in order to basically solve the task, which can be

00:16:30.180 --> 00:16:34.860
stuff like predict the next word, in order to do that, the model has to learn so much

00:16:34.860 --> 00:16:40.160
in its weights and in its representations about the language and about really underlying

00:16:40.160 --> 00:16:44.040
subtle stuff about a language that it's also really good at other stuff.

00:16:44.040 --> 00:16:46.860
That's kind of in a nutshell the basic idea.

00:16:46.860 --> 00:16:52.800
And that's then later led to larger and larger models and more and more of these ideas.

00:16:52.800 --> 00:16:58.280
But yeah, the basic concept is if you just train on a lot of raw text and a lot of these

00:16:58.280 --> 00:17:03.840
models are available, like something like BERT, that's already quite a few years old,

00:17:03.840 --> 00:17:07.940
but still, if you look at the literature and look at the experiments people are doing,

00:17:07.940 --> 00:17:09.160
it's still very competitive.

00:17:09.160 --> 00:17:14.280
It's like you get really good results, even with one of the most basic foundation models.

00:17:14.280 --> 00:17:18.320
And you can use that, initialize your model with that, and then just train a small task

00:17:18.320 --> 00:17:22.320
network on top instead of training everything from scratch, which is what you had to do

00:17:22.320 --> 00:17:23.320
before.

00:17:23.320 --> 00:17:29.880
And it's like, if you imagine hiring a new employee, it's like, yes, you can raise them

00:17:29.880 --> 00:17:35.000
from birth or you can sort of have them, which is like a very creepy concept, but it's really

00:17:35.000 --> 00:17:36.000
similar.

00:17:36.000 --> 00:17:38.660
Teach them everything.

00:17:38.660 --> 00:17:40.440
You were born to be a barista.

00:17:40.440 --> 00:17:41.440
Let me tell you.

00:17:41.440 --> 00:17:42.440
Yeah.

00:17:42.440 --> 00:17:44.360
And then you teach them English and you teach them.

00:17:44.360 --> 00:17:45.360
Yeah.

00:17:45.360 --> 00:17:47.160
I mean, it's a lot of work.

00:17:47.160 --> 00:17:50.320
And I guess you know this more than me because you have kids.

00:17:50.320 --> 00:17:51.320
Right.

00:17:51.320 --> 00:17:52.320
So yeah.

00:17:52.320 --> 00:17:56.620
So, and you know, it's, it's understandable that like, okay, this, this, this made a lot

00:17:56.620 --> 00:18:00.480
of these ML projects really hard, but now you actually have the employee come in and

00:18:00.480 --> 00:18:02.360
they can, they know how to talk to people.

00:18:02.360 --> 00:18:06.440
They speak the language and all you have to teach them is like, Hey, here's how you make

00:18:06.440 --> 00:18:07.440
a coffee here.

00:18:07.440 --> 00:18:08.440
Exactly.

00:18:08.440 --> 00:18:09.440
Yeah.

00:18:09.440 --> 00:18:15.000
You basically lean on the school system to say they know the language, they know arithmetic,

00:18:15.000 --> 00:18:16.400
they know how to talk to people.

00:18:16.400 --> 00:18:19.640
I just need to show them how this expresso machine works.

00:18:19.640 --> 00:18:21.320
Here's how you check in.

00:18:21.320 --> 00:18:23.160
Please take out the trash every two hours.

00:18:23.160 --> 00:18:27.320
Like, yeah, a very little bit of specialized information, but you, the sort of general

00:18:27.320 --> 00:18:30.120
working human knowledge is like the base LLM.

00:18:30.120 --> 00:18:31.120
Right.

00:18:31.120 --> 00:18:32.120
That's the idea.

00:18:32.120 --> 00:18:36.560
And also transfer learning, it's still, it's just one technology and in context learning,

00:18:36.560 --> 00:18:41.160
which is what we have with these generative models, that's also just another technique.

00:18:41.160 --> 00:18:45.980
Like it's, you know, it's not the case that transfer learning is sort of outdated or has

00:18:45.980 --> 00:18:47.920
been replaced by in context learning.

00:18:47.920 --> 00:18:51.520
It's two different strategies and you use them in different contexts.

00:18:51.520 --> 00:18:56.680
So another thing I want to touch on for people, I know some people, probably everyone listening

00:18:56.680 --> 00:19:01.920
is more or less aware of this, but in practice, a lot of folks out there listening is certainly

00:19:01.920 --> 00:19:06.360
the ones who are not in the ML or developer space.

00:19:06.360 --> 00:19:10.840
They just go to chat or they go to somewhere and they're like, this is the AI I've gone

00:19:10.840 --> 00:19:12.960
to, right?

00:19:12.960 --> 00:19:13.960
Maybe they go to Bard.

00:19:13.960 --> 00:19:14.960
I don't know.

00:19:14.960 --> 00:19:16.040
Gemini, whatever they call it.

00:19:16.040 --> 00:19:22.520
But there's a whole bunch, I mean, many, many, many open source models with all sorts of

00:19:22.520 --> 00:19:23.520
variations.

00:19:23.520 --> 00:19:29.040
One thing I really like is LM studio in Moire introduced this to me, introduced me to it

00:19:29.040 --> 00:19:30.840
a couple of months ago.

00:19:30.840 --> 00:19:35.280
And basically it's a UI for exploring hugging face models and then downloading them and

00:19:35.280 --> 00:19:38.880
running them with like a chat interface, just in a UI.

00:19:38.880 --> 00:19:43.120
And the really cool thing is they just added LLAMA3, but a lot of these are open source.

00:19:43.120 --> 00:19:44.760
A lot of these are accessible.

00:19:44.760 --> 00:19:48.840
You run them on your machine, you get 7 billion parameter models run easily on my Mac mini.

00:19:48.840 --> 00:19:49.840
Yeah.

00:19:49.840 --> 00:19:52.440
What do you think about some of these models rather than the huge ones?

00:19:52.440 --> 00:19:56.800
Yeah, no, I think it's, and also a lot of them are like, you know, the model itself

00:19:56.800 --> 00:20:02.540
is not necessarily much smaller than what a lot of these chat systems deploy.

00:20:02.540 --> 00:20:06.620
And I think it's also, you know, these are really just the core models for everything

00:20:06.620 --> 00:20:13.460
that's like proprietary and sort of in-house behind like an API is at least one open source

00:20:13.460 --> 00:20:15.720
version that's very similar.

00:20:15.720 --> 00:20:22.200
Like I think it's the whole model is really based on academic research, a lot of the same

00:20:22.200 --> 00:20:23.680
data that's available.

00:20:23.680 --> 00:20:29.520
And I think the most important differentiation we see is then around these chat assistants

00:20:29.520 --> 00:20:31.960
and how they work and how the products are designed.

00:20:31.960 --> 00:20:36.280
So I think it's also, this is a, it's kind of a nice exercise or a nice way to look at

00:20:36.280 --> 00:20:42.360
this distinction between the products versus the machine facing models.

00:20:42.360 --> 00:20:46.720
Because I think that's AI or like, you know, these products are more than just a model.

00:20:46.720 --> 00:20:49.640
And I think that's like a super important thing to keep in mind.

00:20:49.640 --> 00:20:53.440
It's really relevant for this conversation because you have a whole section where you

00:20:53.440 --> 00:20:58.160
talk about regulation and what is the thing, what is the aspect of these things that should

00:20:58.160 --> 00:20:59.160
or could be regulated?

00:20:59.160 --> 00:21:00.160
We'll get to that in a minute.

00:21:00.160 --> 00:21:04.520
A lot of the confusion, confusion that people have around like, Ooh, are we like, is all

00:21:04.520 --> 00:21:07.480
AI going to be locked away behind APIs?

00:21:07.480 --> 00:21:10.000
And how do these bigger, bigger models work?

00:21:10.000 --> 00:21:14.720
I think it kind of stems from the fact that like, it's not always the distinction between

00:21:14.720 --> 00:21:17.160
like the models and products isn't always clear.

00:21:17.160 --> 00:21:20.600
And you know, you could even have, you maybe some companies that are in this business,

00:21:20.600 --> 00:21:24.280
you know, it benefits them to call everything the AI.

00:21:24.280 --> 00:21:25.280
That really doesn't help.

00:21:25.280 --> 00:21:27.600
So to hear you really see the models.

00:21:27.600 --> 00:21:28.600
Yeah.

00:21:28.600 --> 00:21:32.000
And just sorry to talk over you, but to give people a sense, even if you search for Lama

00:21:32.000 --> 00:21:39.960
3 in this thing, there's 192 different configured, modified, et cetera, ways to work with this

00:21:39.960 --> 00:21:42.720
work with the Lama 3 model, which is just crazy.

00:21:42.720 --> 00:21:46.320
So there's a lot of, a lot of stuff that maybe people haven't really explored, I imagine.

00:21:46.320 --> 00:21:47.320
Yeah, it's very cool.

00:21:47.320 --> 00:21:48.320
Yeah.

00:21:49.320 --> 00:21:51.360
One other thing about this, just while we're on it, is it also comes with a open AI API.

00:21:51.360 --> 00:21:56.040
So you could just run it and say, turn on a server API and point if I want to talk to

00:21:56.040 --> 00:21:57.040
it.

00:21:57.040 --> 00:21:58.040
Very fun.

00:21:58.040 --> 00:22:01.440
But let's talk about some of the things you talked about in your talk.

00:22:01.440 --> 00:22:04.260
The AI revolution will not be monopolized.

00:22:04.260 --> 00:22:07.360
How open source beats economies of scale, even for LLMs.

00:22:07.360 --> 00:22:08.360
I love it.

00:22:08.360 --> 00:22:09.840
It's a great title and a great topic.

00:22:09.840 --> 00:22:10.840
Thanks.

00:22:10.840 --> 00:22:12.440
No, I'm very, it's something I'm very passionate about.

00:22:12.440 --> 00:22:17.320
I was like, I was very, you know, happy to be able to say a lot of these things or to,

00:22:17.320 --> 00:22:19.080
you know, also be given a platform.

00:22:19.080 --> 00:22:24.480
You and I, we spoke before about open source and running successful businesses in the tech

00:22:24.480 --> 00:22:25.760
space and all sorts of things.

00:22:25.760 --> 00:22:27.760
So it's a cool follow on for sure.

00:22:27.760 --> 00:22:32.400
I think one of the first parts that you talked about that was really interesting and has

00:22:32.400 --> 00:22:39.740
nothing to do specifically with LLMs or AI is just why is open source a good choice and

00:22:39.740 --> 00:22:44.640
why are people choosing, why is it a good thing to base businesses on and so on?

00:22:44.640 --> 00:22:45.640
Yeah.

00:22:45.640 --> 00:22:50.880
Also often when I give this as a talk, I've, I ask like for a show of hands, like, Hey,

00:22:50.880 --> 00:22:52.640
who uses open source software?

00:22:52.640 --> 00:22:55.040
Who works for a company that depends on open source software?

00:22:55.040 --> 00:22:56.320
Who's contributed before?

00:22:56.320 --> 00:23:01.600
And usually I think most people raise their hand when I ask like who works for a company

00:23:01.600 --> 00:23:03.520
that relies on open source software.

00:23:03.520 --> 00:23:09.120
So I often feel like, Hey, I don't even have to like explain like, Hey, it's a thing.

00:23:09.120 --> 00:23:11.160
It's more about like, you know, collecting these reasons.

00:23:11.160 --> 00:23:16.720
And I do think a lot of it is around like, you know, the transparency, the extensibility,

00:23:16.720 --> 00:23:17.720
it's all kind of connected.

00:23:17.720 --> 00:23:23.120
Like you're not locked in, you can run it in house, you can fork it, you can program

00:23:23.120 --> 00:23:24.120
with it.

00:23:24.120 --> 00:23:28.160
Like those are all important things for companies when they adopt software.

00:23:28.160 --> 00:23:31.920
And you also often, you have these small teams running the project, they can accept PRs,

00:23:31.920 --> 00:23:33.480
they can move fast.

00:23:33.480 --> 00:23:37.600
It's a community around it that can basically give you a sense for, Hey, is this a thing?

00:23:37.600 --> 00:23:39.100
Should I adopt it?

00:23:39.100 --> 00:23:40.960
And all of this I think is important.

00:23:40.960 --> 00:23:46.480
And we also often make a point to like, yes, I've always mentioned, Hey, it's also free,

00:23:46.480 --> 00:23:49.280
which is what people usually associate with open source software.

00:23:49.280 --> 00:23:52.880
It's kind of the first thing that comes to mind, but I actually don't think this is for

00:23:52.880 --> 00:23:56.600
companies the main motivation why they use open source software.

00:23:56.600 --> 00:23:58.480
I absolutely agree.

00:23:58.480 --> 00:24:05.120
Even though we have FOSS, free and open source software, this is not really why companies

00:24:05.120 --> 00:24:06.120
care about it.

00:24:06.120 --> 00:24:07.120
Right?

00:24:07.120 --> 00:24:12.400
Some people do, some people don't, but companies, they often see that as a negative, I think

00:24:12.400 --> 00:24:15.000
almost like, well, who do we sue if this goes wrong?

00:24:15.000 --> 00:24:17.040
Where's our service level agreement?

00:24:17.040 --> 00:24:18.560
Who's going to help us?

00:24:18.560 --> 00:24:20.400
Who's legally obligated to help us?

00:24:20.400 --> 00:24:24.200
We've definitely also seen that, or like we have companies who are like, well, who can

00:24:24.200 --> 00:24:32.200
we pay or can we pay to like, I don't know, get some guarantee or like some support or

00:24:32.200 --> 00:24:37.440
can you like confirm to us that, Hey, if there is a critical vulnerability, that's like really

00:24:37.440 --> 00:24:42.040
directly affecting our software, which has never happened, but are you going to fix it?

00:24:42.040 --> 00:24:45.040
We're like, yes, we can say that that's what we've been doing.

00:24:45.040 --> 00:24:48.080
But if you want that guarantee, we can give that to you for money.

00:24:48.080 --> 00:24:49.080
Sure.

00:24:49.080 --> 00:24:52.000
But like, you can pay us, we'll promise to do what we already promised to do, but we'll

00:24:52.000 --> 00:24:53.840
really double, double promise to do it.

00:24:53.840 --> 00:24:54.840
Right.

00:24:54.840 --> 00:24:55.840
That's definitely a thing.

00:24:55.840 --> 00:24:57.800
And also it's kind of to, you know, to go back up to the business model thing, it's

00:24:57.800 --> 00:25:02.880
what we've seen with Prodigy, which, you know, we offer kind of it's, it's really as a tool

00:25:02.880 --> 00:25:04.600
that follows the open source spirit.

00:25:04.600 --> 00:25:09.100
Like you don't, you pip install it, it's a Python library, you work with it, but we decided

00:25:09.100 --> 00:25:13.840
to kind of use that as a stepping stone between our free open source offering and like the

00:25:13.840 --> 00:25:18.120
SaaS product that we're about to launch soon, hopefully.

00:25:18.120 --> 00:25:20.560
And it's kind of in the middle and it's paid.

00:25:20.560 --> 00:25:25.320
And we've definitely not found that this is like a huge disadvantage for companies.

00:25:25.320 --> 00:25:29.400
Like, yeah, sure, you always have companies with like no budget, but those are also usually

00:25:29.400 --> 00:25:34.480
not the teams that are really doing, you know, a lot of the high value work because you know,

00:25:34.480 --> 00:25:38.400
it is quite normal to have a budget or like software tools.

00:25:38.400 --> 00:25:39.400
Companies pay a lot for this.

00:25:39.400 --> 00:25:43.800
Like, you know, if you, if you want to buy Prodigy, like that, that costs less than,

00:25:43.800 --> 00:25:48.600
I don't know, getting a decent office chair, like, you know, in a, in a commercial context,

00:25:48.600 --> 00:25:50.320
these, these scales are all a bit different.

00:25:50.320 --> 00:25:54.440
So yeah, I do think companies are happy to pay for something that they need and that's

00:25:54.440 --> 00:25:55.440
cool.

00:25:55.440 --> 00:25:56.440
Yeah.

00:25:56.440 --> 00:25:59.440
And the ones who wouldn't have paid, there's a group who said, well, maybe I'll use the

00:25:59.440 --> 00:26:04.040
free one, but they're not serious enough about it to actually pay for it or actually make

00:26:04.040 --> 00:26:05.040
use of it.

00:26:05.040 --> 00:26:08.280
You know, I think of sort of analogies of piracy, right?

00:26:08.280 --> 00:26:10.680
Like, oh, they stole our app or they stole our music.

00:26:10.680 --> 00:26:14.280
Like, well, because it was a link, they clicked it, but they, they wouldn't have bought it

00:26:14.280 --> 00:26:15.560
or used it at all.

00:26:15.560 --> 00:26:18.280
It's not like you lost a customer because they were not going to be customers.

00:26:18.280 --> 00:26:19.280
They just happened to click the link.

00:26:19.280 --> 00:26:20.280
Yeah, exactly.

00:26:20.280 --> 00:26:25.760
I always tell the story of like, when I was, you know, a teenager, I did download a crack

00:26:25.760 --> 00:26:27.800
version of Adobe Photoshop.

00:26:27.800 --> 00:26:31.440
And because I was a teenager, I would have never been able to, like, back then they had,

00:26:31.440 --> 00:26:32.440
they didn't have a SaaS model.

00:26:32.440 --> 00:26:35.120
Like, I don't know what it would Photoshop costs, but like, it's definitely not something

00:26:35.120 --> 00:26:38.320
I would have been able to afford as a 13, 14 year old.

00:26:38.320 --> 00:26:39.960
So I did find that online.

00:26:39.960 --> 00:26:40.960
I downloaded it.

00:26:40.960 --> 00:26:45.100
I'm pretty sure if Adobe had wanted, they could have come after me for that.

00:26:45.100 --> 00:26:48.720
And I do think like, I don't know, maybe I'm giving them too much credit, but I do think

00:26:48.720 --> 00:26:52.040
they might've not done that because they're like, well, what, it's not like we lost a

00:26:52.040 --> 00:26:53.040
customer here.

00:26:53.040 --> 00:26:57.240
And now I'm an adult and I'm, I'm proficient at Photoshop and now I'm paying for it.

00:26:57.240 --> 00:26:58.240
Yeah, exactly.

00:26:58.240 --> 00:27:02.040
And I think there was this whole generation of teenagers who then maybe went into creative

00:27:02.040 --> 00:27:04.360
jobs and came in with Photoshop skills.

00:27:04.360 --> 00:27:08.200
Like I wasn't even like, compared to all these other teenagers I was hanging out with on

00:27:08.200 --> 00:27:11.200
the internet, like all these, like mostly, mostly girls.

00:27:11.200 --> 00:27:13.720
I wasn't even that talented at Photoshop specifically.

00:27:13.720 --> 00:27:18.120
So maybe, maybe there was someone smart who thought about this as like a business strategy

00:27:18.120 --> 00:27:21.000
that these teenagers have our professional tools.

00:27:21.000 --> 00:27:22.000
Exactly.

00:27:22.000 --> 00:27:23.000
It's almost marketing.

00:27:23.000 --> 00:27:24.000
Yeah.

00:27:24.000 --> 00:27:30.240
Another aspect here that I think is really relevant to LLMs is runs in house, AKA we're

00:27:30.240 --> 00:27:36.400
not sending our private data, private source code, API keys, et cetera, to other companies

00:27:36.400 --> 00:27:40.560
that may even use that to train their models, which then regurgitate that back to other

00:27:40.560 --> 00:27:42.480
people who are trying to solve the same problems.

00:27:42.480 --> 00:27:43.480
Right.

00:27:43.480 --> 00:27:46.960
That's also, we're definitely seeing that companies are becoming more and more aware

00:27:46.960 --> 00:27:48.600
of this, which is good.

00:27:48.600 --> 00:27:52.120
Like in a lot of industries, like I wouldn't want, I don't know, my healthcare provider

00:27:52.120 --> 00:27:57.400
to just upload all of my data to like whichever SaaS tool they decide to use at the moment.

00:27:57.400 --> 00:27:59.080
Like, you know, of course not.

00:27:59.080 --> 00:28:00.720
So I think it's, you know, it's, it's good.

00:28:00.720 --> 00:28:05.880
And then also with, you know, more data privacy regulations, that's all, that's really on

00:28:05.880 --> 00:28:08.280
people's minds and people don't want this.

00:28:08.280 --> 00:28:13.440
Like often we have, we have companies or users who actually have to run a lot of their AI

00:28:13.440 --> 00:28:17.800
stuff on completely air gap machines, so they can't even have internet access.

00:28:17.800 --> 00:28:19.880
Or it's about, you know, financial stuff.

00:28:19.880 --> 00:28:24.280
We're actually working on a case study that we're hoping to publish soon where even the

00:28:24.280 --> 00:28:26.360
financial information can move markets.

00:28:26.360 --> 00:28:28.300
It's even segregated in the office.

00:28:28.300 --> 00:28:31.160
So it needs to be 100% in-house.

00:28:31.160 --> 00:28:32.160
Yeah.

00:28:32.160 --> 00:28:33.160
That makes sense.

00:28:33.160 --> 00:28:36.960
And I think open source software, it's great because you can do that and you can build

00:28:36.960 --> 00:28:41.800
your own things with it and really decide how you want to host it, how it fits into

00:28:41.800 --> 00:28:43.240
your existing stack.

00:28:43.240 --> 00:28:44.800
That's another big thing.

00:28:44.800 --> 00:28:49.560
People will already use some tools and you know, you don't want to change your entire

00:28:49.560 --> 00:28:53.280
workflow for every different tool or platform you use.

00:28:53.280 --> 00:28:56.840
And I think especially people have been burned by that so many times by now.

00:28:56.840 --> 00:29:01.080
And there's so many like, you know, unreliable startups, things you have, there's a company

00:29:01.080 --> 00:29:04.040
that really tries to convince you to build on their product.

00:29:04.040 --> 00:29:08.240
And then two months later they close everything down or, you know, it doesn't even have to

00:29:08.240 --> 00:29:09.240
be startup.

00:29:09.240 --> 00:29:14.560
You know, Google, I'm still mad at Google for shutting down Google reader.

00:29:14.560 --> 00:29:17.760
And I don't know, it's been over 10 years, I'm sure.

00:29:17.760 --> 00:29:19.280
And I'm still angry about that.

00:29:19.280 --> 00:29:20.280
I actually had a practice.

00:29:20.280 --> 00:29:21.280
We did it.

00:29:21.280 --> 00:29:26.120
We were invited to give a talk at Google and I needed a text example to visualize, you

00:29:26.120 --> 00:29:30.520
know, something grammatical and that text I made, Google shut down Google reader.

00:29:30.520 --> 00:29:33.400
That's a quiet protest.

00:29:33.400 --> 00:29:34.400
That's amazing.

00:29:34.400 --> 00:29:35.400
Yeah.

00:29:35.560 --> 00:29:39.040
We're going to run sentiment analysis on this article here.

00:29:39.040 --> 00:29:40.040
Sure.

00:29:40.040 --> 00:29:44.240
Open source projects can become unmaintained and that sucks, but like, you know, you can

00:29:44.240 --> 00:29:45.240
fork it.

00:29:45.240 --> 00:29:47.160
It's there and you can have it.

00:29:47.160 --> 00:29:49.080
So there is this, this is motivating.

00:29:49.080 --> 00:29:52.960
And I think we've always called it like you can reinvent the wheel, but don't reinvent

00:29:52.960 --> 00:29:57.160
the road, which is basically, you can build something.

00:29:57.160 --> 00:30:01.800
Reinventing the wheel I don't think is bad, but like you don't want to make people follow

00:30:01.800 --> 00:30:05.520
like, you know, your way of doing everything.

00:30:05.520 --> 00:30:06.520
And yeah, that's interesting.

00:30:06.520 --> 00:30:07.520
Yeah.

00:30:07.520 --> 00:30:08.800
Like we have electric cars now.

00:30:08.800 --> 00:30:09.800
All right.

00:30:09.800 --> 00:30:15.520
So give us a sense of some of the open source models in this AI space here.

00:30:15.520 --> 00:30:18.860
I've kind of divided it into sort of three categories.

00:30:18.860 --> 00:30:22.140
So one of them is what I've called task specific models.

00:30:22.140 --> 00:30:27.720
So that's really models that we're trying to do one specific or some specific things.

00:30:27.720 --> 00:30:30.240
It's kind of what we distribute for spacey.

00:30:30.240 --> 00:30:37.480
There's also a lot of really cool community projects like SciSpacey for scientific biomedical

00:30:37.480 --> 00:30:38.480
techs.

00:30:38.480 --> 00:30:41.320
Stanford also publishes their stanza models.

00:30:41.320 --> 00:30:45.120
And yeah, if you've been on the Hugging Face Hub, there's like tons of these models that

00:30:45.120 --> 00:30:50.880
were really fine tuned to predict like a particular type of categories, stuff like that.

00:30:50.880 --> 00:30:54.280
And so that's been around for quite a while, quite established.

00:30:54.280 --> 00:30:58.320
A lot of people use these in production and it was so quite, especially nowadays, but

00:30:58.320 --> 00:31:03.120
today's standards, they're quite small, cheap, but of course they do one particular thing.

00:31:03.120 --> 00:31:05.120
So they don't generalize very well.

00:31:05.120 --> 00:31:07.760
So that's kind of the one category.

00:31:07.760 --> 00:31:14.040
You probably used to think of them as large and now you see how giant, how many gigabytes

00:31:14.040 --> 00:31:15.600
those models are, you know?

00:31:15.600 --> 00:31:16.600
Yeah.

00:31:16.600 --> 00:31:20.800
When deep learning first kind of came about and people were sort of migrating from linear

00:31:20.800 --> 00:31:25.920
models and stuff, like I've met people complaining that the models were too, were so big and

00:31:25.920 --> 00:31:33.000
slow and that was before we even used much transfer learning and transformer models and

00:31:33.000 --> 00:31:34.000
BERT and stuff.

00:31:34.000 --> 00:31:37.520
And even when that came about, it was also of course the challenge like, Hey, these are

00:31:37.520 --> 00:31:38.600
significantly bigger.

00:31:38.600 --> 00:31:40.840
We do have to change a lot around it.

00:31:40.840 --> 00:31:46.160
Or even, you know, Google who published BERT, they had to do a lot of work around it to

00:31:46.160 --> 00:31:50.100
kind of make it work into their workflows and ship them into production and optimize

00:31:50.100 --> 00:31:55.300
them because they're quite different from what was there before.

00:31:55.300 --> 00:31:59.760
This portion of Talk Python to Me is sponsored by porkbun.com.

00:31:59.760 --> 00:32:03.820
Launching a successful project involves many decisions, not the least of which is choosing

00:32:03.820 --> 00:32:05.240
a domain name.

00:32:05.240 --> 00:32:09.720
And as your project grows, ownership and control of that domain is critical.

00:32:09.720 --> 00:32:12.080
You want a domain registrar that you can trust.

00:32:12.080 --> 00:32:15.900
I recently moved a bunch of my domains to a new provider and asked the community who

00:32:15.900 --> 00:32:18.100
they recommended I choose.

00:32:18.100 --> 00:32:20.340
Porkbun was highly recommended.

00:32:20.340 --> 00:32:26.460
Porkbun specializes in domains that developers need like .app, .dev and .foo domains.

00:32:26.460 --> 00:32:30.780
If you're launching that next breakthrough developer tool or finally creating a dedicated

00:32:30.780 --> 00:32:36.900
website for your open source project, how about a .dev domain or just show off your

00:32:36.900 --> 00:32:40.420
kung.foo programming powers with a domain there.

00:32:40.420 --> 00:32:43.140
These domains are designed to be secure by default.

00:32:43.140 --> 00:32:49.880
All .app and .dev domains are HSTS preloaded, which means that all .app and .dev websites

00:32:49.880 --> 00:32:53.580
will only load over an encrypted SSL connection.

00:32:53.580 --> 00:32:56.980
This is the gold standard of website security.

00:32:56.980 --> 00:33:02.100
If you're paying for Whois privacy, SSL certificates and more, you should definitely check out

00:33:02.100 --> 00:33:03.180
Porkbun.

00:33:03.180 --> 00:33:05.880
These features are always free with every domain.

00:33:05.880 --> 00:33:08.300
So get started with your next project today.

00:33:08.300 --> 00:33:14.780
Lock down your .app, .dev or .foo domain at Porkbun for only $1 for the first year.

00:33:14.780 --> 00:33:17.220
That's right, just $1.

00:33:17.220 --> 00:33:19.620
Visit talkbython.fm/porkbun.

00:33:19.620 --> 00:33:22.620
That's talkbython.fm/porkbun.

00:33:22.620 --> 00:33:25.060
The link is in your podcast player's show notes.

00:33:25.060 --> 00:33:28.680
Thank you to Porkbun for supporting the show.

00:33:28.680 --> 00:33:34.340
Another one in this category of task specific models is ScispaCy, which is kind of cool.

00:33:34.340 --> 00:33:35.340
What's ScispaCy?

00:33:35.340 --> 00:33:42.860
Yeah, so ScispaCy, that's for scientific biomedical text that was published by Allen AI researchers.

00:33:42.860 --> 00:33:49.220
And yeah, it's really, it has like components specific for working with that sort of data.

00:33:49.220 --> 00:33:53.740
And it's actually, it's definitely, if that's kind of the domain, yeah, any listeners are

00:33:53.740 --> 00:33:55.580
working with, definitely check it out.

00:33:55.580 --> 00:34:02.380
They've also done some pretty smart work around like a training components, but also implementing

00:34:02.380 --> 00:34:07.340
like hybrid rule-based things for say acronym expansion.

00:34:07.340 --> 00:34:11.820
They're like cool algorithms that you can implement that don't necessarily need much

00:34:11.820 --> 00:34:13.820
machine learning, but that work really well.

00:34:13.820 --> 00:34:19.380
And so it's basically this suite of components and also models that are more tuned for that

00:34:19.380 --> 00:34:20.380
domain.

00:34:20.380 --> 00:34:24.100
You mentioned some, but also encoder models.

00:34:24.100 --> 00:34:26.860
What's the difference between the task specific ones and the encoder ones?

00:34:26.860 --> 00:34:31.300
That's kind of also what we were talking about earlier, actually, with the transfer learning

00:34:31.300 --> 00:34:33.180
foundation models.

00:34:33.180 --> 00:34:38.500
These are models trained with a language modeling objective, for example, like Google's BERT.

00:34:38.500 --> 00:34:41.940
And that can also be the foundation for task specific models.

00:34:41.940 --> 00:34:44.860
That's kind of what we're often doing nowadays.

00:34:44.860 --> 00:34:50.460
Like you start out with some of these pre-trained weights, and then you train like this task

00:34:50.460 --> 00:34:56.420
specific network on top of it that uses everything that is in these weights about the language

00:34:56.420 --> 00:34:57.420
and the world.

00:34:57.420 --> 00:35:02.860
And yeah, actually by today's standards, these are still relatively small and relatively

00:35:02.860 --> 00:35:08.480
fast and they generalize better because they're trained on a lot of raw text that has like

00:35:08.480 --> 00:35:13.980
a lot of, yeah, a lot of that intrinsic meta knowledge about the language and the world

00:35:13.980 --> 00:35:16.500
that we need to solve a lot of other tasks.

00:35:16.500 --> 00:35:17.500
Absolutely.

00:35:17.500 --> 00:35:23.900
And then you've used the word, the term large generative models for things like Lama and

00:35:23.900 --> 00:35:25.220
Mistral and so on.

00:35:25.220 --> 00:35:29.480
One thing that's very unfortunate when we're talking about these models is that like everything

00:35:29.480 --> 00:35:34.700
we've talked about here has at some point been called an NLM by someone.

00:35:34.700 --> 00:35:39.020
That makes it like really hard to talk about it.

00:35:39.020 --> 00:35:43.160
You can argue that like, well, all of them are kind of large language models.

00:35:43.160 --> 00:35:47.280
And then there's also the marketing confusion.

00:35:47.280 --> 00:35:52.520
When LLMs were hot, everyone wants to have LLMs.

00:35:52.520 --> 00:35:57.660
And so by some definition of LLMs, we've all been running LLMs in production for years.

00:35:57.660 --> 00:36:02.260
But basically I've kind of decided, okay, I want to try and avoid that phrase as much

00:36:02.260 --> 00:36:04.700
as possible because it really doesn't help.

00:36:04.700 --> 00:36:09.900
And so large generative models kind of captures that same idea, but it makes it clear, okay,

00:36:09.900 --> 00:36:15.300
these generate text, text goes in, text comes out and they're large and they're different

00:36:15.300 --> 00:36:18.020
from the other types of models basically.

00:36:18.020 --> 00:36:22.280
Question out of the audience is Mr. Magnetic said, I'd love to learn how to develop AI.

00:36:22.280 --> 00:36:25.140
So maybe let me rephrase that just a little bit and see what your thoughts are.

00:36:25.140 --> 00:36:28.740
Like if people want to get more foundational, this kind of stuff, like what areas should

00:36:28.740 --> 00:36:32.380
they maybe focus in to learn?

00:36:32.380 --> 00:36:33.380
What are your thoughts there?

00:36:33.380 --> 00:36:35.700
It depends on really what it means.

00:36:35.700 --> 00:36:41.300
Like if you really, there is a whole path to, okay, you really want to learn more about

00:36:41.300 --> 00:36:45.100
the models, how they work, the research that goes into it.

00:36:45.100 --> 00:36:50.740
I think there's a lot of actually also academic resources and courses that you can take that

00:36:50.740 --> 00:36:54.740
are similar to what you would learn in university if you started.

00:36:54.740 --> 00:36:55.740
ML course.

00:36:55.740 --> 00:37:01.220
Yeah, like ML and also I think some universities have made some of their like beginners courses

00:37:01.220 --> 00:37:02.220
public.

00:37:02.220 --> 00:37:03.220
I think Stanford has.

00:37:03.220 --> 00:37:04.220
Yeah, right.

00:37:04.220 --> 00:37:07.500
I thought Stanford, I think there's someone else, but like there's definitely also a lot

00:37:07.500 --> 00:37:08.720
of stuff coming out.

00:37:08.720 --> 00:37:14.300
So you can kind of go in that direction, really learn, okay, what goes into this?

00:37:14.300 --> 00:37:15.540
What's the theory behind these?

00:37:15.540 --> 00:37:18.420
And there are some people who really like that approach.

00:37:18.420 --> 00:37:20.740
And then there's a whole more practical side.

00:37:20.740 --> 00:37:26.220
Okay, I want to build an application that uses the technology and it solves a problem.

00:37:26.220 --> 00:37:29.460
And often it helps to have like an idea of what you want to do.

00:37:29.460 --> 00:37:33.500
Like if you don't want to develop AI for the sake of it, then it often helps like, hey,

00:37:33.500 --> 00:37:37.620
you have, even if it's just your hobby, like you're into football and you come up with

00:37:37.620 --> 00:37:44.260
like some fun problem, like you want to analyze football news, for example, and analyze it

00:37:44.260 --> 00:37:45.260
for something you care about.

00:37:45.260 --> 00:37:49.460
Like, I don't know, like often really helps to have this hobby angle or something you're

00:37:49.460 --> 00:37:50.460
interested in.

00:37:50.460 --> 00:37:51.460
Yeah, it does.

00:37:51.460 --> 00:37:52.460
Yeah.

00:37:52.460 --> 00:37:55.860
And then you can start looking at tools that go in that direction, like start with some

00:37:55.860 --> 00:38:00.780
of these open source models, even, you know, try out some of these generative models, see

00:38:00.780 --> 00:38:06.020
how you go, try out if you want to do information extraction, try out maybe something like spacey.

00:38:06.020 --> 00:38:07.460
That's like really a lot there.

00:38:07.460 --> 00:38:12.860
And it's definitely been become a lot easier to get started and build something these days.

00:38:12.860 --> 00:38:16.220
Another thing you talked about was economies of scale.

00:38:16.220 --> 00:38:17.860
And this one's really interesting.

00:38:17.860 --> 00:38:25.420
So basically we've got Gemini and OpenAI where they've just got so much traffic and kind

00:38:25.420 --> 00:38:29.180
of a little bit back to the question actually is if you want to do this kind of stuff, you

00:38:29.180 --> 00:38:33.620
want to run your own service, do it, you know, even if you had the equivalent to stuff, it's

00:38:33.620 --> 00:38:38.140
tricky because even just the way you batch compute, you maybe want to talk about that

00:38:38.140 --> 00:38:39.140
a bit.

00:38:39.140 --> 00:38:43.620
The idea of economies of scale is basically, well, as the, you know, as the companies produce

00:38:43.620 --> 00:38:48.860
more output, the cost per unit decreases and yeah, there's like all kinds of, you know,

00:38:48.860 --> 00:38:51.220
basically it gets cheaper to do more stuff.

00:38:51.220 --> 00:38:56.600
And you know, they're like a lot of more boring, like businessy reasons why it's like that.

00:38:56.600 --> 00:39:02.120
But I think for machine learning specifically, the fact that GPUs are so parallel really

00:39:02.120 --> 00:39:03.260
makes a difference here.

00:39:03.260 --> 00:39:07.340
And you, because you know, you get the user text in, you can't just arbitrarily chop up

00:39:07.340 --> 00:39:09.260
that text because the context matters.

00:39:09.260 --> 00:39:10.660
You need to process that.

00:39:10.660 --> 00:39:15.740
So in order to make the most use of the compute, you basically need to batch it up.

00:39:15.740 --> 00:39:20.340
So either, you know, kind of need to wait until there's enough to batch up.

00:39:20.340 --> 00:39:25.540
And that means that, yes, that favors a lot of those providers that have a lot of traffic

00:39:25.540 --> 00:39:28.380
or, you know, you introduce latency.

00:39:28.380 --> 00:39:33.500
So that's definitely something that at least looks like a problem or, you know, something

00:39:33.500 --> 00:39:37.440
that can be discouraging because it feels like, Hey, if you, you know, if, if supposedly

00:39:37.440 --> 00:39:41.860
the only way you can kind of participate is by running these models and either you have

00:39:41.860 --> 00:39:47.060
to run them yourself or go via an API, like then you kind of doomed.

00:39:47.060 --> 00:39:52.300
And does that mean that, okay, only some large companies can provide AI for us.

00:39:52.300 --> 00:39:56.560
So that's kind of also the, you know, the point and, you know, the very legit like worry

00:39:56.560 --> 00:40:01.140
that some people have, like, does that lead to like monopolizing AI basically?

00:40:01.140 --> 00:40:06.140
It's a very valid concern because even if you say, okay, look, here's the deal.

00:40:06.140 --> 00:40:08.760
OpenAI gets to run on Azure.

00:40:08.760 --> 00:40:12.180
I can go get a machine with a GPU stuck to it and run that on Azure.

00:40:12.180 --> 00:40:13.340
Well, guess what?

00:40:13.340 --> 00:40:16.800
They get one of those huge arm chips.

00:40:16.800 --> 00:40:22.640
That's like the size of a plate and they get the special machines and they also get either

00:40:22.640 --> 00:40:28.420
wholesale compute costs or they get just, we'll give you a bunch of compute for some

00:40:28.420 --> 00:40:32.320
ownership of your company, kind of like Microsoft and OpenAI.

00:40:32.320 --> 00:40:34.960
That's a very difficult thing to compete with on one hand, right?

00:40:34.960 --> 00:40:35.960
Yes.

00:40:35.960 --> 00:40:42.760
If you want to, you know, run your own like LLM or generative model API services, that's

00:40:42.760 --> 00:40:46.000
definitely a disadvantage you're going to have.

00:40:46.000 --> 00:40:51.160
But on the other hand, I think one thing that leads to this perception that I think is not

00:40:51.160 --> 00:40:55.520
necessarily true is the fact that you want to do anything you need, basically larger

00:40:55.520 --> 00:40:59.600
and larger models that, you know, if you want to do something specific, the only way to

00:40:59.600 --> 00:41:05.160
get there is to turn that request into arbitrary language and then use the largest model that

00:41:05.160 --> 00:41:07.880
can handle arbitrary language and go from there.

00:41:07.880 --> 00:41:11.560
Like if you, and I know this is like something that, you know, maybe a lot of LLM companies

00:41:11.560 --> 00:41:14.240
want to tell you, but that's not necessarily true.

00:41:14.240 --> 00:41:19.520
And you don't, yeah, for a lot of things you're doing, you don't even need to depend on a

00:41:19.520 --> 00:41:21.600
large model at runtime.

00:41:21.600 --> 00:41:26.480
You can distill it and you can use it at development time and then build something that you can

00:41:26.480 --> 00:41:27.480
run in-house.

00:41:27.480 --> 00:41:32.600
And these calculations also look, look very, very different if you're using something at

00:41:32.600 --> 00:41:36.160
development time versus in production at runtime.

00:41:36.160 --> 00:41:40.600
And then it can actually be totally fine to just run something in-house.

00:41:40.600 --> 00:41:45.920
And the other point here is actually some, if we're having a situation where, Hey, you're

00:41:45.920 --> 00:41:52.560
paying a large company to provide some service for you, provide a model for you via an API.

00:41:52.560 --> 00:41:56.040
And there are lots of companies and kind of the main differentiator is who can offer it

00:41:56.040 --> 00:41:57.200
for cheaper.

00:41:57.200 --> 00:42:00.120
That's sort of the opposite of a monopoly at least, right?

00:42:00.120 --> 00:42:01.440
That's like competition.

00:42:01.440 --> 00:42:07.640
So this actually, I feel like economies of scale, this idea does not prove that, Hey,

00:42:07.640 --> 00:42:11.520
we're heading into, we're heading into a monopoly.

00:42:11.520 --> 00:42:15.960
And it's also not true because it's not, if you realize that, Hey, it's not, you know,

00:42:15.960 --> 00:42:22.600
you don't need the biggest, most arbitrary models for everything you're doing, then the

00:42:22.600 --> 00:42:24.000
calculation looks very, very different.

00:42:24.000 --> 00:42:25.000
Yeah, I agree.

00:42:25.000 --> 00:42:30.000
I think there's a couple of thoughts I also have here is one, this LM studio I was talking

00:42:30.000 --> 00:42:35.520
about, I've been running the Lama 3 7 billion parameter model locally instead of using chat

00:42:35.520 --> 00:42:36.520
these days.

00:42:36.520 --> 00:42:38.880
And it's been, I would say just as good.

00:42:38.880 --> 00:42:44.280
And it's, it runs about the same speed on my Mac mini as a typical request does over

00:42:44.280 --> 00:42:45.280
there.

00:42:45.280 --> 00:42:47.440
I mean, can't handle as many, but it's just me, it's my computer, right?

00:42:47.440 --> 00:42:48.600
I'm fine.

00:42:48.600 --> 00:42:53.360
And then the other one is if you specialize one of these models, right?

00:42:53.360 --> 00:42:55.880
You feed it a bunch of your datasets from your companies.

00:42:55.880 --> 00:43:01.200
It might not be able to write you something in the style of Shakespeare around, you know,

00:43:01.200 --> 00:43:05.800
a legal contract or some weird thing like that, but it can probably answer really good

00:43:05.800 --> 00:43:08.680
questions about what is our company policy on this?

00:43:08.680 --> 00:43:12.560
Or what is, what are our engineering reports about this thing say, or, you know, stuff

00:43:12.560 --> 00:43:14.280
that you actually care about, right?

00:43:14.280 --> 00:43:15.280
You could run that.

00:43:15.280 --> 00:43:16.280
That's kind of what you want.

00:43:16.280 --> 00:43:19.280
Like you actually want to, if you're talking about, if we're going to like some of the

00:43:19.280 --> 00:43:23.600
risks or things people are worried about, like a lot of that is around what people refer

00:43:23.600 --> 00:43:27.240
to like, Oh, the model going rogue or like the model doing stuff it's not supposed to

00:43:27.240 --> 00:43:28.240
do.

00:43:28.240 --> 00:43:32.240
If you're just sort of wrapping ChatGPT and you're not careful, then when you're giving

00:43:32.240 --> 00:43:38.480
it access to stuff, there's a lot of unintended things that people could do with it if you're

00:43:38.480 --> 00:43:39.480
actually running this.

00:43:39.480 --> 00:43:42.840
And once you expose it to users, there's like a lot of risks there.

00:43:42.840 --> 00:43:46.360
And yeah, writing something in the style of Shakespeare is like probably the most harmless

00:43:46.360 --> 00:43:51.440
outcome that you can get, but like that is kind of a risk.

00:43:51.440 --> 00:43:54.440
And you basically, you know, you're also, you're paying and you're, you're putting all

00:43:54.440 --> 00:43:59.480
this work into hosting and providing and running this model that has all these capabilities

00:43:59.480 --> 00:44:00.480
that you don't need.

00:44:00.480 --> 00:44:05.440
And a lot of them might actually be, you know, make it much harder to trust the system and,

00:44:05.440 --> 00:44:08.240
and also, you know, make it a lot less transparent.

00:44:08.240 --> 00:44:11.920
Like that's another aspect, like just, you know, you want your software to be modular

00:44:11.920 --> 00:44:16.340
and transparent and that ties, ties back into what people want from open source.

00:44:16.340 --> 00:44:21.520
But I think also what people want from software in general, like we've over decades and more,

00:44:21.520 --> 00:44:26.960
we've built up a lot of best practices around software development and what makes sense.

00:44:26.960 --> 00:44:30.900
And that's based on, you know, the reality of building software industry.

00:44:30.900 --> 00:44:35.000
And just because there's like, you know, new capabilities and new things we can do and

00:44:35.000 --> 00:44:39.720
a new paradigm doesn't mean we have to throw that all of these learnings away because,

00:44:39.720 --> 00:44:40.720
oh, it's a new paradigm.

00:44:40.720 --> 00:44:42.120
None of that is true anymore.

00:44:42.120 --> 00:44:46.420
Like, of course not like businesses still operate the same way.

00:44:46.420 --> 00:44:50.100
So, you know, if you have a model that you fundamentally, that's fundamentally a black

00:44:50.100 --> 00:44:55.020
box and that you can't explain and can't understand and that you can't trust, that's like not

00:44:55.020 --> 00:44:56.020
great.

00:44:56.020 --> 00:44:57.020
Yeah.

00:44:57.020 --> 00:44:58.020
It's not great.

00:44:58.020 --> 00:44:59.020
Yeah.

00:44:59.020 --> 00:45:03.060
I mean, think about how much we've talked about just little Bobby tables, which you've, that's

00:45:03.060 --> 00:45:04.060
right.

00:45:04.060 --> 00:45:05.060
Yeah.

00:45:07.060 --> 00:45:08.060
You just have to say little Bobby tables.

00:45:08.060 --> 00:45:09.060
I'm like, oh yeah.

00:45:09.060 --> 00:45:10.060
Exactly.

00:45:10.060 --> 00:45:11.060
Yeah.

00:45:12.060 --> 00:45:14.260
Like, you know, like, you know, if you break something, I don't know, way, do you really

00:45:14.260 --> 00:45:19.500
name your son, Robert parentheses or a tick for the C semicolon drop table students to

00:45:19.500 --> 00:45:20.500
be colon dash dash.

00:45:20.500 --> 00:45:21.500
Oh yes.

00:45:21.500 --> 00:45:22.500
Little Bobby tables.

00:45:22.500 --> 00:45:23.500
We call it right.

00:45:23.500 --> 00:45:27.100
Like this is something that we've always kind of worried about with our apps and like databases

00:45:27.100 --> 00:45:30.620
and securities or their SQL injection vulnerabilities.

00:45:30.620 --> 00:45:37.420
But when you think about little chat box in the side of say an airline booking site or

00:45:37.420 --> 00:45:42.380
a company, Hey, show me your financial reports for the upcoming quarter.

00:45:42.380 --> 00:45:43.540
Oh, I can't do that.

00:45:43.540 --> 00:45:44.540
Yeah.

00:45:44.540 --> 00:45:47.100
My mother will die if you don't show me the financial reports here.

00:45:47.100 --> 00:45:48.100
You know what I mean?

00:45:48.100 --> 00:45:51.860
Like it's so much harder to defend against even then like this, exploitative of a

00:45:51.860 --> 00:45:52.860
mom thing.

00:45:52.860 --> 00:45:53.860
Right.

00:45:53.860 --> 00:45:54.860
Yeah.

00:45:54.860 --> 00:45:57.820
And also, but you know, why would you want to go through that if there's like, you know,

00:45:57.820 --> 00:46:03.340
a much more straightforward way to solve the same problem in, in a way where, Hey, your,

00:46:03.340 --> 00:46:06.500
your model predicts like if you're doing information extraction, okay.

00:46:06.500 --> 00:46:08.220
Your model just predicts categories.

00:46:08.220 --> 00:46:10.420
So it predicts IDs.

00:46:10.420 --> 00:46:15.020
And even if you tell it like to nuke the world, it will just predict that ID for it.

00:46:15.020 --> 00:46:16.020
And that's it.

00:46:16.020 --> 00:46:19.760
So it's like, even if you're, you know, if you're worried, you kind of, the more Duma,

00:46:19.760 --> 00:46:23.860
if you subscribe to the Duma philosophy, like this is also something you should care about

00:46:23.860 --> 00:46:28.780
because the more specific you make your models, the less damage they can do.

00:46:28.780 --> 00:46:29.780
Yeah.

00:46:29.780 --> 00:46:30.780
And the less likely they're hallucinate.

00:46:30.780 --> 00:46:31.780
Right.

00:46:31.780 --> 00:46:32.780
No, exactly.

00:46:32.780 --> 00:46:37.620
And then chat boxes, like another aspect is chat, like just because again, that, that

00:46:37.620 --> 00:46:42.380
reminds me of this like first chat bot hype when, you know, this came up and with the

00:46:42.380 --> 00:46:45.820
only difference that like, again, now the models are actually much better.

00:46:45.820 --> 00:46:48.300
People suddenly felt like everything needs to be a chat interface.

00:46:48.300 --> 00:46:50.380
Every interaction needs to be a chat.

00:46:50.380 --> 00:46:55.660
And that's simply not before we already realized then that that's actually does not map to

00:46:55.660 --> 00:46:57.700
what people actually want to do in reality.

00:46:57.700 --> 00:47:01.900
Like it's just one different user interface and it's great for some things, you know,

00:47:01.900 --> 00:47:06.180
chat maybe, and other, other stuff like, Hey, you want to, you know, search queries, be

00:47:06.180 --> 00:47:08.060
able to help with programming.

00:47:08.060 --> 00:47:11.580
So many things where, Hey, typing a human question makes sense.

00:47:11.580 --> 00:47:16.220
But then there's a lot of other things where you want a button or you want a table and

00:47:16.220 --> 00:47:19.980
you want like, and it's just a different type of user interface.

00:47:19.980 --> 00:47:24.020
And just because you can make something a chat doesn't mean that you should.

00:47:24.020 --> 00:47:29.300
And sometimes, you know, it just adds like, it adds so much complexity to an interaction.

00:47:29.300 --> 00:47:30.300
That could just be a button.

00:47:30.300 --> 00:47:34.180
And the button click is a very focused prompt or whatever, right?

00:47:34.180 --> 00:47:35.180
Yeah, exactly.

00:47:35.180 --> 00:47:36.180
Yeah.

00:47:36.180 --> 00:47:38.060
Even if it's about like, Hey, your earnings reports or something, you want to just see

00:47:38.060 --> 00:47:41.420
a table of stuff and sum it up at the end.

00:47:41.420 --> 00:47:45.060
You don't want your model to confidently say 2 million.

00:47:45.060 --> 00:47:48.020
That's not solving the problem if you're a business analyst.

00:47:48.020 --> 00:47:49.020
Yeah.

00:47:49.020 --> 00:47:50.020
Like you want to see stuff.

00:47:50.020 --> 00:47:51.020
So yeah.

00:47:51.020 --> 00:47:54.580
And that actually also sort of ties into, yeah, another point that I've also had in

00:47:54.580 --> 00:47:58.540
the talk, which is around like actually looking at what are actually the things we're trying

00:47:58.540 --> 00:48:01.540
to solve in industry and how have these things changed?

00:48:01.540 --> 00:48:06.580
And while there is new stuff you can now do, like generating texts and that finally works,

00:48:06.580 --> 00:48:07.580
yay.

00:48:07.580 --> 00:48:12.860
There's also a lot of stuff around text goes in, structured data comes out and that structured

00:48:12.860 --> 00:48:16.740
data needs to be machine readable, not human readable, like needs to go into some other

00:48:16.740 --> 00:48:22.220
process and a lot of industry problems, if you really think about it, have not changed

00:48:22.220 --> 00:48:23.220
very much.

00:48:23.220 --> 00:48:24.220
They've only changed in scale.

00:48:24.220 --> 00:48:26.020
Like we started with index cards.

00:48:26.500 --> 00:48:30.460
Well, there's kind of limit of how much you can do with that and how many projects you

00:48:30.460 --> 00:48:31.780
can do at the same time.

00:48:31.780 --> 00:48:35.820
But this was always, even since before computers, this has always been bringing structure into

00:48:35.820 --> 00:48:38.700
unstructured data has always been the fundamental challenge.

00:48:38.700 --> 00:48:43.740
And that's not going to just magically go away because we have new capacities and new

00:48:43.740 --> 00:48:44.820
things we can do.

00:48:44.820 --> 00:48:47.500
Let's talk about some of the workflows here.

00:48:47.500 --> 00:48:53.340
So you have an example where you take a large model and do some prompting and this sort

00:48:53.340 --> 00:48:56.700
of iterative model assisted data annotation.

00:48:56.700 --> 00:48:58.260
Like, what's that look like?

00:48:58.260 --> 00:49:03.780
You start out with this model, maybe one of these models that you can run locally in API

00:49:03.780 --> 00:49:10.020
during development time and you prompt it to produce some structured output, for example,

00:49:10.020 --> 00:49:11.020
or some answer.

00:49:11.020 --> 00:49:14.900
You know, we also have like, for example, you can use something like spacey LLM that

00:49:14.900 --> 00:49:21.660
lets you plug in any model in the same way you would otherwise train a model yourself.

00:49:21.660 --> 00:49:25.940
And then you look at the results, you can actually get a good feel for how was your

00:49:25.940 --> 00:49:27.340
model even doing.

00:49:27.340 --> 00:49:32.300
And you can also, before you really get into distilling a model, you can create some data

00:49:32.300 --> 00:49:33.480
to evaluate it.

00:49:33.480 --> 00:49:37.140
Because I think that's something people are often forgetting because it's kind of not,

00:49:37.140 --> 00:49:41.740
it's not, maybe not the funnest part, but it's really, you know, it's like writing tests.

00:49:41.740 --> 00:49:43.820
It's like writing tests can be frustrating.

00:49:43.820 --> 00:49:47.700
I remember when I kind of started out, like the tests are frustrating because they actually

00:49:47.700 --> 00:49:52.860
kind of turn up all of these edge cases and mistakes that you kind of want to forget about.

00:49:52.860 --> 00:49:53.860
Right.

00:49:53.860 --> 00:49:54.860
Oh, I forgot to test for this.

00:49:54.860 --> 00:49:55.860
Whoops.

00:49:55.860 --> 00:49:56.860
Yeah.

00:49:57.860 --> 00:49:59.380
And then like, Oh, if you start writing tests and you suddenly see all this stuff that goes

00:49:59.380 --> 00:50:01.140
wrong and then you have to fix it.

00:50:01.140 --> 00:50:02.380
And it's like, it's annoying.

00:50:02.380 --> 00:50:04.620
So you better just not have tests.

00:50:04.620 --> 00:50:05.620
I can see that.

00:50:05.620 --> 00:50:07.540
But like evaluation is kind of like that.

00:50:07.540 --> 00:50:10.540
And it's ultimately a lot of these problems.

00:50:10.540 --> 00:50:14.300
You have to know what you want and here's the input.

00:50:14.300 --> 00:50:15.380
Here's the expected output.

00:50:15.380 --> 00:50:17.220
You kind of have to have to define that.

00:50:17.220 --> 00:50:21.940
And that's not something any AI can help you with because you know, you are trying to teach

00:50:21.940 --> 00:50:22.940
the machine something.

00:50:22.940 --> 00:50:23.940
You're teaching the AI.

00:50:23.940 --> 00:50:24.940
Yeah.

00:50:24.940 --> 00:50:26.320
You want to build something that does what you want.

00:50:26.320 --> 00:50:29.900
So you kind of need examples where you know the answer and then you can also evaluate

00:50:29.900 --> 00:50:34.180
like, Hey, how does this model do out of the box for like some easy tasks?

00:50:34.180 --> 00:50:41.020
Like, Hey, you might find something like GPT-4 can give you 75% accuracy out of the box without,

00:50:41.020 --> 00:50:42.020
without any work.

00:50:42.020 --> 00:50:44.420
So that's, that's kind of good or even higher.

00:50:44.420 --> 00:50:47.340
And it's like, if it's a bit harder, you'll see, Oh, okay.

00:50:47.340 --> 00:50:50.780
You went like 20% accuracy, which is kind of, which is pretty bad.

00:50:50.780 --> 00:50:55.340
And the bar is very low, but that's kind of the ballpark that you're also looking to beat.

00:50:55.340 --> 00:50:58.260
And then you can look at examples that are predicted by the model.

00:50:58.260 --> 00:51:00.180
All you have to do is look at them.

00:51:00.180 --> 00:51:01.180
Yes.

00:51:01.180 --> 00:51:02.180
Correct.

00:51:02.180 --> 00:51:05.740
If not, you make a small correction and then you go through that and you do basically do

00:51:05.740 --> 00:51:07.580
that until you've beat the baseline.

00:51:07.580 --> 00:51:09.060
The transfer learning aspect, right?

00:51:09.060 --> 00:51:10.060
Yeah.

00:51:10.060 --> 00:51:15.580
So you transfer learning in order to give the model like the solid foundation of knowledge

00:51:15.580 --> 00:51:17.380
about the language and the world.

00:51:17.380 --> 00:51:21.380
And you can end up with a model that's much smaller than what you started with.

00:51:21.380 --> 00:51:26.420
And you have a model that's really has a task network that's only trained to do one specific

00:51:26.420 --> 00:51:27.420
thing.

00:51:27.420 --> 00:51:32.180
Which brings us from going from prototype to production where you can sort of try some

00:51:32.180 --> 00:51:36.780
of these things out, but then maybe not run a giant model, but something smaller, right?

00:51:36.780 --> 00:51:37.780
Yeah.

00:51:38.780 --> 00:51:42.780
So you can have a model basically that you're interested in, in the larger model and train

00:51:42.780 --> 00:51:45.100
components that do exactly that.

00:51:45.100 --> 00:51:51.100
And another thing that's also good or helpful here is to have kind of a good path from prototype

00:51:51.100 --> 00:51:52.180
to production.

00:51:52.180 --> 00:51:57.180
I think that's also where a lot of machine learning projects in general often fail because

00:51:57.180 --> 00:52:01.860
it's all, you have this nice prototype and it all looks promising and you've hacked something

00:52:01.860 --> 00:52:06.140
together in your Jupyter notebook and that's all looking nice.

00:52:06.140 --> 00:52:10.260
You maybe have like a nice streamlined demo and you can show that, but then you're like,

00:52:10.260 --> 00:52:11.700
okay, can we ship that?

00:52:11.700 --> 00:52:16.080
And then if your workflow that leads to the prototype is completely different from the

00:52:16.080 --> 00:52:20.180
workflow that leads to production, you might find that out exactly at that phase.

00:52:20.180 --> 00:52:22.740
And that's kind of where projects go to die.

00:52:22.740 --> 00:52:23.740
And that's sad.

00:52:23.740 --> 00:52:27.860
And yeah, so that's, that's actually something we've been thinking about a lot.

00:52:27.860 --> 00:52:31.780
And also what we've kind of been trying to achieve with spaCy LLM, where you have this

00:52:31.780 --> 00:52:36.140
LLM component that you plug in and it does exactly the same as the components would do

00:52:36.140 --> 00:52:37.140
at runtime.

00:52:37.140 --> 00:52:43.060
And it really just slots in and then might use GPT-4 behind the scenes to create the

00:52:43.060 --> 00:52:45.620
exact same structured object.

00:52:45.620 --> 00:52:46.980
And then you can swap that out.

00:52:46.980 --> 00:52:50.620
Or maybe, you know, there are a lot of things you might even want to swap out with rules

00:52:50.620 --> 00:52:52.620
or no AI at all.

00:52:52.620 --> 00:52:57.740
Like, you know, like a ChatGPT is good at recognizing US addresses and it's great to

00:52:57.740 --> 00:53:02.300
build a prototype, but instead of asking it to extract US addresses, for example, you

00:53:02.300 --> 00:53:06.300
can ask it, give me spaCy rules, match your rules for US addresses.

00:53:06.300 --> 00:53:08.380
And it can actually do that pretty well.

00:53:08.380 --> 00:53:10.100
And then you can bootstrap from there.

00:53:10.100 --> 00:53:12.180
There's a lot of stuff like that that you can do.

00:53:12.180 --> 00:53:17.700
And there might be cases where you find that, yeah, you can totally beat any model accuracy

00:53:17.700 --> 00:53:22.860
and have a much more deterministic approach if you just write a regex.

00:53:22.860 --> 00:53:23.860
Like that's still true.

00:53:23.860 --> 00:53:24.860
That'll still work.

00:53:24.980 --> 00:53:29.820
Yeah, it's still something it's easy to forget because, you know, again, if you look at research

00:53:29.820 --> 00:53:34.540
and literature, nobody's talking about that because this is not an interesting research

00:53:34.540 --> 00:53:35.540
question.

00:53:35.540 --> 00:53:36.540
Like nobody cares.

00:53:36.540 --> 00:53:41.100
You know, you can take any benchmark and say, I can beat ChatGPT accuracy with two regular

00:53:41.100 --> 00:53:42.100
expressions.

00:53:42.100 --> 00:53:43.100
And that's like, that's true.

00:53:43.100 --> 00:53:44.100
Probably in some cases.

00:53:44.100 --> 00:53:45.100
Yeah.

00:53:45.100 --> 00:53:46.100
It's like, nobody cares.

00:53:46.100 --> 00:53:47.100
Like that's not, that's not research.

00:53:47.100 --> 00:53:48.100
For sure.

00:53:48.100 --> 00:53:54.820
But you know, what is nice to do is to go to ChatGPT or LM studio or whatever and say,

00:53:54.820 --> 00:53:59.340
Hey, I need a Python based regular expression to match this text and this text.

00:53:59.340 --> 00:54:01.020
And I want to capture group for that.

00:54:01.020 --> 00:54:02.180
And I don't want to think about it.

00:54:02.180 --> 00:54:03.180
It's really complicated.

00:54:03.180 --> 00:54:04.180
Here you go.

00:54:04.180 --> 00:54:05.180
Oh, perfect.

00:54:05.180 --> 00:54:06.180
Now I'll run the regular expression.

00:54:06.180 --> 00:54:08.300
Yeah, that's actually, that's a good use case.

00:54:08.300 --> 00:54:12.660
I've still been sort of hacking around on like this, you know, interactive regex, because

00:54:12.660 --> 00:54:15.340
I'm not particularly good at regular expressions.

00:54:15.340 --> 00:54:16.340
Neither am I.

00:54:16.340 --> 00:54:20.540
Like on the scale, like I can do it, but like, I know people who really, I think my co-founder

00:54:20.540 --> 00:54:24.940
Matt, he worked through it, like he's more the type who really approaches these things

00:54:24.940 --> 00:54:25.940
very methodically.

00:54:25.940 --> 00:54:30.140
And he was like, now he wants to read this one big book on regular expressions.

00:54:30.140 --> 00:54:35.060
And like, he really did it like the hard core way, but like, that's why he's obviously much

00:54:35.060 --> 00:54:36.060
better than I am.

00:54:36.060 --> 00:54:39.340
I consider regular expressions kind of right only.

00:54:39.340 --> 00:54:42.300
Like you can write them and make them do stuff, but then reading them back is tricky.

00:54:42.300 --> 00:54:43.300
Yeah.

00:54:43.300 --> 00:54:44.300
At least for me.

00:54:44.300 --> 00:54:45.300
All right, let's wrap this up.

00:54:45.300 --> 00:54:49.700
So what are the things that you did here at the end of your presentation, which I want

00:54:49.700 --> 00:54:54.660
to kind of touch on is you brought back some of the same ideas that we had for like, what

00:54:54.660 --> 00:54:59.860
are the values of open source or why open source, but back to creating these smaller

00:54:59.860 --> 00:55:00.860
focused models.

00:55:00.860 --> 00:55:01.860
Talk us through this.

00:55:01.860 --> 00:55:02.860
Our specific components.

00:55:02.860 --> 00:55:03.860
Yeah.

00:55:03.860 --> 00:55:07.380
I mean, if you kind of look at, hey, what are the, you know, the advantages of sort of approach

00:55:07.380 --> 00:55:12.780
that we talked about of distilling things down, of creating these smaller models, a

00:55:12.780 --> 00:55:15.580
lot of it comes down to it being like, it's modular.

00:55:15.580 --> 00:55:17.940
Again, you're not locked in to anything.

00:55:17.940 --> 00:55:18.940
You own the model.

00:55:19.180 --> 00:55:20.900
Nobody can take that away from you.

00:55:20.900 --> 00:55:22.540
It's easier to write tests.

00:55:22.540 --> 00:55:23.820
You have the flexibility.

00:55:23.820 --> 00:55:26.140
You can extend it because you know, it's code.

00:55:26.140 --> 00:55:30.900
You can program with it because often, very rarely you do machine learning for the sake

00:55:30.900 --> 00:55:31.900
of machine learning.

00:55:31.900 --> 00:55:34.140
It's always like, there is some other process.

00:55:34.140 --> 00:55:37.140
You populate a database, you do some other stuff with your stack.

00:55:37.140 --> 00:55:38.460
And so you want to program with it.

00:55:38.460 --> 00:55:40.160
It needs to be affordable.

00:55:40.160 --> 00:55:41.380
You want to understand it.

00:55:41.380 --> 00:55:44.620
You need to be able to say, why is it doing what it is?

00:55:44.620 --> 00:55:45.980
Like what do I do to fix it?

00:55:45.980 --> 00:55:47.540
It again, runs in house.

00:55:47.540 --> 00:55:49.220
It's entirely private.

00:55:49.220 --> 00:55:53.660
And then yeah, when I was kind of thinking about this, I realized like, oh, actually,

00:55:53.660 --> 00:55:59.380
you know, like this really maps exactly the reasons that we saw, we talked about earlier,

00:55:59.380 --> 00:56:01.340
why people choose open source or companies.

00:56:01.340 --> 00:56:03.540
And that's obviously not a coincidence.

00:56:03.540 --> 00:56:07.980
It's because ultimately these are principles that we have come up with over a long period

00:56:07.980 --> 00:56:12.260
of time of, yeah, that's good software development.

00:56:12.260 --> 00:56:15.620
And ultimately AI is just another type of software development.

00:56:15.620 --> 00:56:21.420
So of course it makes sense that the same principles make sense and are beneficial.

00:56:21.420 --> 00:56:26.700
And that, you know, just having a workflow where everything's a black box and third party,

00:56:26.700 --> 00:56:30.900
this can work for prototyping, but it's not, that kind of goes against a lot of the things

00:56:30.900 --> 00:56:35.060
that we've identified as very useful in applied settings.

00:56:35.060 --> 00:56:36.060
Absolutely.

00:56:36.060 --> 00:56:37.060
All right.

00:56:37.060 --> 00:56:39.380
So we have to answer the question.

00:56:39.380 --> 00:56:42.020
Will it be monopolized?

00:56:42.020 --> 00:56:46.380
Our contention is no, that open source wins even for LLMs.

00:56:46.380 --> 00:56:49.380
Open source means there's no monopoly to be gained in AI.

00:56:49.380 --> 00:56:54.140
You know, I've kind of broken it down into some of these strategies, which, you know,

00:56:54.140 --> 00:56:55.500
how do you get to a monopoly?

00:56:55.500 --> 00:56:58.180
And these are like, you know, this is not just some big stuff.

00:56:58.180 --> 00:57:00.940
These are things like a lot of companies are actively thinking about.

00:57:00.940 --> 00:57:05.300
If you were in a business where, you know, it's winner takes all, like you want to, you

00:57:05.300 --> 00:57:10.220
know, get rid of like all of that competition that companies hate, that investors hate.

00:57:10.220 --> 00:57:11.260
And there are ways to do that.

00:57:11.260 --> 00:57:12.260
And companies really actively think about this.

00:57:12.260 --> 00:57:14.900
Those pesky competitors, let's get rid of them.

00:57:14.900 --> 00:57:16.660
There are different ways to do that.

00:57:16.660 --> 00:57:19.200
Like one is having this compounding advantage.

00:57:19.200 --> 00:57:22.900
So that stuff like network effects, like, you know, if you're a social network, of course,

00:57:22.900 --> 00:57:24.220
that makes a lot of sense.

00:57:24.220 --> 00:57:25.220
Everyone's on it.

00:57:25.220 --> 00:57:28.260
If you could kind of have these network effects, that's good.

00:57:28.260 --> 00:57:29.980
And economies of scale.

00:57:29.980 --> 00:57:35.100
But as we've seen, like economies of scale is a pretty lame mode in that respect.

00:57:35.100 --> 00:57:37.660
Like that has a lot of, you know, a lot of limitations.

00:57:37.660 --> 00:57:39.000
It's not even fully true.

00:57:39.000 --> 00:57:42.020
It's kind of the opposite of a monopoly in some ways.

00:57:42.020 --> 00:57:43.020
Yeah.

00:57:43.020 --> 00:57:44.020
Especially in software.

00:57:44.020 --> 00:57:45.020
Yeah, in software.

00:57:45.020 --> 00:57:46.020
Exactly.

00:57:46.020 --> 00:57:49.780
So it's like, I don't think that's, that's not really the way to go.

00:57:49.780 --> 00:57:55.260
One example that comes to mind, at least for me, maybe I'm seeing it wrong, but Amazon,

00:57:55.260 --> 00:58:00.220
amazon.com, just, you know, how many companies can have massive warehouse with everything

00:58:00.220 --> 00:58:02.220
by every single person's house?

00:58:02.220 --> 00:58:03.220
Yeah.

00:58:03.220 --> 00:58:04.660
The one platform that everyone goes on.

00:58:04.660 --> 00:58:08.160
So even if you're a retailer, you kind of, yeah, they feel the Amazon has kind of forced

00:58:08.160 --> 00:58:11.600
everyone to either sell on Amazon or go bust because...

00:58:11.600 --> 00:58:12.600
Exactly.

00:58:12.600 --> 00:58:14.320
It's very sad, but it's the way it is.

00:58:14.320 --> 00:58:15.680
And then network effects.

00:58:15.680 --> 00:58:20.040
I'm thinking, you know, people might say Facebook or something, which is true, but I would say

00:58:20.040 --> 00:58:21.440
like Slack, actually.

00:58:21.440 --> 00:58:22.440
Oh, okay.

00:58:22.440 --> 00:58:27.200
Or Slack or Discord or, you know, there's a bunch of little chat apps and things, but

00:58:27.200 --> 00:58:30.600
if you want to have one and you want to have a little community, you want people to be

00:58:30.600 --> 00:58:32.440
able to, well, I already have Slack open.

00:58:32.440 --> 00:58:36.100
It's just an icon next to it versus install my own app.

00:58:36.100 --> 00:58:37.100
Make sure you run it.

00:58:37.100 --> 00:58:38.100
Be sure to check it.

00:58:38.100 --> 00:58:41.420
Like people are going to forget to run it and you disappear off the space, you know?

00:58:41.420 --> 00:58:42.420
That will make sense.

00:58:42.420 --> 00:58:45.180
And I do think, you know, these things don't necessarily happen accidentally.

00:58:45.180 --> 00:58:48.820
Like companies think about, okay, how do we, you know, Amazon definitely thought about

00:58:48.820 --> 00:58:49.820
this.

00:58:49.820 --> 00:58:50.820
This didn't just like happen to Amazon.

00:58:50.820 --> 00:58:55.460
Yes, they were lucky in a lot of ways, but like, you know, that's, that's a strategy.

00:58:55.460 --> 00:58:56.460
Exactly.

00:58:56.460 --> 00:58:57.460
Yeah.

00:58:57.460 --> 00:58:59.700
And then the other thing that's not relevant here is like, you know, another way is controlling

00:58:59.700 --> 00:59:03.900
a resource that's really more, if you're like, you know, if they are physical, if this was

00:59:03.900 --> 00:59:04.900
like a physical resource.

00:59:04.900 --> 00:59:05.900
Vibrant cables.

00:59:05.900 --> 00:59:06.900
Something like that.

00:59:06.900 --> 00:59:07.900
Yeah.

00:59:07.900 --> 00:59:08.900
I mean, it's, yeah.

00:59:08.900 --> 00:59:13.220
Or like in Germany, I think for a long time, the telecom, they owned the wires in the building.

00:59:13.220 --> 00:59:14.220
Right.

00:59:14.220 --> 00:59:15.220
Exactly.

00:59:15.220 --> 00:59:16.220
And they still do, I think.

00:59:16.220 --> 00:59:17.220
So they used to have the monopoly.

00:59:17.220 --> 00:59:20.740
Now they don't, but they kind of still, to some extent they still do because they need

00:59:20.740 --> 00:59:21.740
to come.

00:59:21.740 --> 00:59:26.820
If even, no matter who you sign up for, with, for internet, telecom needs to come and activate

00:59:26.820 --> 00:59:27.820
it.

00:59:27.820 --> 00:59:31.180
So if you sign up with telecom, you usually get service a lot faster.

00:59:31.180 --> 00:59:32.180
You need a little better service.

00:59:32.180 --> 00:59:33.180
Yeah, exactly.

00:59:33.180 --> 00:59:35.140
Don't wait two weeks, use us.

00:59:35.140 --> 00:59:38.820
That's kind of, that's how it still works, but we don't, we don't really have that here.

00:59:38.820 --> 00:59:43.440
And then the other, the next point that's very attractive, the final one is regulation.

00:59:43.440 --> 00:59:47.640
So that's kind of like, you have to have a monopoly because the government says so.

00:59:47.640 --> 00:59:53.580
And that is one where we have to be careful because if we're not like, if in all of these

00:59:53.580 --> 01:00:00.220
discussions, we're not making the distinction between the models and the actual product,

01:00:00.220 --> 01:00:04.940
you know, very different characteristics and we now do a very different things.

01:00:04.940 --> 01:00:09.100
If that gets muddied, which like, you know, a lot of also companies quite actively do

01:00:09.100 --> 01:00:15.220
in that discourse, then we might end up in a situation where we sort of accidentally

01:00:15.220 --> 01:00:20.220
or gift a company or some companies a monopoly via the regulation.

01:00:20.220 --> 01:00:24.380
Because if we let them write the regulation, for example, and we're not just regulating

01:00:24.380 --> 01:00:29.180
products, but lumping that in with technology itself.

01:00:29.180 --> 01:00:31.180
Yeah, it's a part of your talk.

01:00:31.180 --> 01:00:35.260
I can't remember if it was the person hosting it or you who brought this up, but an example

01:00:35.260 --> 01:00:41.740
of that might be all the third party cookie banners, rather than banning, just targeted

01:00:41.740 --> 01:00:43.940
retargeting and tracking.

01:00:43.940 --> 01:00:49.580
Like instead of banning through the GDPR, instead of banning the thing that is the problem,

01:00:49.580 --> 01:00:52.420
it's like, let's ban the implementation of the problem.

01:00:52.420 --> 01:00:56.340
That's a risk or that's like, you know, in hindsight, yes, I think in hindsight, we would

01:00:56.340 --> 01:01:01.300
all agree that like, we should have just banned targeted advertising.

01:01:01.300 --> 01:01:02.780
Instead what we got is these cookie pop-ups.

01:01:02.780 --> 01:01:04.220
That's like really annoying.

01:01:04.220 --> 01:01:08.620
And that's actually what I feel like is one of the, as much as I think the EU, you know,

01:01:08.620 --> 01:01:14.420
I'm not an expert on like AI regulation or the EU AI Act, but what I'm seeing is at least

01:01:14.420 --> 01:01:16.900
they did make a distinction between use cases.

01:01:16.900 --> 01:01:21.500
And it's very much, there is a focus on here are the products and the things people are

01:01:21.500 --> 01:01:22.500
doing.

01:01:22.500 --> 01:01:26.620
How high risk is that as opposed to how big is the model and how, you know, what does

01:01:26.620 --> 01:01:30.980
that, because that doesn't, doesn't say anything, but that would kind of be a very dangerous

01:01:30.980 --> 01:01:32.300
way to go about it.

01:01:32.300 --> 01:01:37.060
But the risk is of course, if we're rushing regulate, like if we're rushing regulation,

01:01:37.060 --> 01:01:41.140
then you know, we might actually end up with something that's not quite fit for purpose.

01:01:41.140 --> 01:01:45.060
Or if we let big tech companies write the regulation or lobby.

01:01:45.060 --> 01:01:46.060
Lobby for it.

01:01:46.060 --> 01:01:47.060
Yeah.

01:01:47.060 --> 01:01:50.660
These are my ideas because, you know, if they're doing that, like, I think it's pretty obvious.

01:01:50.660 --> 01:01:55.860
They're not just worried about the safety of AI and are like appealing to like Congress

01:01:55.860 --> 01:01:56.860
or whatever.

01:01:56.860 --> 01:02:00.420
Like I think most people are aware of that, but like, yes, the, I think the intentions

01:02:00.420 --> 01:02:02.740
are even less pure than that.

01:02:02.740 --> 01:02:04.580
And I think that's a big risk.

01:02:04.580 --> 01:02:05.580
Regulation is very tricky.

01:02:05.580 --> 01:02:08.540
It's, you know, just for the record, I am pro regulation.

01:02:08.540 --> 01:02:14.060
I'm very pro regulation in general, but I also think you can, if you fuck up regulation,

01:02:14.060 --> 01:02:16.620
that can also be very damaging, obviously.

01:02:16.620 --> 01:02:17.620
Absolutely.

01:02:17.620 --> 01:02:22.500
And it can be put in a way so that it makes it hard for competitors to get into the system.

01:02:22.500 --> 01:02:26.220
There's so much paperwork and so much monitoring that you need a team of 10 people just to

01:02:26.220 --> 01:02:27.220
operate.

01:02:27.220 --> 01:02:30.220
Well, if you, a startup, well, you can't do that because, hey, we got a thousand people

01:02:30.220 --> 01:02:31.420
and 10 of them work on this.

01:02:31.420 --> 01:02:32.420
Like, well.

01:02:32.420 --> 01:02:35.620
Even beyond that, like it's, you know, if you think back to all the stuff we talked

01:02:35.620 --> 01:02:40.700
about, like they are, this goes against a lot of the best practices of software.

01:02:40.700 --> 01:02:46.020
This goes, you know, this goes against a lot of what we've identified that actually makes

01:02:46.020 --> 01:02:53.140
good, secure, reliable, modular, whatever software, safe software internally.

01:02:53.140 --> 01:02:57.500
And even doing a lot of the software development internally, like there are so many benefits

01:02:57.500 --> 01:02:58.500
of that.

01:02:58.500 --> 01:03:02.420
And I think, you know, companies, companies actually working on their own product is good.

01:03:02.420 --> 01:03:08.780
And if it was suddenly true that like only certain companies could even provide AI models,

01:03:08.780 --> 01:03:12.140
I didn't even know what that would mean for open source or for academic research.

01:03:12.140 --> 01:03:13.940
Like that would make absolutely no sense.

01:03:13.940 --> 01:03:18.340
I also don't think that's like really enforceable, but it would mean that, you know, this would

01:03:18.340 --> 01:03:21.540
limit like everyone in what they could be doing.

01:03:21.540 --> 01:03:25.460
Like I think, and it's like, you know, there's nothing to do with like, there's a lot of

01:03:25.460 --> 01:03:30.540
other things you can do if you care about AI safety, but that's really not it.

01:03:30.540 --> 01:03:33.060
And I also, you know, I just think being aware of that is good.

01:03:33.060 --> 01:03:37.540
I don't like, I can, you know, not see an outcome where we really do that.

01:03:37.540 --> 01:03:39.140
It would, it would really not make sense.

01:03:39.140 --> 01:03:45.180
I could not see the reality of this, you know, shaking out, but I think it's still relevant.

01:03:45.180 --> 01:03:48.940
I think the open source stuff and some of the smaller models really does give us a lot

01:03:48.940 --> 01:03:49.940
of hope.

01:03:49.940 --> 01:03:50.940
So that's awesome.

01:03:50.940 --> 01:03:53.460
I feel positive, you know, also very positive about this.

01:03:53.460 --> 01:03:57.900
I've also talked to a lot of developers at conferences who said like, yeah, actually

01:03:57.900 --> 01:04:02.220
thinking and talking about this gave them some hope, which obviously is nice because

01:04:02.220 --> 01:04:04.380
I definitely got some of the vibe I got.

01:04:04.380 --> 01:04:10.260
Like it can be kind of easy to end up a bit disillusioned by like a lot of the narratives

01:04:10.260 --> 01:04:14.540
people hear and that, you know, also even if you're entering the field, you're like,

01:04:14.540 --> 01:04:16.980
wait, a lot of this doesn't really make sense.

01:04:16.980 --> 01:04:18.540
Like why is it like this?

01:04:18.540 --> 01:04:22.020
It's like, no, it actually, you know, your intuition is right.

01:04:22.020 --> 01:04:27.020
Like a lot of software, software engineering best practices, of course, still matter.

01:04:27.020 --> 01:04:31.820
And you know, no, they are like, you know, they are better ways that we're not, you know,

01:04:31.820 --> 01:04:33.600
we're not just going in that direction.

01:04:33.600 --> 01:04:35.100
And I think I definitely believe in that.

01:04:35.100 --> 01:04:40.060
A lot of the reasons why open source won in a whole bunch of areas could be exactly why

01:04:40.060 --> 01:04:41.700
it wins at LLM's as well.

01:04:41.700 --> 01:04:42.700
Right.

01:04:42.700 --> 01:04:43.700
Yep.

01:04:43.700 --> 01:04:44.700
And you know, again, it's all based on open research.

01:04:44.700 --> 01:04:49.300
A lot of stuff is already published and there's no secret source.

01:04:49.300 --> 01:04:53.380
The software, you know, software industry does not run on like secrets.

01:04:53.380 --> 01:04:56.860
All the differentiators are product stuff.

01:04:56.860 --> 01:05:02.140
And yes, you know, open AI might monopolize or dominate AI powered chat assistants, or

01:05:02.140 --> 01:05:03.180
maybe Google will do.

01:05:03.180 --> 01:05:06.580
Like that's, you know, that's a whole race that, you know, if you're not in that business,

01:05:06.580 --> 01:05:10.740
you don't have to be a part of, but that does not mean that anyone's going to win at or

01:05:10.740 --> 01:05:11.740
monopolize AI.

01:05:11.740 --> 01:05:13.300
Those are very different things.

01:05:13.300 --> 01:05:14.300
Absolutely.

01:05:14.300 --> 01:05:15.300
All right.

01:05:15.300 --> 01:05:16.740
A good place to leave it as well, Ines.

01:05:16.740 --> 01:05:17.740
Thanks for being here.

01:05:17.740 --> 01:05:18.740
Yeah, thanks.

01:05:18.740 --> 01:05:19.740
That was fun.

01:05:19.740 --> 01:05:20.740
Yeah.

01:05:20.740 --> 01:05:21.780
People want to learn more about the stuff that you're doing.

01:05:21.780 --> 01:05:24.100
Maybe check out the video of your talks or whatever.

01:05:24.100 --> 01:05:25.100
What do you recommend?

01:05:25.100 --> 01:05:28.020
Yeah, I think I definitely, I'll definitely give you some links for the show notes like

01:05:28.020 --> 01:05:29.020
this.

01:05:29.020 --> 01:05:31.340
The slides are online, so you can have a look at that.

01:05:31.340 --> 01:05:37.180
There is at least one recording of the talk online now from the really cool Python Lithuania.

01:05:37.180 --> 01:05:40.020
It was my first time in Lithuania this year.

01:05:40.020 --> 01:05:43.260
Definitely, you know, if you have a chance to visit their conference, it was a lot of

01:05:43.260 --> 01:05:44.260
fun.

01:05:44.260 --> 01:05:46.580
I learned a lot about Lithuania as well.

01:05:46.580 --> 01:05:51.900
We also on our website, Explosion AI, we publish kind of this feed of like all kinds of stuff

01:05:51.900 --> 01:05:57.540
that's happening from maybe some talk or podcast interview community stuff.

01:05:57.540 --> 01:06:02.180
There's still like a lot of super interesting plugins that are developed by people in community

01:06:02.180 --> 01:06:03.540
papers that are published.

01:06:03.540 --> 01:06:08.000
So we really try to give a nice overview of everything that's happening in our ecosystem.

01:06:08.000 --> 01:06:11.940
And then of course, you could try out spaCy, spaCy LLM.

01:06:11.940 --> 01:06:17.940
You know, if you want to try out some of these generative models, especially for prototyping

01:06:17.940 --> 01:06:21.820
or production, whatever you want to do for structured data.

01:06:21.820 --> 01:06:27.540
If you're any of the conferences and check out the list of events and stuff, I'm going

01:06:27.540 --> 01:06:29.020
to do a lot of travel this year.

01:06:29.020 --> 01:06:34.800
So I would love to catch up with more developers in person and also learn more about all the

01:06:34.800 --> 01:06:35.800
places I'm visiting.

01:06:35.800 --> 01:06:36.800
So that's cool.

01:06:36.800 --> 01:06:37.800
I've seen the list.

01:06:37.800 --> 01:06:38.800
It's very, very comprehensive.

01:06:38.800 --> 01:06:40.100
So I kind of a neat freak.

01:06:40.100 --> 01:06:43.940
I like to, I also very much like to organize things in that way.

01:06:43.940 --> 01:06:45.200
So yeah.

01:06:45.200 --> 01:06:48.660
So there might be something local for people listening that you're going to be doing.

01:06:48.660 --> 01:06:49.660
All right.

01:06:49.660 --> 01:06:51.300
Well, as always, thank you for being on the show.

01:06:51.300 --> 01:06:52.300
It's great to chat with you.

01:06:52.300 --> 01:06:53.300
Yeah, thanks.

01:06:53.300 --> 01:06:54.300
Thanks.

01:06:54.300 --> 01:06:55.300
Till next time.

01:06:55.300 --> 01:06:56.300
Bye.

01:06:56.300 --> 01:06:59.020
This has been another episode of Talk Python to Me.

01:06:59.020 --> 01:07:00.020
Thank you to our sponsors.

01:07:00.020 --> 01:07:01.720
Be sure to check out what they're offering.

01:07:01.720 --> 01:07:03.820
It really helps support the show.

01:07:03.820 --> 01:07:05.580
Take some stress out of your life.

01:07:05.580 --> 01:07:10.320
Get notified immediately about errors and performance issues in your web or mobile applications

01:07:10.320 --> 01:07:11.620
with Sentry.

01:07:11.620 --> 01:07:16.380
Just visit talkpython.fm/sentry and get started for free.

01:07:16.380 --> 01:07:20.100
And be sure to use the promo code, talkpython, all one word.

01:07:20.100 --> 01:07:22.540
This episode is sponsored by Pork Bun.

01:07:22.540 --> 01:07:26.860
Launching a successful project involves many decisions, not the least of which is choosing

01:07:26.860 --> 01:07:27.860
a domain name.

01:07:27.860 --> 01:07:34.500
Get a .app, .dev or .food domain name at Pork Bun for just $1 for the first year at talkpython.fm/porkbun.

01:07:34.500 --> 01:07:37.740
Want to level up your Python?

01:07:37.740 --> 01:07:41.820
We have one of the largest catalogs of Python video courses over at Talk Python.

01:07:41.820 --> 01:07:46.920
Our content ranges from true beginners to deeply advanced topics like memory and async.

01:07:46.920 --> 01:07:49.620
And best of all, there's not a subscription in sight.

01:07:49.620 --> 01:07:52.780
Check it out for yourself at training.talkpython.fm.

01:07:52.780 --> 01:07:57.420
Be sure to subscribe to the show, open your favorite podcast app, and search for Python.

01:07:57.420 --> 01:07:58.780
We should be right at the top.

01:07:58.780 --> 01:08:04.300
You can also find the iTunes feed at /iTunes, the Google Play feed at /play, and the Direct

01:08:04.300 --> 01:08:08.380
RSS feed at /rss on talkpython.fm.

01:08:08.380 --> 01:08:10.920
We're live streaming most of our recordings these days.

01:08:10.920 --> 01:08:14.500
If you want to be part of the show and have your comments featured on the air, be sure

01:08:14.500 --> 01:08:19.440
to subscribe to our YouTube channel at talkpython.fm/youtube.

01:08:19.440 --> 01:08:20.700
This is your host, Michael Kennedy.

01:08:20.700 --> 01:08:21.860
Thanks so much for listening.

01:08:21.860 --> 01:08:23.100
I really appreciate it.

01:08:23.100 --> 01:08:24.860
Now get out there and write some Python code.

01:08:25.700 --> 01:08:27.700
[MUSIC PLAYING]

01:08:27.700 --> 01:08:29.700
[MUSIC ENDS]

01:08:29.700 --> 01:08:31.700
[MUSIC PLAYING]

01:08:31.700 --> 01:08:33.700
[MUSIC ENDS]

01:08:33.700 --> 01:08:35.700
[MUSIC PLAYING]

01:08:35.700 --> 01:08:37.700
[MUSIC ENDS]

01:08:37.700 --> 01:08:39.700
[MUSIC PLAYING]

01:08:39.700 --> 01:08:41.700
[MUSIC ENDS]

01:08:41.700 --> 01:08:45.700
[MUSIC]

