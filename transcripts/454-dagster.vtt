WEBVTT

00:00:00.000 --> 00:00:05.040
Do you have data that you pull from external sources or that is generated and appears at

00:00:05.040 --> 00:00:06.160
your digital doorstep?

00:00:06.160 --> 00:00:11.120
I bet that data needs processed, filtered, transformed, distributed, and much more.

00:00:11.120 --> 00:00:16.600
One of the biggest tools to create these data pipelines with Python is Dagster.

00:00:16.600 --> 00:00:20.080
And we're fortunate to have Pedram Navid on the show to tell us about it.

00:00:20.080 --> 00:00:24.240
Pedram is the head of data engineering and dev rel at Dagster Labs.

00:00:24.240 --> 00:00:28.600
And we're talking data pipelines this week here at Talk Python.

00:00:28.600 --> 00:00:48.560
This is Talk Python to Me, episode 454, recorded January 11th, 2024.

00:00:48.560 --> 00:00:51.980
Welcome to Talk Python to Me, a weekly podcast on Python.

00:00:51.980 --> 00:00:53.840
This is your host, Michael Kennedy.

00:00:53.840 --> 00:00:58.760
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,

00:00:58.760 --> 00:01:01.360
both on fosstodon.org.

00:01:01.360 --> 00:01:06.600
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.

00:01:06.600 --> 00:01:10.360
We've started streaming most of our episodes live on YouTube.

00:01:10.360 --> 00:01:16.200
Subscribe to our YouTube channel over at talkpython.fm/youtube to get notified about upcoming shows and be

00:01:16.200 --> 00:01:19.040
part of that episode.

00:01:19.040 --> 00:01:23.400
This episode is sponsored by Posit Connect from the makers of Shiny.

00:01:23.400 --> 00:01:26.280
Posit Connect lets you share and deploy all of your data projects that you're creating

00:01:26.280 --> 00:01:27.280
using Python.

00:01:27.280 --> 00:01:34.400
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quatro, Reports, Dashboards, and APIs.

00:01:34.400 --> 00:01:36.320
Posit Connect supports all of them.

00:01:36.320 --> 00:01:42.800
Try Posit Connect for free by going to talkpython.fm/posit, P-O-S-I-T.

00:01:42.800 --> 00:01:46.560
And it's also brought to you by us over at Talk Python Training.

00:01:46.560 --> 00:01:50.800
Did you know that we have over 250 hours of Python courses?

00:01:50.800 --> 00:01:52.080
Yeah, that's right.

00:01:52.080 --> 00:01:56.560
Check them out at talkpython.fm/courses.

00:01:56.560 --> 00:02:01.360
Last week, I told you about our new course, Build an AI Audio App with Python.

00:02:01.360 --> 00:02:06.160
Well, I have another brand new and amazing course to tell you about.

00:02:06.160 --> 00:02:10.640
This time, it's all about Python's typing system and how to take the most advantage

00:02:10.640 --> 00:02:11.880
of it.

00:02:11.880 --> 00:02:16.200
It's a really awesome course called Rock Solid Python with Python Typing.

00:02:16.200 --> 00:02:20.200
This is one of my favorite courses that I've created in the last couple of years.

00:02:20.200 --> 00:02:26.360
Python type hints are really starting to transform Python, especially from the ecosystem's perspective.

00:02:26.360 --> 00:02:30.080
Think FastAPI, Pytantic, BearType, et cetera.

00:02:30.080 --> 00:02:34.880
This course shows you the ins and outs of Python typing syntax, of course, but it also

00:02:34.880 --> 00:02:38.300
gives you guidance on when and how to use type hints.

00:02:38.300 --> 00:02:44.680
Check out this four and a half hour in-depth course at talkpython.fm/courses.

00:02:44.680 --> 00:02:47.720
Now onto those data pipelines.

00:02:47.720 --> 00:02:50.920
Pedram, welcome to Talk Python to Me.

00:02:50.920 --> 00:02:51.920
It's amazing to have you here.

00:02:51.920 --> 00:02:52.920
>> Michael, great to have you.

00:02:52.920 --> 00:02:53.920
Good to be here.

00:02:53.920 --> 00:02:54.920
>> Yeah.

00:02:54.920 --> 00:02:59.600
We're going to talk about data, data pipelines, automation, and boy, oh boy, let me tell you,

00:02:59.600 --> 00:03:03.480
have I been in the DevOps side of things this week.

00:03:03.480 --> 00:03:07.280
And I'm going to have a special, special appreciation of it.

00:03:07.280 --> 00:03:08.760
I can tell already.

00:03:08.760 --> 00:03:10.680
So excited to talk.

00:03:10.680 --> 00:03:12.120
>> My condolences.

00:03:12.120 --> 00:03:13.120
>> Indeed.

00:03:13.760 --> 00:03:18.480
>> So before we get to that, though, before we talk about Dagster and data pipelines and

00:03:18.480 --> 00:03:22.520
orchestration more broadly, let's just get a little bit of background on you.

00:03:22.520 --> 00:03:23.520
Introduce yourself for people.

00:03:23.520 --> 00:03:27.200
How'd you get into Python and data orchestration and all those things?

00:03:27.200 --> 00:03:28.200
>> Of course, yeah.

00:03:28.200 --> 00:03:29.520
So my name is Pedram Naveed.

00:03:29.520 --> 00:03:33.680
I'm the head of data engineering and dev rel at Dagster.

00:03:33.680 --> 00:03:34.840
That's a mouthful.

00:03:34.840 --> 00:03:38.680
And I've been a longtime Python user since 2.7.

00:03:38.680 --> 00:03:42.840
And I got started with Python like I do with many things just out of sheer laziness.

00:03:42.840 --> 00:03:47.880
I was working at a bank and there was this rote task, something involving going into

00:03:47.880 --> 00:03:52.560
servers, opening up a text file and seeing if a patch was applied to a server.

00:03:52.560 --> 00:03:57.600
A nightmare scenario when there's 100 servers to check and 15 different patches to confirm.

00:03:57.600 --> 00:04:02.000
>> Yeah, so this kind of predates like the cloud and all that automation and stuff, right?

00:04:02.000 --> 00:04:04.160
>> This was definitely before cloud.

00:04:04.160 --> 00:04:07.960
This was like right between Python 2 and Python 3, and we were trying to figure out how to

00:04:07.960 --> 00:04:09.160
use print statements correctly.

00:04:09.160 --> 00:04:10.160
That's when I learned Python.

00:04:10.160 --> 00:04:12.200
I was like, there's got to be a better way.

00:04:12.200 --> 00:04:13.760
And honestly, I've not looked back.

00:04:13.760 --> 00:04:18.960
I think if you look at my entire career trajectory, you'll see it's just punctuated by finding

00:04:18.960 --> 00:04:22.520
ways to be more lazy in many ways.

00:04:22.520 --> 00:04:23.520
>> Yeah.

00:04:23.520 --> 00:04:24.720
Who was it?

00:04:24.720 --> 00:04:29.240
I think it was Matthew Rocklin that had the phrase something like productive laziness

00:04:29.240 --> 00:04:31.480
or something like that.

00:04:31.480 --> 00:04:36.720
Like, I'm going to find a way to leverage my laziness to force me to build automation

00:04:36.720 --> 00:04:39.080
so I never ever have to do this sort of thing again.

00:04:39.080 --> 00:04:40.720
I got that sort of print.

00:04:40.720 --> 00:04:43.080
>> It's very motivating to not have to do something.

00:04:43.080 --> 00:04:44.760
And I'll do anything to not do something.

00:04:44.760 --> 00:04:45.760
>> Yeah, yeah, yeah.

00:04:45.760 --> 00:04:46.760
It's incredible.

00:04:46.760 --> 00:04:51.600
And like that DevOps stuff I was talking about, just, you know, one command and there's maybe

00:04:51.600 --> 00:04:55.800
eight or nine new apps with all their tiers redeployed, updated, resynced.

00:04:55.800 --> 00:04:59.080
And it took me a lot of work to get there.

00:04:59.080 --> 00:05:03.000
But now I never have to think about it again, at least not for a few years.

00:05:03.000 --> 00:05:04.000
And it's amazing.

00:05:04.000 --> 00:05:05.000
I can just be productive.

00:05:05.000 --> 00:05:07.240
It's like right in line with that.

00:05:07.240 --> 00:05:11.280
>> So what are some of the Python projects you've been, you've worked on, talked about

00:05:11.280 --> 00:05:13.360
different ways to apply this over the years?

00:05:13.360 --> 00:05:14.360
>> Oh, yeah.

00:05:14.360 --> 00:05:18.880
So it started with internal, just like Python projects, trying to automate, like I said,

00:05:18.880 --> 00:05:20.480
some rote tasks that I had.

00:05:20.480 --> 00:05:23.620
And that accidentally becomes, you know, a bigger project.

00:05:23.620 --> 00:05:25.720
People see it and they're like, oh, I want that too.

00:05:25.720 --> 00:05:29.480
And so, well, now I have to build like a GUI interface because most people don't speak

00:05:29.480 --> 00:05:30.640
Python.

00:05:30.640 --> 00:05:35.280
And so that got me into iGUI, I think it was called, way back when.

00:05:35.280 --> 00:05:36.540
That was a fun journey.

00:05:36.540 --> 00:05:38.120
And then from there, it's really taken off.

00:05:38.120 --> 00:05:41.120
A lot of it has been mostly personal projects.

00:05:41.120 --> 00:05:46.360
Trying to understand open source was a really big learning path for me as well.

00:05:46.360 --> 00:05:50.720
Really being absorbed by things like SQLAlchemy and requests back when they were coming out.

00:05:50.720 --> 00:05:55.880
Eventually, it led to more of a data engineering type of role, where I got involved with tools

00:05:55.880 --> 00:06:01.120
like Airflow and tried to automate data pipelines instead of patches on a server.

00:06:01.120 --> 00:06:06.700
That one day led to, I guess, making a long story short, a role at Dagster, where now I

00:06:06.700 --> 00:06:08.060
contribute a little bit to Dagster.

00:06:08.060 --> 00:06:11.860
I work on Dagster, the core project itself, but I also use Dagster internally to build

00:06:11.860 --> 00:06:13.860
our own data pipelines.

00:06:13.860 --> 00:06:19.700
I'm sure it's interesting to see how you all both build Dagster and then consume Dagster.

00:06:19.700 --> 00:06:21.820
Yeah, it's been wonderful.

00:06:21.820 --> 00:06:23.380
I think there's a lot of great things about it.

00:06:23.380 --> 00:06:27.980
One is like getting access to Dagster before it's fully released, right?

00:06:27.980 --> 00:06:33.100
Internally, we dog food, new features, new concepts, and we work with the product team,

00:06:33.100 --> 00:06:35.300
the engineering team, and say, "Hey, this makes sense.

00:06:35.300 --> 00:06:36.300
This doesn't.

00:06:36.300 --> 00:06:37.300
This works really well.

00:06:37.300 --> 00:06:38.300
That doesn't."

00:06:38.300 --> 00:06:43.140
That feedback loop is so fast and so iterative that for me personally, being able to see

00:06:43.140 --> 00:06:45.860
that come to fruition is really, really compelling.

00:06:45.860 --> 00:06:50.020
But at the same time, I get to work at a place that's building a tool for me.

00:06:50.020 --> 00:06:51.580
You don't often get that luxury.

00:06:51.580 --> 00:06:53.140
I've worked in ads.

00:06:53.140 --> 00:06:54.980
I've worked in insurance.

00:06:54.980 --> 00:06:55.980
It's like banking.

00:06:55.980 --> 00:06:59.340
These are nice things, but it's not built for me, right?

00:06:59.340 --> 00:07:01.980
And so for me, that's probably been the biggest benefit, I would say.

00:07:01.980 --> 00:07:02.980
Right.

00:07:02.980 --> 00:07:06.620
If you work in some marketing thing, you're like, "You know, I retargeted myself so well

00:07:06.620 --> 00:07:07.620
today.

00:07:07.620 --> 00:07:08.620
You wouldn't believe it.

00:07:08.620 --> 00:07:09.620
I really enjoyed it."

00:07:09.620 --> 00:07:15.100
I've seen the ads that I've created before, so it's a little fun, but it's not the same.

00:07:15.100 --> 00:07:16.100
Yeah.

00:07:16.100 --> 00:07:21.420
I've heard of people who are really, really good at ad targeting and finding groups where

00:07:21.420 --> 00:07:26.100
they've pranked their wife or something, or just had an ad that would only show up for

00:07:26.100 --> 00:07:27.100
their wife by running it.

00:07:27.100 --> 00:07:30.300
It was so specific and freaked them out a little bit.

00:07:30.300 --> 00:07:31.300
That's pretty clever.

00:07:31.300 --> 00:07:32.300
Yeah.

00:07:32.300 --> 00:07:34.540
Maybe it wasn't appreciated, but it is clever.

00:07:34.540 --> 00:07:35.940
Who knows?

00:07:35.940 --> 00:07:37.420
All right.

00:07:37.420 --> 00:07:43.220
Well, before we jump in, you said that, of course, you built GUIs with PyGUI and those

00:07:43.220 --> 00:07:48.220
sorts of things because people don't speak Python back then, two, seven days and whatever.

00:07:48.220 --> 00:07:49.500
Is that different now?

00:07:49.500 --> 00:07:53.060
Not that people speak Python, but is it different in the sense that, "Hey, I could give them

00:07:53.060 --> 00:07:58.580
a Jupyter notebook," or, "I could give them Streamlit," or one of these things, right?

00:07:58.580 --> 00:08:01.020
A little more or less you building and just plug it in?

00:08:01.020 --> 00:08:02.020
I think so.

00:08:02.020 --> 00:08:06.380
Like you said, it's not different in that most people probably still to this day don't

00:08:06.380 --> 00:08:07.380
speak Python.

00:08:07.380 --> 00:08:11.940
I know we had this movement a little bit back where everyone was going to learn SQL and

00:08:11.940 --> 00:08:13.380
everyone was going to learn to code.

00:08:13.380 --> 00:08:18.980
I was never that bullish on that trend because if I'm a marketing person, I've got 10,000

00:08:18.980 --> 00:08:22.860
things to do and learning to code isn't going to be a priority ever.

00:08:22.860 --> 00:08:27.260
So I think building interfaces for people that are easy to use and speak well to them

00:08:27.260 --> 00:08:29.020
is always useful.

00:08:29.020 --> 00:08:32.340
That never has gone away, but I think the tooling around it has been better, right?

00:08:32.340 --> 00:08:34.140
I don't think I'll ever want to use PyGUI again.

00:08:34.140 --> 00:08:35.460
Nothing wrong with the platform.

00:08:35.460 --> 00:08:37.580
It's just not fun to write.

00:08:37.580 --> 00:08:39.860
Streamlit makes it so easy to do that.

00:08:39.860 --> 00:08:43.620
So it's something like retool and there's a thousand other ways now that you can bring

00:08:43.620 --> 00:08:47.740
these tools in front of your stakeholders and your users that just wasn't possible before.

00:08:47.740 --> 00:08:49.260
I think it's a pretty exciting time.

00:08:49.260 --> 00:08:51.380
There are a lot of pretty polished tools.

00:08:51.380 --> 00:08:52.900
Yeah, it's gotten so good.

00:08:52.900 --> 00:08:53.900
Yeah.

00:08:53.900 --> 00:08:54.900
There's some interesting ones like OpenBB.

00:08:54.900 --> 00:08:55.900
Do you know that?

00:08:55.900 --> 00:08:58.220
The financial dashboard thing.

00:08:58.220 --> 00:08:59.220
I've heard of this.

00:08:59.220 --> 00:09:00.220
I haven't seen it.

00:09:00.220 --> 00:09:01.220
Yeah.

00:09:01.220 --> 00:09:05.340
It's basically for traders, but it's like a terminal type thing that has a bunch of

00:09:05.340 --> 00:09:10.660
Matplotlib and other interactive stuff that pops up kind of compared to say Bloomberg

00:09:10.660 --> 00:09:13.220
dashboard type things.

00:09:13.220 --> 00:09:18.180
But yeah, that's one sense where like maybe like traders go and learn Python because it's

00:09:18.180 --> 00:09:19.980
like, all right, there's enough value here.

00:09:19.980 --> 00:09:24.100
But in general, I don't think people are going to stop what they're doing and learning the

00:09:24.100 --> 00:09:25.100
code.

00:09:25.100 --> 00:09:26.100
So these new UI things are not.

00:09:26.100 --> 00:09:32.060
All right, let's dive in and talk about this general category first of data pipelines,

00:09:32.060 --> 00:09:33.540
data orchestration, all those things.

00:09:33.540 --> 00:09:36.540
We'll talk about Dagster and some of the trends and that.

00:09:36.540 --> 00:09:41.780
So let's grab some random internet search for what does a data pipeline maybe look like?

00:09:41.780 --> 00:09:47.000
But people out there listening who don't necessarily live in that space, which I think is honestly

00:09:47.000 --> 00:09:51.700
many of us, maybe we should, but maybe in our minds, we don't think we live in data

00:09:51.700 --> 00:09:52.700
pipeline land.

00:09:52.700 --> 00:09:53.700
Tell them about it.

00:09:53.700 --> 00:09:54.700
Yeah, for sure.

00:09:54.700 --> 00:09:57.560
It is hard to think about if you haven't done or built one before.

00:09:57.560 --> 00:10:02.900
In many ways, a data pipeline is just a series of steps that you apply to some dataset that

00:10:02.900 --> 00:10:08.540
you have in order to transform it to something a little bit more valuable at the very end.

00:10:08.540 --> 00:10:12.340
It's a simplified version, the devil's in the details, but really like at the end of

00:10:12.340 --> 00:10:16.300
the day, you're in a business, the production of data sort of happens by the very nature

00:10:16.300 --> 00:10:17.300
of operating that business.

00:10:17.300 --> 00:10:21.260
It tends to be the core thing that all businesses have in common.

00:10:21.260 --> 00:10:25.180
And then the other sort of output is you have people within that business who are trying

00:10:25.180 --> 00:10:27.780
to understand how the business is operating.

00:10:27.780 --> 00:10:31.140
And this used to be easy when all we had was a single spreadsheet that we could look at

00:10:31.140 --> 00:10:32.140
once a month.

00:10:32.140 --> 00:10:36.060
Yeah, I think businesses have gone a little bit more complex than these days.

00:10:36.060 --> 00:10:37.060
Computers and automation.

00:10:37.060 --> 00:10:38.060
And the expectations.

00:10:38.060 --> 00:10:42.140
I expect to be able to see almost real time, not I'll see it at the end of the month sort

00:10:42.140 --> 00:10:43.140
of.

00:10:43.140 --> 00:10:44.140
That's right.

00:10:44.140 --> 00:10:45.140
Yeah.

00:10:45.140 --> 00:10:47.620
I think people have gotten used to getting data too, which is both good and bad.

00:10:47.620 --> 00:10:49.620
Good in the sense that now people are making better decisions.

00:10:49.620 --> 00:10:51.700
Bad, and then there's more work for us to do.

00:10:51.700 --> 00:10:55.620
And we can't just sit on our feet for half a day, half a month waiting for the next request

00:10:55.620 --> 00:10:56.620
to come in.

00:10:56.620 --> 00:10:59.020
There's just an endless stream that seems to never end.

00:10:59.020 --> 00:11:00.940
So that's what really a pipeline is all about.

00:11:00.940 --> 00:11:06.420
It's like taking these data and making it consumable in a way that users, tools will

00:11:06.420 --> 00:11:09.500
understand that helps people make decisions at the very end of the day.

00:11:09.500 --> 00:11:11.060
That's sort of the nuts and bolts of it.

00:11:11.060 --> 00:11:14.660
In your mind, does data acquisition live in this land?

00:11:14.660 --> 00:11:19.820
So for example, maybe we have a scheduled job that goes and does web scraping, calls

00:11:19.820 --> 00:11:24.820
an API once an hour, and that might kick off a whole pipeline of processing.

00:11:24.820 --> 00:11:32.180
Or we watch a folder for people to upload over FTP, like a CSV file or something horrible

00:11:32.180 --> 00:11:33.180
like that.

00:11:33.180 --> 00:11:34.620
You don't even, it's unspeakable.

00:11:34.620 --> 00:11:38.500
But something like that where you say, oh, a new CSV has arrived for me to get, right?

00:11:38.500 --> 00:11:43.420
Yeah, I think that's the beginning of all data pipeline journeys in my mind, very much,

00:11:43.420 --> 00:11:44.420
right?

00:11:44.420 --> 00:11:46.620
Like an FTP, as much as we hate it, it's not terrible.

00:11:46.620 --> 00:11:52.100
I mean, the worst, there are worse ways to transfer files, but it's, I think still very

00:11:52.100 --> 00:11:53.100
much in use today.

00:11:53.100 --> 00:11:57.740
And every data pipeline journey at some point has to begin with that consumption of data

00:11:57.740 --> 00:11:58.740
from somewhere.

00:11:58.740 --> 00:11:59.740
Yeah.

00:11:59.740 --> 00:12:02.020
Hopefully it's SFTP, not just straight FTP.

00:12:02.020 --> 00:12:06.260
Like the encrypted, don't just send your password in the plain text.

00:12:06.260 --> 00:12:09.420
Oh, well, I've seen that go wrong.

00:12:09.420 --> 00:12:11.460
That's a story for another day, honestly.

00:12:11.460 --> 00:12:12.460
All right.

00:12:12.460 --> 00:12:14.820
Well, let's talk about the project that you work on.

00:12:14.820 --> 00:12:18.020
We've been talking about it in general, but let's talk about Dagster.

00:12:18.020 --> 00:12:20.020
Like, where does it fit in this world?

00:12:20.020 --> 00:12:21.020
Yes.

00:12:21.020 --> 00:12:24.660
Dagster to me is a way to build a data platform.

00:12:24.660 --> 00:12:28.680
It's also a different way of thinking about how you build data pipelines.

00:12:28.680 --> 00:12:32.420
Maybe it's good to compare it with kind of what the world was like, I think, before Dagster

00:12:32.420 --> 00:12:34.940
and how it came about to be.

00:12:34.940 --> 00:12:39.900
So if you think of Airflow, I think it's probably the most canonical orchestrator out there,

00:12:39.900 --> 00:12:43.980
but there are other ways which people used to orchestrate these data pipelines.

00:12:43.980 --> 00:12:45.660
They were often task-based, right?

00:12:45.660 --> 00:12:50.500
Like I would download file, I would unzip file, I would upload file.

00:12:50.500 --> 00:12:55.900
These are sort of the words we use to describe the various steps within a pipeline.

00:12:55.900 --> 00:12:59.120
Some of those little steps might be Python functions that you write.

00:12:59.120 --> 00:13:00.780
Maybe there's some pre-built other ones.

00:13:00.780 --> 00:13:02.420
Yeah, they might be Python.

00:13:02.420 --> 00:13:03.420
Could be a bash script.

00:13:03.420 --> 00:13:05.940
It'd be logging into a server and downloading a file.

00:13:05.940 --> 00:13:09.420
Could be hitting request to download something from the internet, unzipping it.

00:13:09.420 --> 00:13:12.140
Just a various, you know, hodgepodge of commands that would run.

00:13:12.140 --> 00:13:13.900
That's typically how we thought about it.

00:13:13.900 --> 00:13:17.420
For more complex scenarios where your data is bigger, maybe it's running against a Hadoop

00:13:17.420 --> 00:13:19.260
cluster or a Spark cluster.

00:13:19.260 --> 00:13:21.260
The compute's been offloaded somewhere else.

00:13:21.260 --> 00:13:25.300
But the sort of conceptual way you tended to think about these things is in terms of

00:13:25.300 --> 00:13:26.300
tasks, right?

00:13:26.300 --> 00:13:31.420
Process this thing, do this massive data dump, run a bunch of things, and then your job is

00:13:31.420 --> 00:13:32.420
complete.

00:13:32.420 --> 00:13:35.860
With Airflow, or sorry, with Dijkstra, we kind of flip it around a little bit on our

00:13:35.860 --> 00:13:40.340
heads and we say, instead of thinking about tasks, what if we flipped that around and

00:13:40.340 --> 00:13:43.780
thought about the actual underlying assets that you're creating?

00:13:43.780 --> 00:13:47.660
What if you told us not, you know, the step that you're going to take, but the thing that

00:13:47.660 --> 00:13:48.660
you produce?

00:13:48.660 --> 00:13:52.940
Because it turns out as people and as data people and stakeholders, really, we don't

00:13:52.940 --> 00:13:56.220
care about the task, like we just assume that you're going to do it.

00:13:56.220 --> 00:14:01.900
What we care about is, you know, that table, that model, that file, that Jupyter notebook.

00:14:01.900 --> 00:14:06.300
And if we model our pipeline through that, then we get a whole bunch of other benefits.

00:14:06.300 --> 00:14:09.100
And that's sort of the Dijkstra sort of pitch, right?

00:14:09.100 --> 00:14:13.660
Like if you want to understand the things that are being produced by these tasks, tell

00:14:13.660 --> 00:14:15.620
us about the underlying assets.

00:14:15.620 --> 00:14:19.100
And then when a stakeholder says and comes to you and says, you know, how old is this

00:14:19.100 --> 00:14:20.100
table?

00:14:20.100 --> 00:14:21.100
Has it been refreshed lately?

00:14:21.100 --> 00:14:22.580
You don't have to go look at a specific task.

00:14:22.580 --> 00:14:26.220
And remember that task ABC had model XYZ.

00:14:26.220 --> 00:14:29.220
You just go and look up model XYZ directly there, and it's there for you.

00:14:29.220 --> 00:14:33.060
And because you've defined things in this way, you get other nice things like a lineage

00:14:33.060 --> 00:14:34.060
graph.

00:14:34.060 --> 00:14:36.300
You get to understand how fresh your data is.

00:14:36.300 --> 00:14:39.620
You can do event-based orchestration and all kinds of nice things that are a lot harder

00:14:39.620 --> 00:14:41.220
to do in a task world.

00:14:41.220 --> 00:14:45.060
Yeah, more declarative, less imperative, I suppose.

00:14:45.060 --> 00:14:48.540
Yeah, it's been the trend, I think, in lots of tooling.

00:14:48.540 --> 00:14:51.020
React, I think was famous for this as well, right?

00:14:51.020 --> 00:14:52.020
In many ways.

00:14:52.020 --> 00:14:55.940
It was a hard framework, I think, for people to sort of get their heads around initially

00:14:55.940 --> 00:15:01.060
because we were so used to like the jQuery declarative or jQuery style of doing things.

00:15:01.060 --> 00:15:02.060
Yeah.

00:15:02.060 --> 00:15:03.460
How do I hook the event that makes the thing happen?

00:15:03.460 --> 00:15:06.140
And React said, let's think about it a little bit differently.

00:15:06.140 --> 00:15:08.660
Let's do this event-based orchestration.

00:15:08.660 --> 00:15:10.460
And I think the proof's in the pudding.

00:15:10.460 --> 00:15:13.140
React's everywhere now and jQuery, maybe not so much.

00:15:13.140 --> 00:15:14.140
Yeah.

00:15:14.140 --> 00:15:18.340
There's still a lot of jQuery out there, but there's not a lot of active jQuery.

00:15:18.340 --> 00:15:19.900
But I imagine there's some.

00:15:19.900 --> 00:15:20.900
There is.

00:15:20.900 --> 00:15:21.900
Yeah.

00:15:21.900 --> 00:15:22.900
Just because people are like, you know what?

00:15:22.900 --> 00:15:23.900
Don't touch that.

00:15:23.900 --> 00:15:24.900
That works.

00:15:24.900 --> 00:15:27.260
Which is probably the smartest thing people can do, I think.

00:15:27.260 --> 00:15:28.260
Yeah, honestly.

00:15:28.260 --> 00:15:30.100
Even though new frameworks are shiny.

00:15:30.100 --> 00:15:35.780
And if there's any ecosystem that loves to chase the shiny new idea, it's the JavaScript

00:15:35.780 --> 00:15:36.780
web world.

00:15:36.780 --> 00:15:37.780
Oh, yeah.

00:15:37.780 --> 00:15:40.260
There's no shortage of new frameworks coming out every time.

00:15:40.260 --> 00:15:44.220
We do too, but not as much as like, that's six months old.

00:15:44.220 --> 00:15:46.220
That's so old, we can't possibly do that anymore.

00:15:46.220 --> 00:15:47.220
We're rewriting it.

00:15:47.220 --> 00:15:48.580
We're going to do the big rewrite again.

00:15:48.580 --> 00:15:49.580
Yep.

00:15:49.580 --> 00:15:50.580
Fun.

00:15:50.580 --> 00:15:54.060
So, Dagster is the company, but also is open source.

00:15:54.060 --> 00:15:57.300
What's the story around like, can I use it for free?

00:15:57.300 --> 00:15:58.300
Is it open source?

00:15:58.300 --> 00:15:59.300
Do I pay for it?

00:15:59.300 --> 00:16:00.300
100%.

00:16:00.300 --> 00:16:01.300
Okay.

00:16:01.300 --> 00:16:02.300
So, Dagster Labs is the company.

00:16:02.300 --> 00:16:03.300
Dagster open source is the product.

00:16:03.300 --> 00:16:04.300
It's 100% free.

00:16:04.300 --> 00:16:06.700
We're very committed to the open source model.

00:16:06.700 --> 00:16:11.540
I would say 95% of the things you can get out of Dagster are available through open source.

00:16:11.540 --> 00:16:14.460
And we tend to try to release everything through that model.

00:16:14.460 --> 00:16:19.680
You can run very complex pipelines, and you can deploy it all on your own if you wish.

00:16:19.680 --> 00:16:23.400
There is a Dagster cloud product, which is really the hosted version of Dagster.

00:16:23.400 --> 00:16:27.320
If you want hosted plain, we can do that for you through Dagster cloud, but it all runs

00:16:27.320 --> 00:16:32.140
on the same code base and the modeling and the files all essentially look the same.

00:16:32.140 --> 00:16:33.140
Okay.

00:16:33.140 --> 00:16:36.060
So obviously you could get, like I talked about at the beginning, you could go down

00:16:36.060 --> 00:16:41.500
the DevOps side, get your own open source Dagster set up, schedule it, run it on servers,

00:16:41.500 --> 00:16:42.500
all those things.

00:16:42.500 --> 00:16:47.020
But if we just wanted something real simple, we could just go to you guys and say, "Hey,

00:16:47.020 --> 00:16:48.020
I built this with Dagster.

00:16:48.020 --> 00:16:49.300
Will you run it for me?"

00:16:49.300 --> 00:16:50.300
Pretty much.

00:16:50.300 --> 00:16:51.300
Yeah.

00:16:51.300 --> 00:16:52.300
Right.

00:16:52.300 --> 00:16:53.300
So there's two options there.

00:16:53.300 --> 00:16:55.340
You can do the serverless model, which says, "Dagster, just run it.

00:16:55.340 --> 00:16:58.520
We take care of the compute, we take care of the execution for you, and you just write

00:16:58.520 --> 00:17:04.040
the code and upload it to GitHub or any repository of your choice, and we'll sync to that and

00:17:04.040 --> 00:17:05.040
then run it."

00:17:05.040 --> 00:17:06.400
The other option is to do the hybrid model.

00:17:06.400 --> 00:17:09.140
So you basically do the CI/CD aspect.

00:17:09.140 --> 00:17:11.980
You just say, you push to name your branch.

00:17:11.980 --> 00:17:15.960
If you push to that branch, that means we're just going to deploy a new version and whatever

00:17:15.960 --> 00:17:18.540
happens after that, it'll be in production, right?

00:17:18.540 --> 00:17:19.540
Exactly.

00:17:19.540 --> 00:17:20.540
Yeah.

00:17:20.540 --> 00:17:23.660
And we offer some templates that you can use in GitHub for workflows in order to accommodate

00:17:23.660 --> 00:17:24.660
that.

00:17:24.660 --> 00:17:25.660
Excellent.

00:17:25.660 --> 00:17:26.660
Then I cut you off.

00:17:26.660 --> 00:17:27.660
You're saying something about hybrid.

00:17:27.660 --> 00:17:30.620
Hybrid is the other option for those of you who want to run your own compute.

00:17:30.620 --> 00:17:32.740
You don't want the data leaving your ecosystem.

00:17:32.740 --> 00:17:37.300
You can say, "We've got this Kubernetes cluster, this ECS cluster, but we still want to use

00:17:37.300 --> 00:17:40.300
a Dagster Cloud product to sort of manage the control plane.

00:17:40.300 --> 00:17:41.460
Dagster Cloud will do that."

00:17:41.460 --> 00:17:44.900
And then you can go off and execute things on your own environment if that's something

00:17:44.900 --> 00:17:45.900
you wish to do.

00:17:45.900 --> 00:17:46.900
Oh, yeah.

00:17:46.900 --> 00:17:51.740
Because running stuff in containers isn't too bad, but running container clusters, all

00:17:51.740 --> 00:17:55.060
of a sudden you're back doing a lot of work, right?

00:17:55.060 --> 00:17:56.060
Exactly.

00:17:56.060 --> 00:17:57.060
Yeah.

00:17:57.060 --> 00:17:58.060
Okay.

00:17:58.060 --> 00:17:59.060
Well, let's maybe talk about Dagster for a bit.

00:17:59.060 --> 00:18:02.740
I want to talk about some of the trends as well, but let's just talk through maybe setting

00:18:02.740 --> 00:18:04.380
up a pipeline.

00:18:04.380 --> 00:18:05.380
What does it look like?

00:18:05.380 --> 00:18:10.180
You talked about in general, less imperative, more declarative, but what does it look like?

00:18:10.180 --> 00:18:15.260
Be careful about talking about code on audio, but just give us a sense of what the programming

00:18:15.260 --> 00:18:16.780
model feels like for us.

00:18:16.780 --> 00:18:20.100
As much as possible, it really feels like just writing Python.

00:18:20.100 --> 00:18:21.340
It's pretty easy.

00:18:21.340 --> 00:18:25.980
You add a decorator on top of your existing Python function that does something.

00:18:25.980 --> 00:18:28.260
That's a simple decorator called asset.

00:18:28.260 --> 00:18:31.580
And then your pipeline, that function becomes a data asset.

00:18:31.580 --> 00:18:33.900
That's how it's represented in the Dagster UI.

00:18:33.900 --> 00:18:39.180
So you could imagine you've got a pipeline that gets like maybe Slack analytics and uploads

00:18:39.180 --> 00:18:41.580
that to some dashboard, right?

00:18:41.580 --> 00:18:45.300
Your first pipeline, your function will be called something like Slack data, and that

00:18:45.300 --> 00:18:46.460
would be your asset.

00:18:46.460 --> 00:18:51.060
In that function is where you do all the transform, the downloading of the data until you've really

00:18:51.060 --> 00:18:53.660
created that fundamental data asset that you care about.

00:18:53.660 --> 00:18:58.620
And that could be stored either in a data warehouse to S3, however you sort of want

00:18:58.620 --> 00:19:00.260
to persist it, that's really up to you.

00:19:00.260 --> 00:19:04.740
And then the resources is sort of where the power, I think, of a lot of Dagster comes in.

00:19:04.740 --> 00:19:08.140
So the asset is sort of like declaration of the thing I'm going to create.

00:19:08.140 --> 00:19:12.100
The resource is how I'm going to operate on that, right?

00:19:12.100 --> 00:19:17.020
Because sometimes you might want to have a, let's say a DuckDB instance locally, because

00:19:17.020 --> 00:19:18.900
it's easier and faster to operate.

00:19:18.900 --> 00:19:23.100
But when you're moving to the cloud, you want to have a Databricks or a Snowflake.

00:19:23.100 --> 00:19:28.060
You can swap out resources based on environments and your asset can reference that resource.

00:19:28.060 --> 00:19:32.740
And as long as it has that same sort of API, you can really flexibly change between where

00:19:32.740 --> 00:19:34.220
that data is going to be persistent.

00:19:34.220 --> 00:19:37.020
Does Dagster know how to talk to those different platforms?

00:19:37.020 --> 00:19:40.180
Does it like natively understand DuckDB and Snowflake?

00:19:40.180 --> 00:19:41.180
Yeah.

00:19:41.180 --> 00:19:42.180
Interesting.

00:19:42.180 --> 00:19:44.580
People often look to Dagster and like, "Oh, does it do X?"

00:19:44.580 --> 00:19:48.900
And the question is like, "Dagster does anything you can do Python with?"

00:19:48.900 --> 00:19:49.900
Which is most things, yeah.

00:19:49.900 --> 00:19:50.900
Which is most things.

00:19:50.900 --> 00:19:54.020
So I think if you come from the Airflow world, you're very much used to like these Airflow

00:19:54.020 --> 00:19:55.460
providers and if you want to use...

00:19:55.460 --> 00:19:56.820
That's kind of what I was thinking, yeah.

00:19:56.820 --> 00:19:57.820
Yeah.

00:19:57.820 --> 00:19:59.740
You want to use a Postgres, you need to find the Postgres provider.

00:19:59.740 --> 00:20:01.980
You want to use S3, you need to find the S3 provider.

00:20:01.980 --> 00:20:04.740
With Dagster, you kind of say you don't have to do any of that.

00:20:04.740 --> 00:20:08.580
If you want to use Snowflake, for example, install the Snowflake connector package from

00:20:08.580 --> 00:20:11.300
Snowflake and you use that as a resource directly.

00:20:11.300 --> 00:20:13.580
And then you just run your SQL that way.

00:20:13.580 --> 00:20:18.180
There are some places where we do have integrations that help if you want to get into the weeds

00:20:18.180 --> 00:20:21.600
with I/O Manager, it's where we persist the data on your behalf.

00:20:21.600 --> 00:20:26.100
And so for S3, for Snowflake, for example, there's other ways where we can persist that

00:20:26.100 --> 00:20:27.300
data for you.

00:20:27.300 --> 00:20:30.700
But if you're just trying to run a query, just trying to execute something, just trying

00:20:30.700 --> 00:20:33.640
to save something somewhere, you don't have to use that system at all.

00:20:33.640 --> 00:20:38.180
You can just use whatever Python package you would use anyway to do that.

00:20:38.180 --> 00:20:43.420
So maybe some data is expensive for us to get as a company, like maybe we're charged

00:20:43.420 --> 00:20:46.540
on a usage basis or super slow or something.

00:20:46.540 --> 00:20:50.920
I could write just Python code that goes and say, well, look in my local database.

00:20:50.920 --> 00:20:53.260
If it's already there, use that and it's not too stale.

00:20:53.260 --> 00:20:57.380
Otherwise, then do actually go get it, put it there and then get it back.

00:20:57.380 --> 00:21:00.700
And like that kind of stuff would be up to me to put together.

00:21:00.700 --> 00:21:01.700
Yeah.

00:21:01.700 --> 00:21:06.340
And that's the nice thing is you're not really limited by like anyone's data model or worldview

00:21:06.340 --> 00:21:09.700
on how data should be retrieved or saved or augmented.

00:21:09.700 --> 00:21:10.700
You could do it a couple of ways.

00:21:10.700 --> 00:21:15.220
You could say whenever I'm working locally, use this persistent data store that we're

00:21:15.220 --> 00:21:18.020
just going to use for development purposes.

00:21:18.020 --> 00:21:20.220
Fancy database called SQLite, something like that.

00:21:20.220 --> 00:21:21.220
Exactly.

00:21:21.220 --> 00:21:22.220
Yes.

00:21:22.220 --> 00:21:23.220
A wonderful database.

00:21:23.220 --> 00:21:24.220
Actually, it is.

00:21:24.220 --> 00:21:25.220
Yeah.

00:21:25.220 --> 00:21:26.220
It'll work really, really well.

00:21:26.220 --> 00:21:28.220
And then you just say when I'm in a different environment, when I'm in production, swap

00:21:28.220 --> 00:21:33.780
out my SQLite resource for a name, your favorite cloud warehouse resource, and go fetch that

00:21:33.780 --> 00:21:34.780
data from there.

00:21:34.780 --> 00:21:36.820
Or I want to use it mini IO locally.

00:21:36.820 --> 00:21:39.260
I want to use S3 on prod.

00:21:39.260 --> 00:21:40.780
It's very simple to swap these things out.

00:21:40.780 --> 00:21:41.780
Okay.

00:21:41.780 --> 00:21:42.780
Yeah.

00:21:42.780 --> 00:21:46.580
So it looks like you build up these assets as y'all call them, these pieces of data,

00:21:46.580 --> 00:21:48.440
Python code that accesses them.

00:21:48.440 --> 00:21:54.180
And then you have a nice UI that lets you go and build those out kind of workflow style,

00:21:54.180 --> 00:21:55.180
right?

00:21:55.180 --> 00:21:56.180
Yeah, exactly.

00:21:56.180 --> 00:22:00.260
This is where we get into the wonderful world of DAGs, which stands for directed acyclic

00:22:00.260 --> 00:22:01.260
graph.

00:22:01.260 --> 00:22:04.820
So basically it stands for a bunch of things that are not connected in a circle, but are

00:22:04.820 --> 00:22:05.820
connected in some way.

00:22:05.820 --> 00:22:07.180
So there can't be any loops, right?

00:22:07.180 --> 00:22:09.220
Because then you never know where to start or where to end.

00:22:09.220 --> 00:22:11.220
Could be an assignment, but not a circle.

00:22:11.220 --> 00:22:12.220
Not a circle.

00:22:12.220 --> 00:22:18.060
As long as there's like a path through this dataset, where the beginning and an end, then

00:22:18.060 --> 00:22:21.700
we can kind of start to model this connected graph of things.

00:22:21.700 --> 00:22:23.220
And then we know how to execute them, right?

00:22:23.220 --> 00:22:26.980
We can say, well, this is the first thing we have to run because that's where all dependencies

00:22:26.980 --> 00:22:27.980
start.

00:22:27.980 --> 00:22:31.260
And then we can branch off in parallel or we can continue linearly until everything

00:22:31.260 --> 00:22:32.260
is complete.

00:22:32.260 --> 00:22:35.380
And if something breaks in the middle, we can resume from that broken spot.

00:22:35.380 --> 00:22:36.580
Okay, excellent.

00:22:36.580 --> 00:22:38.340
And is that the recommended way?

00:22:38.340 --> 00:22:42.940
Like if I write all this Python code that works on the pieces, then the next recommendation

00:22:42.940 --> 00:22:45.580
would be to fire up the UI and start building it?

00:22:45.580 --> 00:22:49.060
Or do you say, ah, you should really write it in code and then you can just visualize

00:22:49.060 --> 00:22:50.580
it or monitor it?

00:22:50.580 --> 00:22:52.100
Everything in Dagster is written as code.

00:22:52.100 --> 00:22:57.420
The UI reads that code and it interprets it as a DAG and then it displays that for you.

00:22:57.420 --> 00:23:00.700
There are some things you do with the UI, like you can materialize assets, you can make

00:23:00.700 --> 00:23:06.340
them run, you can do backfills, you can view metadata, you can sort of enable and disable

00:23:06.340 --> 00:23:07.340
schedules.

00:23:07.340 --> 00:23:11.340
But the core, we really believe this is Dagster, like the core declaration of how things are

00:23:11.340 --> 00:23:13.460
done, it's always done through code.

00:23:13.460 --> 00:23:14.660
Okay, excellent.

00:23:14.660 --> 00:23:19.380
So when you say materialize, maybe I have an asset, which is really a Python function

00:23:19.380 --> 00:23:22.820
I wrote that goes and pulls down a CSV file.

00:23:22.820 --> 00:23:28.260
The materialize would be, I want to see kind of representative data in this, in the UI.

00:23:28.260 --> 00:23:30.540
And so I could go, all right, I think this is right.

00:23:30.540 --> 00:23:31.540
Let's keep passing it down.

00:23:31.540 --> 00:23:33.060
Is that what that means?

00:23:33.060 --> 00:23:37.860
Materialize really means just run this particular asset, make this asset new again, fresh again,

00:23:37.860 --> 00:23:38.860
right?

00:23:38.860 --> 00:23:41.020
As part of that materialization, we sometimes output metadata.

00:23:41.020 --> 00:23:44.620
And you can kind of see this on the right, if you're looking at the screen here, where

00:23:44.620 --> 00:23:49.940
we talk about what the timestamp was, the URL, there's a nice little graph of like number

00:23:49.940 --> 00:23:51.700
of rows over time.

00:23:51.700 --> 00:23:56.500
All that metadata is something you can emit, and we emit some ourselves by default with

00:23:56.500 --> 00:23:57.500
the framework.

00:23:57.500 --> 00:24:00.780
And then as you materialize these assets, as you run that asset over and over again,

00:24:00.780 --> 00:24:01.780
over time, we capture all that.

00:24:01.780 --> 00:24:06.540
And then you can really get a nice overview of, you know, this assets lifetime, essentially.

00:24:06.540 --> 00:24:07.540
Nice.

00:24:07.540 --> 00:24:10.820
I think the asset, the metadata is really pretty excellent, right?

00:24:10.820 --> 00:24:13.860
Over time, you can see how the data's grown and changed.

00:24:13.860 --> 00:24:16.780
And yeah, the metadata is really powerful.

00:24:16.780 --> 00:24:19.700
And it's one of the nice benefits of being in this asset world, right?

00:24:19.700 --> 00:24:23.060
Because you don't really want to metadata on like this task that run, you want to know

00:24:23.060 --> 00:24:27.660
like this table that I created, how many rows has it had every single time it's run?

00:24:27.660 --> 00:24:31.140
If that number drops by like 50%, that's a big problem.

00:24:31.140 --> 00:24:35.560
Conversely, if the runtime is slowly increasing every single day, you might not notice it,

00:24:35.560 --> 00:24:40.020
but over a month or two, it went from a 30 second pipeline to 30 minutes.

00:24:40.020 --> 00:24:43.660
Maybe there's like a great place to start optimizing that one specific asset.

00:24:43.660 --> 00:24:44.660
Right.

00:24:44.660 --> 00:24:48.580
And what's cool, if it's just Python code, you know how to optimize that probably, right?

00:24:48.580 --> 00:24:49.580
Hopefully, yeah.

00:24:49.580 --> 00:24:54.460
Well, as much as you're going to, yeah, you got, you have all the power of Python and

00:24:54.460 --> 00:24:57.700
you should be able to, as opposed to it's deep down inside some framework that you don't

00:24:57.700 --> 00:24:58.700
really control.

00:24:58.700 --> 00:24:59.700
Exactly.

00:24:59.700 --> 00:25:00.700
Yeah.

00:25:00.700 --> 00:25:01.700
You use Python, you can benchmark it.

00:25:01.700 --> 00:25:04.460
There's probably, you probably knew you didn't write it that well when you first started

00:25:04.460 --> 00:25:07.700
and you can always find ways to improve it.

00:25:07.700 --> 00:25:11.060
So this UI is something that you can just run locally, kind of like Jupyter.

00:25:11.060 --> 00:25:12.060
100%.

00:25:12.060 --> 00:25:16.180
Just type Dijkstra dev and then you get the full UI experience.

00:25:16.180 --> 00:25:17.940
You get to see the runs, all your assets.

00:25:17.940 --> 00:25:18.940
Is it a web app?

00:25:18.940 --> 00:25:19.940
It is.

00:25:19.940 --> 00:25:20.940
Yeah.

00:25:20.940 --> 00:25:21.940
It's a web app.

00:25:21.940 --> 00:25:22.940
There's a Postgres backend.

00:25:22.940 --> 00:25:25.820
And then there's a couple of services that run the web server, the GraphQL, and then

00:25:25.820 --> 00:25:26.820
the workers.

00:25:26.820 --> 00:25:27.820
Nice.

00:25:27.820 --> 00:25:28.820
Yeah.

00:25:28.820 --> 00:25:31.820
So pretty serious web app, it sounds like, but you probably just run it all.

00:25:31.820 --> 00:25:32.820
Yeah.

00:25:33.820 --> 00:25:37.980
Just something you run all probably containers or something you just fire up when you download

00:25:37.980 --> 00:25:38.980
it, right?

00:25:38.980 --> 00:25:39.980
Locally, it doesn't even use containers.

00:25:39.980 --> 00:25:43.020
It's just all pure Python for that.

00:25:43.020 --> 00:25:46.580
But once you deploy, yeah, I think you might want to go down the container route, but it's

00:25:46.580 --> 00:25:50.260
nice not having to have Docker just to run a simple test deployment.

00:25:50.260 --> 00:25:51.260
Yeah.

00:25:51.260 --> 00:25:53.780
I guess not everyone's machine has that, for sure.

00:25:53.780 --> 00:25:56.020
So question from the audience here.

00:25:56.020 --> 00:25:59.700
Jazzy asks, does it hook into AWS in particular?

00:25:59.700 --> 00:26:04.860
Is it compatible with existing pipelines like ingestion lambdas or transform lambdas?

00:26:04.860 --> 00:26:06.820
Yeah, you can hook into AWS.

00:26:06.820 --> 00:26:09.420
So we have some AWS integrations built in.

00:26:09.420 --> 00:26:13.660
Like I mentioned before, there's nothing stopping you from importing Boto3 and doing anything

00:26:13.660 --> 00:26:14.660
really you want.

00:26:14.660 --> 00:26:15.980
So a very simple use case.

00:26:15.980 --> 00:26:20.260
Like let's say you already have an existing transformation being triggered in AWS through

00:26:20.260 --> 00:26:21.260
some lambda.

00:26:21.260 --> 00:26:25.580
You could just model that within Dijkstra and say, you know, trigger that lambda Boto3.

00:26:25.580 --> 00:26:26.580
Okay.

00:26:26.580 --> 00:26:31.060
Then the asset itself is really that representation of that pipeline, but you're not actually running

00:26:31.060 --> 00:26:32.820
that code within Dijkstra itself.

00:26:32.820 --> 00:26:34.380
That's still occurring on the AWS framework.

00:26:34.380 --> 00:26:38.980
And that's a really simple way to start adding a little bit of observability and orchestration

00:26:38.980 --> 00:26:40.100
to existing pipelines.

00:26:40.100 --> 00:26:41.100
Okay.

00:26:41.100 --> 00:26:45.740
That's pretty cool because now you have this nice UI and these metadata in this history,

00:26:45.740 --> 00:26:47.300
but it's someone else's cloud.

00:26:47.300 --> 00:26:48.300
Exactly.

00:26:48.300 --> 00:26:49.300
Yeah.

00:26:49.300 --> 00:26:50.420
And you can start to pull more information in there.

00:26:50.420 --> 00:26:54.260
And over time you might decide, you know, this, you know, lambda that I had, it's starting

00:26:54.260 --> 00:26:55.260
to get out of hand.

00:26:55.260 --> 00:26:59.620
I want to kind of break it apart into multiple assets where I want to sort of optimize it

00:26:59.620 --> 00:27:01.940
a little way and Dijkstra can help you along that.

00:27:01.940 --> 00:27:02.940
Yeah.

00:27:02.940 --> 00:27:03.940
Excellent.

00:27:03.940 --> 00:27:08.260
How do you set up like triggers or observability inside Dijkstra?

00:27:08.260 --> 00:27:11.320
Like Jazzy's asking about S3, but like in general, right?

00:27:11.320 --> 00:27:16.480
If a row is entered into a database, something is dropped in a blob storage or the date changes.

00:27:16.480 --> 00:27:17.480
I don't know.

00:27:17.480 --> 00:27:18.480
Yeah.

00:27:18.480 --> 00:27:19.480
Those are great questions.

00:27:19.480 --> 00:27:20.480
You have a lot of options.

00:27:20.480 --> 00:27:24.080
In Dijkstra, we do model every asset with a couple little flags, I think, that are really

00:27:24.080 --> 00:27:25.480
useful to think about.

00:27:25.480 --> 00:27:28.560
One is whether the code of that particular asset has changed, right?

00:27:28.560 --> 00:27:32.840
And then the other one is whether anything upstream of that asset has changed.

00:27:32.840 --> 00:27:38.160
And those two things really power a lot of automation functionality that we can get downstream.

00:27:38.160 --> 00:27:41.720
So let's start with the S3 example, it's the easiest to understand.

00:27:41.720 --> 00:27:46.160
You have a bucket and there is a file that gets uploaded every day.

00:27:46.160 --> 00:27:48.040
You don't know what time that file gets uploaded.

00:27:48.040 --> 00:27:51.040
You don't know when it'll be uploaded, but you know at some point it will be.

00:27:51.040 --> 00:27:55.680
In Dijkstra, we have a thing called the sensor, which you can just connect to an S3 location.

00:27:55.680 --> 00:27:59.280
You can define how it looks into that file or into that folder.

00:27:59.280 --> 00:28:02.800
And then you would just pull every 30 seconds until something happens.

00:28:02.800 --> 00:28:06.360
When that something happens, that triggers sort of an event.

00:28:06.360 --> 00:28:10.680
And that event can trickle at your will downstream to everything that depends on it as you sort

00:28:10.680 --> 00:28:12.040
of connect to these things.

00:28:12.040 --> 00:28:15.480
So it gets you awake from this like, "Oh, I'm going to schedule something to run every

00:28:15.480 --> 00:28:16.480
hour.

00:28:16.480 --> 00:28:18.440
Maybe the data will be there, but maybe it won't."

00:28:18.440 --> 00:28:20.540
And you can have a much more event-based workflow.

00:28:20.540 --> 00:28:25.160
When this file runs, I want everything downstream to know that this data has changed.

00:28:25.160 --> 00:28:28.920
And as sort of data flows through these systems, everything will sort of work its way down.

00:28:28.920 --> 00:28:32.040
Yeah, I like it.

00:28:32.040 --> 00:28:36.160
This portion of Talk Python to Me is brought to you by Posit, the makers of Shiny, formerly

00:28:36.160 --> 00:28:40.520
RStudio, and especially Shiny for Python.

00:28:40.520 --> 00:28:41.520
Let me ask you a question.

00:28:41.520 --> 00:28:43.520
Are you building awesome things?

00:28:43.520 --> 00:28:44.520
Of course you are.

00:28:44.520 --> 00:28:46.000
You're a developer or a data scientist.

00:28:46.000 --> 00:28:47.040
That's what we do.

00:28:47.040 --> 00:28:49.520
And you should check out Posit Connect.

00:28:49.520 --> 00:28:54.120
Posit Connect is a way for you to publish, share, and deploy all the data products that

00:28:54.120 --> 00:28:56.780
you're building using Python.

00:28:56.780 --> 00:28:59.000
People ask me the same question all the time.

00:28:59.000 --> 00:29:02.640
"Michael, I have some cool data science project or notebook that I built.

00:29:02.640 --> 00:29:05.360
How do I share it with my users, stakeholders, teammates?

00:29:05.360 --> 00:29:10.960
Do I need to learn FastAPI or Flask or maybe Vue or ReactJS?"

00:29:10.960 --> 00:29:11.960
Hold on now.

00:29:11.960 --> 00:29:15.360
Those are cool technologies, and I'm sure you'd benefit from them, but maybe stay focused

00:29:15.360 --> 00:29:16.800
on the data project?

00:29:16.800 --> 00:29:19.240
Let Posit Connect handle that side of things.

00:29:19.240 --> 00:29:23.640
With Posit Connect, you can rapidly and securely deploy the things you build in Python.

00:29:23.640 --> 00:29:30.360
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quarto, Ports, Dashboards, and APIs.

00:29:30.360 --> 00:29:32.600
Posit Connect supports all of them.

00:29:32.600 --> 00:29:37.400
And Posit Connect comes with all the bells and whistles to satisfy IT and other enterprise

00:29:37.400 --> 00:29:38.400
requirements.

00:29:38.400 --> 00:29:42.640
Make deployment the easiest step in your workflow with Posit Connect.

00:29:42.640 --> 00:29:49.040
For a limited time, you can try Posit Connect for free for three months by going to talkpython.fm/posit.

00:29:49.040 --> 00:29:52.320
That's talkpython.fm/POSIT.

00:29:52.320 --> 00:29:54.760
The link is in your podcast player show notes.

00:29:54.760 --> 00:29:59.440
Thank you to the team at Posit for supporting Talk Python.

00:29:59.440 --> 00:30:05.560
The sensor concept is really cool because I'm sure that there's a ton of cloud machines

00:30:05.560 --> 00:30:10.360
people provisioned just because this thing runs every 15 minutes, that runs every 30

00:30:10.360 --> 00:30:14.560
minutes, and you add them up and in aggregate, we need eight machines just to handle the

00:30:14.560 --> 00:30:18.840
automation, rather than, you know, because they're hoping to catch something without

00:30:18.840 --> 00:30:22.480
too much latency, but maybe like that actually only changes once a week.

00:30:22.480 --> 00:30:23.480
Exactly.

00:30:23.480 --> 00:30:27.800
And I think that's where we have to like sometimes step away from the way we're so used to thinking

00:30:27.800 --> 00:30:30.000
about things, and I'm guilty of this.

00:30:30.000 --> 00:30:33.360
When I create a data pipeline, my natural inclination is to create a schedule where

00:30:33.360 --> 00:30:34.960
it's a, is this a daily one?

00:30:34.960 --> 00:30:35.960
Is this weekly?

00:30:35.960 --> 00:30:36.960
Is this monthly?

00:30:36.960 --> 00:30:39.720
But what I'm finding more and more is when I'm creating my pipelines, I'm not adding

00:30:39.720 --> 00:30:40.720
a schedule.

00:30:40.720 --> 00:30:45.320
I'm using Dagster's auto-materialized policies, and I'm just telling it, you figure it out.

00:30:45.320 --> 00:30:46.800
I don't have to think about schedules.

00:30:46.800 --> 00:30:49.000
Just figure out when the things should be updated.

00:30:49.000 --> 00:30:51.540
When it's, you know, parents have been updated, you run.

00:30:51.540 --> 00:30:53.420
When the data has changed, you run.

00:30:53.420 --> 00:30:55.560
And then just like figure it out and leave me alone.

00:30:55.560 --> 00:30:56.560
Yeah.

00:30:56.560 --> 00:30:57.560
And it's worked pretty well for me so far.

00:30:57.560 --> 00:30:58.560
I think it's great.

00:30:58.560 --> 00:31:04.080
I have a search, refresh the search index on the various podcast pages that runs and

00:31:04.080 --> 00:31:08.400
it runs every hour, but the podcast ships weekly, right?

00:31:08.400 --> 00:31:10.160
But I don't know which hour it is.

00:31:10.160 --> 00:31:14.800
And so it seems like that's enough latency, but it would be way better to put just a little

00:31:14.800 --> 00:31:15.800
bit of smarts.

00:31:15.800 --> 00:31:18.800
Like what was the last date that anything changed?

00:31:18.800 --> 00:31:20.040
Was that since the last time you saw it?

00:31:20.040 --> 00:31:21.600
Maybe we'll just leave that alone, you know?

00:31:21.600 --> 00:31:26.080
But yeah, you're starting to inspire me to go write more code, but pretty cool.

00:31:26.080 --> 00:31:27.080
All right.

00:31:27.080 --> 00:31:33.320
So on the homepage at Dagster.io, you've got a nice graphic that shows you both how to

00:31:33.320 --> 00:31:38.220
write the code, like some examples of the code, as well as how that looks in the UI.

00:31:38.220 --> 00:31:41.920
And one of them is called, says to launch backfills.

00:31:41.920 --> 00:31:43.120
What is this backfill thing?

00:31:43.120 --> 00:31:44.960
Oh, this is my favorite thing.

00:31:44.960 --> 00:31:45.960
Okay.

00:31:45.960 --> 00:31:50.400
So when you first start your data journey as a data engineer, you sort of have a pipeline

00:31:50.400 --> 00:31:54.100
and you build it and it just runs on a schedule and that's fine.

00:31:54.100 --> 00:31:58.440
What you soon find is, you know, you might have to go back in time.

00:31:58.440 --> 00:32:01.720
You might say, I've got this data set that updates monthly.

00:32:01.720 --> 00:32:05.200
Here's a great example, AWS cost reporting, right?

00:32:05.200 --> 00:32:10.440
AWS will send you some data around, you know, all your instances and your S3 bucket, all

00:32:10.440 --> 00:32:11.440
that.

00:32:11.440 --> 00:32:14.520
And it'll update that data every day or every month or whatever have you.

00:32:14.520 --> 00:32:18.680
Due to some reason, you've got to go back in time and refresh data that AWS updated

00:32:18.680 --> 00:32:20.600
due to some like discrepancy.

00:32:20.600 --> 00:32:21.600
Backfill is sort of how you do that.

00:32:21.600 --> 00:32:24.800
And it works hand in hand with this idea of a partition.

00:32:24.800 --> 00:32:28.320
A partition is sort of how your data is naturally organized.

00:32:28.320 --> 00:32:31.600
And it's like a nice way to represent that natural organization.

00:32:31.600 --> 00:32:35.080
Has nothing to do with like the fundamental way, how often you want to run it.

00:32:35.080 --> 00:32:39.480
It's more around like, I've got a data set that comes in once a month, it's represented

00:32:39.480 --> 00:32:40.480
monthly.

00:32:40.480 --> 00:32:43.160
It might be updated daily, but it's the representation of the data is monthly.

00:32:43.160 --> 00:32:44.720
So I will partition it by month.

00:32:44.720 --> 00:32:46.280
It doesn't have to be dates.

00:32:46.280 --> 00:32:47.280
It could be strings.

00:32:47.280 --> 00:32:48.680
It could be a list.

00:32:48.680 --> 00:32:55.760
You could have a partition for every company or every client or every domain you have.

00:32:55.760 --> 00:33:00.360
Whatever you sort of think is a natural way to think about breaking apart that pipeline.

00:33:00.360 --> 00:33:03.880
And once you do that partition, you can do these nice things called backfills, which

00:33:03.880 --> 00:33:08.200
says instead of running this entire pipeline and all my data, I want you to pick that one

00:33:08.200 --> 00:33:13.220
month where data went wrong or that one month where data was missing and just run the partition

00:33:13.220 --> 00:33:14.320
on that range.

00:33:14.320 --> 00:33:18.320
And so you limit compute, you save resources and get a little bit more efficient.

00:33:18.320 --> 00:33:22.280
And it's just easier to like, think about your pipeline because you've got this natural

00:33:22.280 --> 00:33:23.800
built in partitioning system.

00:33:23.800 --> 00:33:24.800
Excellent.

00:33:24.800 --> 00:33:27.680
So maybe you missed some important event.

00:33:27.680 --> 00:33:31.520
Maybe your automation went down for a little bit, came back up.

00:33:31.520 --> 00:33:33.360
You're like, Oh no, we've, we've missed it.

00:33:33.360 --> 00:33:34.360
Right.

00:33:34.360 --> 00:33:37.000
But you want to start over for three years.

00:33:37.000 --> 00:33:39.320
So maybe we could just go and run the last day.

00:33:39.320 --> 00:33:40.320
It's worth of.

00:33:40.320 --> 00:33:41.320
Exactly.

00:33:41.320 --> 00:33:45.000
Or another one would be your vendor says, Hey, by the way, we actually screwed up.

00:33:45.000 --> 00:33:48.280
We uploaded this file from two months ago, but the numbers were all wrong.

00:33:48.280 --> 00:33:49.280
Yeah.

00:33:49.280 --> 00:33:51.360
We've uploaded a new version to that destination.

00:33:51.360 --> 00:33:53.240
Can you update your data set?

00:33:53.240 --> 00:33:56.400
One way is to recompute the entire universe from scratch.

00:33:56.400 --> 00:34:00.560
But if you've partitioned things and you can say no limit that to just this one partition

00:34:00.560 --> 00:34:03.840
for that month and that one partition can trickle down all the way to all your other

00:34:03.840 --> 00:34:05.200
assets that depend on that.

00:34:05.200 --> 00:34:10.400
Do you have to pre decide, do you have to think about this partitioning beforehand or

00:34:10.400 --> 00:34:11.400
can you do it retroactively?

00:34:11.400 --> 00:34:12.400
You could do it retroactively.

00:34:12.400 --> 00:34:14.360
And I have done that before as well.

00:34:14.360 --> 00:34:16.640
It really depends on, on where you're at.

00:34:16.640 --> 00:34:19.360
I think it's your first asset ever.

00:34:19.360 --> 00:34:23.360
Probably don't bother with partitions, but it really isn't a lot of work to get them

00:34:23.360 --> 00:34:24.360
to get them started.

00:34:24.360 --> 00:34:25.360
Okay.

00:34:25.360 --> 00:34:26.360
Yeah.

00:34:26.360 --> 00:34:27.360
Really neat.

00:34:27.360 --> 00:34:28.360
I like a lot of the ideas here.

00:34:28.360 --> 00:34:29.360
I like that.

00:34:29.360 --> 00:34:33.080
It's got this visual component that you can see what's going on, inspect it.

00:34:33.080 --> 00:34:36.000
Just so you can debug runs or what happens there.

00:34:36.000 --> 00:34:40.120
Like obviously when you're pulling data from many different sources, maybe it's not your

00:34:40.120 --> 00:34:41.400
data you're taking in.

00:34:41.400 --> 00:34:42.400
Fields could vanish.

00:34:42.400 --> 00:34:43.840
It can be the wrong type.

00:34:43.840 --> 00:34:44.840
Systems can go down.

00:34:44.840 --> 00:34:47.400
I'm sure, sure the debugging is interesting.

00:34:47.400 --> 00:34:52.680
So what's, it looks a little bit kind of like a web browser debug dev tools thing.

00:34:52.680 --> 00:34:54.960
So for the record, my code never fails.

00:34:54.960 --> 00:34:57.960
I've never had a bug in my life, but for the one that you have.

00:34:57.960 --> 00:34:58.960
Yeah.

00:34:58.960 --> 00:34:59.960
Well, mine doesn't either.

00:34:59.960 --> 00:35:03.800
I only do it to make an example and for my, me, how others, yes.

00:35:03.800 --> 00:35:05.400
If I do it's intentional, of course.

00:35:05.400 --> 00:35:06.400
Yeah.

00:35:06.400 --> 00:35:08.160
To humble myself a little bit.

00:35:08.160 --> 00:35:09.160
Exactly.

00:35:09.160 --> 00:35:13.080
This view is one of my favorite, I mean, so many favorite views, but this is, it's actually

00:35:13.080 --> 00:35:16.920
really fun to watch, watch this actually run when you execute this pipeline.

00:35:16.920 --> 00:35:22.120
But really like, let's go back to, you know, the world before orchestrators, we use cron,

00:35:22.120 --> 00:35:23.120
right?

00:35:23.120 --> 00:35:26.360
We'd have a bash script that would do something and we'd have a cron job that said, make sure

00:35:26.360 --> 00:35:27.560
this thing runs.

00:35:27.560 --> 00:35:31.560
And then hopefully it was successful, but sometimes it wasn't.

00:35:31.560 --> 00:35:34.080
And it's a, sometimes it wasn't, that's always been the problem, right?

00:35:34.080 --> 00:35:35.640
It's like, well, what do I do now?

00:35:35.640 --> 00:35:36.640
How do I know why it failed?

00:35:36.640 --> 00:35:38.720
What was, when did it fail?

00:35:38.720 --> 00:35:41.480
You know, what, at what point or what steps did it fail?

00:35:41.480 --> 00:35:42.640
That's really hard to do.

00:35:42.640 --> 00:35:47.640
But this debugger really is, is, is a structured log of every step that's been going on through

00:35:47.640 --> 00:35:48.640
your pipeline, right?

00:35:48.640 --> 00:35:52.120
So in this view, there's three assets that we can kind of see here.

00:35:52.120 --> 00:35:53.120
One is called users.

00:35:53.120 --> 00:35:56.080
One is called orders and one is to run dbt.

00:35:56.080 --> 00:36:00.640
So presumably there's these two, you know, tables that are being updated and then a dbt

00:36:00.640 --> 00:36:01.640
job.

00:36:01.640 --> 00:36:03.120
It looks like that's being updated at the very end.

00:36:03.120 --> 00:36:07.840
Once you execute this pipeline, all the logs are captured from each of those assets.

00:36:07.840 --> 00:36:10.800
So you can manually write your own logs.

00:36:10.800 --> 00:36:14.680
You have access to a Python logger and you can use your info, your error, whatever have

00:36:14.680 --> 00:36:16.400
you in log output that way.

00:36:16.400 --> 00:36:21.600
And it'll be captured in a structured way, but it also capture logs from your integrations.

00:36:21.600 --> 00:36:24.320
So if you're using dbt, we capture those logs as well.

00:36:24.320 --> 00:36:26.960
You can see it processing every single asset.

00:36:26.960 --> 00:36:31.880
So if anything does go wrong, you can filter down and understand at what step, at what

00:36:31.880 --> 00:36:33.720
point did something go wrong.

00:36:33.720 --> 00:36:34.720
- That's awesome.

00:36:34.720 --> 00:36:40.560
And just the historical aspect, cause just going through logs, especially multiple systems

00:36:40.560 --> 00:36:44.760
can be really, really tricky to figure out what's the problem, what actually caused this

00:36:44.760 --> 00:36:49.920
to go wrong, but come back and say, oh, it crashed, pull up the UI and see, all right,

00:36:49.920 --> 00:36:53.320
well show me, show me what this run did, show me what this job did.

00:36:53.320 --> 00:36:57.600
And it seems like it's a lot easier to debug than your standard web API or something like

00:36:57.600 --> 00:36:58.600
that.

00:36:58.600 --> 00:36:59.600
- Exactly.

00:36:59.600 --> 00:37:02.840
You can click on any of these assets that get that metadata that we had earlier as well.

00:37:02.840 --> 00:37:06.840
If you know, one step failed and it's kind of flaky, you can just click on that one step

00:37:06.840 --> 00:37:08.680
and say, just rerun this.

00:37:08.680 --> 00:37:09.680
Everything else is fine.

00:37:09.680 --> 00:37:10.680
It's a restart from scratch.

00:37:10.680 --> 00:37:11.680
- Okay.

00:37:11.680 --> 00:37:15.440
And it'll keep the data from before, so you don't have to rerun that.

00:37:15.440 --> 00:37:16.440
- Yeah.

00:37:16.440 --> 00:37:18.120
I mean, it depends on how you built the pipeline.

00:37:18.120 --> 00:37:21.920
We like to build item potent pipelines is how we sort of talk about it, the data engineering

00:37:21.920 --> 00:37:23.200
landscape, right?

00:37:23.200 --> 00:37:27.040
So you should be able to run something multiple times and not break anything in a perfect

00:37:27.040 --> 00:37:28.040
world.

00:37:28.040 --> 00:37:30.240
That's not always possible, but ideally, yes.

00:37:30.240 --> 00:37:34.360
And so we can presume that if users completed successfully, then we don't have to run that

00:37:34.360 --> 00:37:38.900
again because that data was persisted, you know, database S3 somewhere.

00:37:38.900 --> 00:37:43.120
And if orders was the one that was broken, we can just only run orders and not have to

00:37:43.120 --> 00:37:45.400
worry about rewriting the whole thing from scratch.

00:37:45.400 --> 00:37:46.400
- Excellent.

00:37:46.400 --> 00:37:50.880
So item potent for people who maybe don't know, you run it once or you perform the operation

00:37:50.880 --> 00:37:55.480
once or you perform it 20 times, same outcome should have side effects, right?

00:37:55.480 --> 00:37:56.480
- That's the idea.

00:37:57.480 --> 00:37:58.480
- Easier said than done sometimes.

00:37:58.480 --> 00:37:59.480
- It sure is.

00:37:59.480 --> 00:38:03.720
- Sometimes it's easy, sometimes it's very hard, but the more you can build pipelines

00:38:03.720 --> 00:38:07.960
that way, the easier your life becomes in many ways.

00:38:07.960 --> 00:38:08.960
- Exactly.

00:38:08.960 --> 00:38:10.680
Not always, but generally true for programming as well, right?

00:38:10.680 --> 00:38:14.760
If you talk to functional programming people, they'll say like, it's an absolute, but.

00:38:14.760 --> 00:38:17.520
- Yes, functional programmers love this kind of stuff.

00:38:17.520 --> 00:38:21.200
And it actually does lend itself really well to data pipelines.

00:38:21.200 --> 00:38:24.800
Data pipelines, unlike maybe some of the software engineering stuff, it's a little bit different

00:38:24.800 --> 00:38:30.340
in that the data changing is what causes often most of the headaches, right?

00:38:30.340 --> 00:38:36.100
It's less so the actual code you write, but more the expectation tends to change so frequently

00:38:36.100 --> 00:38:41.120
and so often in new and novel, interesting ways that you would often never expect.

00:38:41.120 --> 00:38:46.160
And so the more you can sort of make that function so pure that you can provide any

00:38:46.160 --> 00:38:51.120
sort of dataset and really test really easily these expectations when they get broken, the

00:38:51.120 --> 00:38:55.080
easier it is to sort of debug these things and build on them in the future.

00:38:55.080 --> 00:38:56.080
- Yeah.

00:38:56.080 --> 00:38:57.080
And cache them as well.

00:38:57.080 --> 00:38:58.080
- Yes, it's always nice.

00:38:58.080 --> 00:38:59.080
- Yeah.

00:38:59.080 --> 00:39:02.200
So speaking of that kind of stuff, like what's the scalability story?

00:39:02.200 --> 00:39:08.580
If I've got some big, huge, complicated data pipeline, can I parallelize them and have

00:39:08.580 --> 00:39:10.940
them run multiple pieces?

00:39:10.940 --> 00:39:12.820
Like if there's different branches or something like that?

00:39:12.820 --> 00:39:13.820
- Yeah, exactly.

00:39:13.820 --> 00:39:20.020
That's one of the key benefits I think in writing your assets in this DAG way, right?

00:39:20.020 --> 00:39:22.780
Anything that is parallelizable will be parallelized.

00:39:22.780 --> 00:39:24.580
Now sometimes you might want to put limits on that.

00:39:24.580 --> 00:39:26.320
Sometimes too much parallelization is bad.

00:39:26.320 --> 00:39:28.020
Your poor little database can't handle it.

00:39:28.020 --> 00:39:32.640
And you can say, you know, maybe a concurrency limit on this one just for today is worth

00:39:32.640 --> 00:39:36.860
putting, or if you're hitting an API for an external vendor, they might not appreciate

00:39:36.860 --> 00:39:39.060
10,000 requests a second on that one.

00:39:39.060 --> 00:39:40.680
So maybe you would slow it down.

00:39:40.680 --> 00:39:41.680
But in this case-

00:39:41.680 --> 00:39:42.680
- Or rate limiting, right?

00:39:42.680 --> 00:39:45.920
You can run into too many requests and then your stuff crashes, then you got to start.

00:39:45.920 --> 00:39:46.920
It can be a whole thing.

00:39:46.920 --> 00:39:47.920
- It can be a whole thing.

00:39:47.920 --> 00:39:51.220
There's memory concerns, but let's pretend the world is simple.

00:39:51.220 --> 00:39:54.440
Anything that can be parallelized will be through Dagster.

00:39:54.440 --> 00:39:57.960
And that's really the benefit of writing these DAGs is that there's a nice algorithm for

00:39:57.960 --> 00:39:59.760
determining what that actually looks like.

00:39:59.760 --> 00:40:00.760
- Yeah.

00:40:00.760 --> 00:40:03.440
I guess if you have a diamond shape or any sort of split, right?

00:40:03.440 --> 00:40:07.800
Those two things now become, 'cause it's a cyclical, they can't turn around and then

00:40:07.800 --> 00:40:09.360
eventually depend on each other again.

00:40:09.360 --> 00:40:12.520
So that's a perfect chance to just go fork it out.

00:40:12.520 --> 00:40:13.520
- Exactly.

00:40:13.520 --> 00:40:15.340
And that's kind of where partitions are also kind of interesting.

00:40:15.340 --> 00:40:19.600
If you have a partitioned asset, you could take your dataset partitioned to five, you

00:40:19.600 --> 00:40:23.480
know, buckets and run all five partitions at once, knowing full well that because you've

00:40:23.480 --> 00:40:28.220
written this in a idempotent and partitioned way, that the first pipeline will only operate

00:40:28.220 --> 00:40:32.040
on apples and the second one only operates on bananas.

00:40:32.040 --> 00:40:35.280
And there is no commingling of apples and bananas anywhere in the pipeline.

00:40:35.280 --> 00:40:36.280
- Oh, that's interesting.

00:40:36.280 --> 00:40:39.840
I hadn't really thought about using the partitions for parallelism, but of course.

00:40:39.840 --> 00:40:40.840
- Yeah.

00:40:40.840 --> 00:40:43.560
It's a fun little way to break things apart.

00:40:43.560 --> 00:40:49.720
- So if we run this on the Dagster cloud or even on our own, is this pretty much automatic?

00:40:49.720 --> 00:40:51.280
We don't have to do anything?

00:40:51.280 --> 00:40:55.200
I think Dagster just looks at it and says, this looks parallelizable and it'll go or?

00:40:55.200 --> 00:40:56.200
- That's right.

00:40:56.200 --> 00:40:57.200
Yeah.

00:40:57.200 --> 00:41:00.560
As long as you've got the full deployment, whether it's OSS or cloud, Dagster will basically

00:41:00.560 --> 00:41:02.920
parallelize it for you, which is possible.

00:41:02.920 --> 00:41:04.640
You can set global currency limits.

00:41:04.640 --> 00:41:09.680
So you might say, you know, 64 is more than enough, you know, parallelization that I need,

00:41:09.680 --> 00:41:13.760
or maybe I want less because I'm worried about overloading systems, but it's really up to

00:41:13.760 --> 00:41:14.760
you.

00:41:14.760 --> 00:41:19.240
- Putting this on a $10 server, please don't kill it.

00:41:19.240 --> 00:41:21.840
First respect that it's somewhat wimpy, but that's okay.

00:41:21.840 --> 00:41:22.840
- It'll get the job done.

00:41:23.840 --> 00:41:24.840
All right.

00:41:24.840 --> 00:41:29.360
I want to talk about some of the tools and some of the tools that are maybe at play here

00:41:29.360 --> 00:41:31.560
when working with Dagster and some of the trends and stuff.

00:41:31.560 --> 00:41:37.680
But before that, maybe speak to where you could see people adopt a tool like Dagster,

00:41:37.680 --> 00:41:39.040
but they generally don't.

00:41:39.040 --> 00:41:43.040
They don't realize like, oh, actually there's a whole framework for this, right?

00:41:43.040 --> 00:41:49.700
Like I could, sure I could go and build just on HTTP server and hook into the request and

00:41:49.700 --> 00:41:50.700
start writing to it.

00:41:50.700 --> 00:41:53.000
But like, maybe I should use Flask or FastAPI.

00:41:53.000 --> 00:41:58.480
Like there's these frameworks that we really naturally adopt for certain situations like

00:41:58.480 --> 00:42:04.040
APIs and others, background jobs, data pipelines, where I think there's probably a good chunk

00:42:04.040 --> 00:42:07.640
of people who could benefit from stuff like this, but they just don't think they need

00:42:07.640 --> 00:42:09.120
a framework for it.

00:42:09.120 --> 00:42:10.320
Like cron is enough.

00:42:10.320 --> 00:42:13.000
- Yeah, it's funny because sometimes cron is enough.

00:42:13.000 --> 00:42:18.200
And I don't want to encourage people not to use cron, but think twice at least is what

00:42:18.200 --> 00:42:19.200
I would say.

00:42:19.200 --> 00:42:24.000
So probably the first like trigger for me of thinking of, you know, is Dagster a good

00:42:24.000 --> 00:42:26.680
choice is like, am I trying to ingest data from somewhere?

00:42:26.680 --> 00:42:28.280
Is that's something that fails.

00:42:28.280 --> 00:42:32.160
Like I think we just can accept that, you know, if you're moving data around, the data

00:42:32.160 --> 00:42:35.320
source will break, the expectations will change.

00:42:35.320 --> 00:42:36.560
You'll need to debug it.

00:42:36.560 --> 00:42:37.800
You'll need to rerun it.

00:42:37.800 --> 00:42:39.480
And doing that in cron is a nightmare.

00:42:39.480 --> 00:42:43.080
So I would say definitely start to think about an orchestration system.

00:42:43.080 --> 00:42:46.840
If you're ingesting data, if you have a simple cron job that sends one email, like you're

00:42:46.840 --> 00:42:47.840
probably fine.

00:42:47.840 --> 00:42:51.440
I don't think you need to implement all of Dagster just to do that.

00:42:51.440 --> 00:42:58.240
But the more closer you get to data pipelining, I think the better your life will be if you're

00:42:58.240 --> 00:43:03.560
not trying to debug a obtuse process that no one really understands six months from

00:43:03.560 --> 00:43:04.560
now.

00:43:04.560 --> 00:43:05.560
- Excellent.

00:43:05.560 --> 00:43:08.720
All right, maybe we could touch on some of the tools that are interesting to see people

00:43:08.720 --> 00:43:09.720
using.

00:43:09.720 --> 00:43:15.400
You talked about DuckDB and DBT, a lot of Ds starting here, but give us a sense of like

00:43:15.400 --> 00:43:19.400
some of the supporting tools you see a lot of folks using that are interesting.

00:43:19.400 --> 00:43:20.400
- Yeah, for sure.

00:43:20.400 --> 00:43:27.760
I think in the data space, probably DBT is one of the most popular choices and DBT in

00:43:27.760 --> 00:43:33.760
many ways, it's nothing more than a command line tool that runs a bunch of SQL in a bag

00:43:33.760 --> 00:43:34.760
as well.

00:43:34.760 --> 00:43:37.920
So there's actually a nice fit with Dagster and DBT together.

00:43:37.920 --> 00:43:44.160
DBT is really used by people who are trying to model that business process using SQL against

00:43:44.160 --> 00:43:45.440
typically a data warehouse.

00:43:45.440 --> 00:43:51.920
So if you have your data in, for example, a Postgres, a Snowflake, Databricks, Microsoft

00:43:51.920 --> 00:43:56.680
SQL, these types of data warehouses, generally you're trying to model some type of business

00:43:56.680 --> 00:44:00.400
process and typically people use SQL to do that.

00:44:00.400 --> 00:44:06.360
Now you can do this without DBT, but DBT has provided a nice clean interface to doing so.

00:44:06.360 --> 00:44:10.440
It makes it very easy to connect these models together, to run them, to have a development

00:44:10.440 --> 00:44:12.400
workflow that works really well.

00:44:12.400 --> 00:44:15.480
And then you can push it to prod and have things run again in production.

00:44:15.480 --> 00:44:17.480
So that's DBT.

00:44:17.480 --> 00:44:18.960
We find it works really well.

00:44:18.960 --> 00:44:21.680
And a lot of our customers are actually using DBT as well.

00:44:21.680 --> 00:44:27.480
There's DuckDB, which is a great, it's like the SQLite for columnar databases, right?

00:44:27.480 --> 00:44:30.680
It's in process, it's fast, it's written by the Dutch.

00:44:30.680 --> 00:44:32.640
There's nothing you can't like about it.

00:44:32.640 --> 00:44:33.640
It's free.

00:44:33.640 --> 00:44:34.640
We love that.

00:44:34.640 --> 00:44:37.000
It's a little bit more simple in Python itself.

00:44:37.000 --> 00:44:38.000
It does.

00:44:38.000 --> 00:44:39.000
It's so easy.

00:44:39.000 --> 00:44:40.000
Yes, exactly.

00:44:40.000 --> 00:44:42.640
The Dutch have given us so much and they've asked nothing of us.

00:44:42.640 --> 00:44:44.760
So I'm always very thankful for them.

00:44:44.760 --> 00:44:45.760
It's fast.

00:44:45.760 --> 00:44:46.760
It's so fast.

00:44:46.760 --> 00:44:51.600
It's like if you've ever used pandas for processing large volumes of data, you've occasionally

00:44:51.600 --> 00:44:56.240
hit memory limits or inefficiencies in doing these large aggregates.

00:44:56.240 --> 00:45:01.640
I won't go into all the reasons of why that is, but DuckDB sort of changes that because

00:45:01.640 --> 00:45:09.040
it's a fast serverless sort of C++ written tooling to do really fast vectorized work.

00:45:09.040 --> 00:45:11.040
And by that, I mean, like it works on columns.

00:45:11.040 --> 00:45:15.640
So typically in like SQLite, you're doing transactions, you're doing single row updates,

00:45:15.640 --> 00:45:18.600
writes, inserts, and SQLite is great at that.

00:45:18.600 --> 00:45:23.760
Where typical transactional databases fail or aren't as powerful is when you're doing

00:45:23.760 --> 00:45:26.000
aggregates, when you're looking at an entire column, right?

00:45:26.000 --> 00:45:27.000
Just the way they're architected.

00:45:27.000 --> 00:45:32.800
If you want to know the average, the median, the sum of some large number of columns, and

00:45:32.800 --> 00:45:36.600
you want to group that by a whole bunch of things, you want to know the first date someone

00:45:36.600 --> 00:45:41.400
did something and the last one, those types of vectorized operations, DuckDB is really,

00:45:41.400 --> 00:45:42.900
really fast at doing.

00:45:42.900 --> 00:45:48.640
And it's a great alternative to, for example, pandas, which can often hit memory limits

00:45:48.640 --> 00:45:50.200
and be a little bit slow in that regard.

00:45:50.200 --> 00:45:54.960
Yeah, it looks like it has some pretty cool aspects, transactions, of course, but it also

00:45:54.960 --> 00:45:59.420
says direct Parquet, CSV, and JSON querying.

00:45:59.420 --> 00:46:04.080
So if you've got a CSV file hanging around and you want to ask questions about it or

00:46:04.080 --> 00:46:09.440
JSON or some of the data science stuff through Parquet, you know, turn a indexed proper query

00:46:09.440 --> 00:46:10.440
engine against it.

00:46:10.440 --> 00:46:12.640
Don't just use a dictionary or something, right?

00:46:12.640 --> 00:46:19.320
Yeah, it's great for reading a CSV, zip files, tar files, Parquet, partition Parquet files,

00:46:19.320 --> 00:46:23.760
all that stuff that usually was really annoying to do and operate on, you can now install

00:46:23.760 --> 00:46:24.760
DuckDB.

00:46:24.760 --> 00:46:25.920
It's got a great CLI too.

00:46:25.920 --> 00:46:30.880
So before you go out and like program your entire pipeline, you just run DuckDB and you

00:46:30.880 --> 00:46:35.640
can start writing SQL against CSV files and all this stuff to really understand your data

00:46:35.640 --> 00:46:37.280
and just really see how quick it is.

00:46:37.280 --> 00:46:43.040
I used it on a bird dataset that I had as an example project and there was, you know,

00:46:43.040 --> 00:46:47.700
millions of rows and I was joining them together and doing massive group buys and it was done

00:46:47.700 --> 00:46:48.700
in like seconds.

00:46:48.700 --> 00:46:52.720
And it's just hard for me to believe that it was even correct because it was so quick.

00:46:52.720 --> 00:46:53.720
So it's wonderful.

00:46:53.720 --> 00:46:55.720
I must have done that wrong somehow.

00:46:55.720 --> 00:46:58.400
Because it's done, it shouldn't be done.

00:46:58.400 --> 00:46:59.400
Yeah.

00:46:59.400 --> 00:47:03.720
And the fact it's in process means there's not a babysit, a server for you to babysit

00:47:03.720 --> 00:47:06.880
patch, make sure it's still running.

00:47:06.880 --> 00:47:08.740
It's accessible, but not too accessible.

00:47:08.740 --> 00:47:09.740
All that, right?

00:47:09.740 --> 00:47:12.480
It's a pip install away, which is always, we love that, right?

00:47:12.480 --> 00:47:13.480
Yeah, absolutely.

00:47:13.480 --> 00:47:18.140
You mentioned, I guess I mentioned Parquet, but also Apache Arrow seems like it's making

00:47:18.140 --> 00:47:22.720
its way into a lot of, a lot of different tools and sort of foundational sort of high

00:47:22.720 --> 00:47:25.780
memory, high performance in memory processing.

00:47:25.780 --> 00:47:26.780
Have you used this Eddie?

00:47:26.780 --> 00:47:30.620
I've used it, especially through like working through different languages.

00:47:30.620 --> 00:47:34.300
So moving data between Python and R is where I last used this.

00:47:34.300 --> 00:47:35.900
I think Arrow's great at that.

00:47:35.900 --> 00:47:41.120
I believe Arrow is like the, underneath some of the rust to Python as well.

00:47:41.120 --> 00:47:42.120
It's working there.

00:47:42.120 --> 00:47:46.940
So typically I don't use Arrow like directly myself, but it's in many of the tooling I

00:47:46.940 --> 00:47:47.940
use.

00:47:47.940 --> 00:47:48.940
It's a great product.

00:47:48.940 --> 00:47:52.220
And like so much of the ecosystem is now built on, on Arrow.

00:47:52.220 --> 00:47:53.220
Yeah.

00:47:53.220 --> 00:47:55.780
I think a lot of it is, I feel like the first time I heard about it was through Polars.

00:47:55.780 --> 00:47:56.780
That's right.

00:47:56.780 --> 00:47:57.780
Yeah.

00:47:57.780 --> 00:48:03.360
I'm pretty sure, which is another rust story for kind of like pandas, but a little bit

00:48:03.360 --> 00:48:05.180
more fluent, lazy API.

00:48:05.180 --> 00:48:06.180
Yes.

00:48:06.180 --> 00:48:07.380
We live in such great times to be honest.

00:48:07.380 --> 00:48:13.300
So Polars is a Python bindings for rust, I believe is kind of how I think about it.

00:48:13.300 --> 00:48:17.600
It does all the transformation and rust, but you've had this Python interface to it and

00:48:17.600 --> 00:48:20.660
it makes things again, incredibly fast.

00:48:20.660 --> 00:48:22.700
I would say similar in speed to DuckDB.

00:48:22.700 --> 00:48:24.660
They both are quite comparable sometimes.

00:48:24.660 --> 00:48:25.660
Yeah.

00:48:25.660 --> 00:48:29.980
It also comes to have vectorized and column runner processing and all that kind of stuff.

00:48:29.980 --> 00:48:31.080
It's pretty incredible.

00:48:31.080 --> 00:48:36.300
So not a drop in replacement for pandas, but if you have the opportunity to use it and

00:48:36.300 --> 00:48:39.740
you don't need to use the full breadth of what pandas offers, because pandas is quite

00:48:39.740 --> 00:48:40.740
a huge package.

00:48:40.740 --> 00:48:44.460
There's a lot it does, but if you're just doing simple transforms, I think Polars is

00:48:44.460 --> 00:48:45.460
a great option to explore.

00:48:45.460 --> 00:48:49.180
Yeah, I talked to a Richie Vink, Vink who is part of that.

00:48:49.180 --> 00:48:54.700
And I think they explicitly chose to not try to make it a drop in replacement for pandas,

00:48:54.700 --> 00:48:58.480
but try to choose an API that would allow the engine to be smarter and go like, I see

00:48:58.480 --> 00:49:02.220
you're asking for this, but the step before you wanted this other thing.

00:49:02.220 --> 00:49:04.540
So let me do that transformation all in one shot.

00:49:04.540 --> 00:49:07.500
And a little bit like a query optimization engine.

00:49:07.500 --> 00:49:08.500
What else is out there?

00:49:08.500 --> 00:49:10.020
A couple of guys, time for just a couple more.

00:49:10.020 --> 00:49:12.700
If there's anything that you're like, Oh yeah, people use this all the time.

00:49:12.700 --> 00:49:16.060
Especially the databases you've said, Postgres, Snowflake, et cetera.

00:49:16.060 --> 00:49:17.260
Yeah, there's so much.

00:49:17.260 --> 00:49:21.240
So another little one I like, it's called DLT, DLT hub.

00:49:21.240 --> 00:49:23.420
It's getting a lot of attraction as well.

00:49:23.420 --> 00:49:25.660
And what I like about it is how lightweight it is.

00:49:25.660 --> 00:49:28.020
I'm such a big fan of lightweight tooling.

00:49:28.020 --> 00:49:29.940
That's not, you know, massive frameworks.

00:49:29.940 --> 00:49:32.940
Loading data is I think still kind of yucky in many ways.

00:49:32.940 --> 00:49:33.940
It's not fun.

00:49:33.940 --> 00:49:36.900
And DLT makes it a little bit simpler and easier to do so.

00:49:36.900 --> 00:49:41.340
So that's what I would recommend people just to look into if you got to either ingest data

00:49:41.340 --> 00:49:45.860
from some API, some website, some CSV file.

00:49:45.860 --> 00:49:47.620
It's a great way to do that.

00:49:47.620 --> 00:49:52.580
It claims it's the Python library for data teams loading data into unexpected places.

00:49:52.580 --> 00:49:53.580
Very interesting.

00:49:53.580 --> 00:49:54.580
Yes, that's great.

00:49:54.580 --> 00:49:56.180
Yeah, this is, this looks cool.

00:49:56.180 --> 00:49:57.180
All right.

00:49:57.180 --> 00:50:01.580
Well, I guess maybe let's talk about, let's talk business and then we can talk about what's

00:50:01.580 --> 00:50:03.140
next and then we'll probably be out of time.

00:50:03.140 --> 00:50:04.540
I'm always fascinated.

00:50:04.540 --> 00:50:09.020
I think there's starting to be a bit of a blueprint for this, but companies that take

00:50:09.020 --> 00:50:12.460
a thing, they make it and they give it away and then they have a company around it.

00:50:12.460 --> 00:50:14.900
And congratulations to you all for doing that.

00:50:14.900 --> 00:50:15.900
Right.

00:50:15.900 --> 00:50:20.340
And a lot of it seems to kind of center around the open core model, which I don't know if

00:50:20.340 --> 00:50:24.100
that's exactly how you would characterize yourself, but yeah, maybe you should talk

00:50:24.100 --> 00:50:25.100
about the business side.

00:50:25.100 --> 00:50:29.260
Because I know there's many successful open source projects that don't necessarily result

00:50:29.260 --> 00:50:32.420
in full-time jobs or companies if people were to want that.

00:50:32.420 --> 00:50:34.220
It's a really interesting place.

00:50:34.220 --> 00:50:39.740
And I don't think it's one that anyone has truly figured out well, I can say this is

00:50:39.740 --> 00:50:42.060
the way forward for everyone, but it is something we're trying.

00:50:42.060 --> 00:50:44.540
And I think for Dexter, I think it's working pretty well.

00:50:44.540 --> 00:50:48.700
And what I think is really powerful about Dexter is like the open source product is

00:50:48.700 --> 00:50:49.860
really, really good.

00:50:49.860 --> 00:50:54.700
And it hasn't really been limited in many ways in order to drive like cloud product

00:50:54.700 --> 00:50:55.700
consumption.

00:50:55.700 --> 00:50:58.780
We really believe that there's actual value in that separation of these things.

00:50:58.780 --> 00:51:01.460
There are some things that we just can't do in the open source platform.

00:51:01.460 --> 00:51:06.700
For example, there's pipelines on cloud that involve ingesting data through our old systems

00:51:06.700 --> 00:51:11.060
in order to do reporting, which just doesn't make sense to do on the open source system.

00:51:11.060 --> 00:51:13.180
It makes the product way too complex.

00:51:13.180 --> 00:51:16.500
But for the most part, I think Dexter open source, we really believe that just getting

00:51:16.500 --> 00:51:19.820
it in the hands of developers is the best way to prove the value of it.

00:51:19.820 --> 00:51:23.620
And if we can build a business on top of that, I think we're all super happy to do so.

00:51:23.620 --> 00:51:27.460
It's nice that we get to sort of drive both sides of it.

00:51:27.460 --> 00:51:29.980
To me, that's one of the more exciting parts, right?

00:51:29.980 --> 00:51:35.020
A lot of the development that we do in Dexter open source is driven by people who are paid

00:51:35.020 --> 00:51:37.460
through what happens on Dexter cloud.

00:51:37.460 --> 00:51:41.900
And I think from what I can tell, there's no better way to build open source product

00:51:41.900 --> 00:51:45.260
than to have people who are adequately paid to develop that product.

00:51:45.260 --> 00:51:48.580
Otherwise it can be a labor of love, but one that doesn't last for very long.

00:51:48.580 --> 00:51:49.580
Yeah.

00:51:49.580 --> 00:51:52.860
Whenever I think about building software, there's 80% of it that's super exciting and

00:51:52.860 --> 00:51:58.620
fun, 10% and then there's that little sliver of like really fine polish that if it's not

00:51:58.620 --> 00:52:03.660
just your job to make that thing polished, you're just for the most part, just not going

00:52:03.660 --> 00:52:04.980
to polish that bit, right?

00:52:04.980 --> 00:52:05.980
It's tough.

00:52:05.980 --> 00:52:08.340
UI, design, support.

00:52:08.340 --> 00:52:12.460
There's all these things that go into making a software like really extraordinary.

00:52:12.460 --> 00:52:14.060
That's really, really tough to do.

00:52:14.060 --> 00:52:17.260
And I think I really like the open source business model.

00:52:17.260 --> 00:52:21.780
I think for me being able to just try something, not having talked to sales and being able

00:52:21.780 --> 00:52:24.860
to just deploy locally and test it out and see if this works.

00:52:24.860 --> 00:52:30.580
And if I choose to do so, deploy it in production, or if I bought the cloud product and I don't

00:52:30.580 --> 00:52:34.460
like the direction that it's going, I can even go to open source as well.

00:52:34.460 --> 00:52:35.460
That's pretty compelling to me.

00:52:35.460 --> 00:52:37.220
Yeah, for sure it is.

00:52:37.220 --> 00:52:43.140
And I think the more moving pieces of infrastructure, more uptime you want and all those types of

00:52:43.140 --> 00:52:49.100
things, the more somebody who's maybe a programmer, but not a DevOps infrastructure person, but

00:52:49.100 --> 00:52:50.660
needs to have it there, right?

00:52:50.660 --> 00:52:52.500
Like that's an opportunity as well, right?

00:52:52.500 --> 00:52:55.220
For you to say, look, you can write the code.

00:52:55.220 --> 00:52:59.020
We made it cool for you to write the code, but you don't have to like get notified when

00:52:59.020 --> 00:53:00.300
the server's down or whatever.

00:53:00.300 --> 00:53:01.940
Like, we'll just take care of that for you.

00:53:01.940 --> 00:53:02.940
That's pretty awesome.

00:53:02.940 --> 00:53:03.940
Yeah.

00:53:03.940 --> 00:53:04.940
And it's efficiencies of scale as well, right?

00:53:04.940 --> 00:53:08.460
Like we've learned the same mistakes over and over again, so you don't have to, which

00:53:08.460 --> 00:53:09.460
is nice.

00:53:09.460 --> 00:53:13.100
I don't know how many people who want to maintain servers, but people do.

00:53:13.100 --> 00:53:15.980
And they're more than welcome to if that's how they choose to do so.

00:53:15.980 --> 00:53:16.980
Yeah, for sure.

00:53:16.980 --> 00:53:17.980
All right.

00:53:17.980 --> 00:53:18.980
Just about out of time.

00:53:18.980 --> 00:53:23.380
Let's wrap up our conversation with where are things going for Dagster?

00:53:23.380 --> 00:53:24.380
What's on the roadmap?

00:53:24.380 --> 00:53:25.540
What are you excited about?

00:53:25.540 --> 00:53:26.620
Oh, that's a good one.

00:53:26.620 --> 00:53:29.740
I think we've actually published our roadmap line somewhere.

00:53:29.740 --> 00:53:31.940
If you search Dagster roadmap, it's probably out there.

00:53:31.940 --> 00:53:36.700
I think for the most part that hasn't changed much going into 2024, though we may update

00:53:36.700 --> 00:53:37.700
it.

00:53:37.700 --> 00:53:38.700
Ah, there it is.

00:53:38.700 --> 00:53:40.700
We're really just doubling down on what we've built already.

00:53:40.700 --> 00:53:45.220
I think there's a lot of work we can do on the product itself to make it easier to use,

00:53:45.220 --> 00:53:46.220
easier to understand.

00:53:46.300 --> 00:53:49.500
My team specifically is really focused around the education piece.

00:53:49.500 --> 00:53:53.820
And so we launched Dagster University's first module, which helps you really understand

00:53:53.820 --> 00:53:56.260
the core concepts around Dagster.

00:53:56.260 --> 00:54:00.540
Our next module is coming up in a couple months, and that'll be around using Dagster with dbt,

00:54:00.540 --> 00:54:02.500
which is our most popular integration.

00:54:02.500 --> 00:54:04.180
We're building up more integrations as well.

00:54:04.180 --> 00:54:09.460
So I built a little integration called embedded ELT that makes it easy to ingest data.

00:54:09.460 --> 00:54:12.940
But I want to actually build an integration with DLT as well, DLT hub.

00:54:12.940 --> 00:54:14.420
So we'll be doing that.

00:54:14.420 --> 00:54:18.060
And there's more coming down the pipe, but I don't know how much I can say.

00:54:18.060 --> 00:54:23.460
Look forward to an event in April where we'll have a launch event on all that's coming.

00:54:23.460 --> 00:54:24.460
Nice.

00:54:24.460 --> 00:54:26.740
Is that an online thing people can attend or something?

00:54:26.740 --> 00:54:27.740
Exactly.

00:54:27.740 --> 00:54:31.140
Yeah, there'll be some announcement there on the Dagster website on that.

00:54:31.140 --> 00:54:33.300
Maybe I will call it one thing that's actually really fun.

00:54:33.300 --> 00:54:35.340
It's called Dagster Open Platform.

00:54:35.340 --> 00:54:39.660
It's a GitHub repo that we launched a couple months ago, I want to say.

00:54:39.660 --> 00:54:40.660
We took our internal...

00:54:40.660 --> 00:54:42.660
I should go back one more.

00:54:42.660 --> 00:54:43.660
Sorry.

00:54:43.660 --> 00:54:45.900
I should go back to GitHub, Dagster Open Platform on GitHub.

00:54:45.900 --> 00:54:47.900
I have it somewhere.

00:54:47.900 --> 00:54:48.900
Yeah.

00:54:48.900 --> 00:54:51.900
It's here under the organization.

00:54:51.900 --> 00:54:54.500
Yes, it should be somewhere here.

00:54:54.500 --> 00:54:55.500
There it is.

00:54:55.500 --> 00:54:57.300
Dagster Open Platform on GitHub.

00:54:57.300 --> 00:54:59.540
And it's really a clone of our production pipelines.

00:54:59.540 --> 00:55:03.700
For the most part, there's some things we've chosen to ignore because they're sensitive.

00:55:03.700 --> 00:55:06.860
But as much as possible, we've defaulted to making it public and open.

00:55:06.860 --> 00:55:10.640
And the whole reason behind this was because, you know, as data engineers, it's often hard

00:55:10.640 --> 00:55:12.900
to see how other data engineers write code.

00:55:12.900 --> 00:55:16.220
We get to see how software engineers write code quite often, but most people don't want

00:55:16.220 --> 00:55:19.460
to share their platforms for various good reasons.

00:55:19.460 --> 00:55:20.460
Right.

00:55:20.460 --> 00:55:23.780
Also, there's like smaller teams or maybe just one person.

00:55:23.780 --> 00:55:29.820
And then like those pipelines are so integrated into your specific infrastructure, right?

00:55:29.820 --> 00:55:32.300
So it's not like, well, here's a web framework to share, right?

00:55:32.300 --> 00:55:36.860
Like, here's how we integrate into that one weird API that we have that no one else has.

00:55:36.860 --> 00:55:39.100
So it's no point in publishing it to you, right?

00:55:39.100 --> 00:55:40.420
That's typically how it goes.

00:55:40.420 --> 00:55:44.380
Or they're so large that they're afraid that there's like some, you know, important information

00:55:44.380 --> 00:55:46.180
that they just don't want to take the risk on.

00:55:46.180 --> 00:55:49.580
And so we built like something that's in the middle where we've taken as much as we can

00:55:49.580 --> 00:55:51.140
and we've publicized it.

00:55:51.140 --> 00:55:52.140
And you can't run this on your own.

00:55:52.140 --> 00:55:53.500
Like it's not, that's not the point.

00:55:53.500 --> 00:55:56.860
The point is to look at the code and see, you know, how does Dagster use Dagster and what

00:55:56.860 --> 00:55:57.860
does that kind of look like?

00:55:57.860 --> 00:55:58.860
Nice.

00:55:58.860 --> 00:55:59.860
Okay.

00:55:59.860 --> 00:56:00.860
All right.

00:56:00.860 --> 00:56:01.860
Well, I'll put a link to that in the show notes and people can check it out.

00:56:01.860 --> 00:56:05.420
Yeah, I guess let's wrap it up with the final call to action.

00:56:05.420 --> 00:56:06.900
People are interested in Dagster.

00:56:06.900 --> 00:56:07.900
How do they get started?

00:56:07.900 --> 00:56:08.900
What do you tell them?

00:56:08.900 --> 00:56:09.900
Oh, yeah.

00:56:09.900 --> 00:56:11.580
Well, Dagster is probably the greatest place to start.

00:56:11.580 --> 00:56:13.700
You can try the cloud product.

00:56:13.700 --> 00:56:18.140
We have free self-serve or you can try the local install as well.

00:56:18.140 --> 00:56:22.660
If you get stuck, a great place to join is our Slack channel, which is up on our website.

00:56:22.660 --> 00:56:27.700
There's even a Ask AI channel where you can just talk to a Slack bot that's been trained

00:56:27.700 --> 00:56:29.500
on all our GitHub issues and discussions.

00:56:29.500 --> 00:56:33.820
And it's surprisingly good at walking you through, you know, any debugging, any issues

00:56:33.820 --> 00:56:34.820
or even advice.

00:56:34.820 --> 00:56:36.380
And that's pretty excellent, actually.

00:56:36.380 --> 00:56:37.380
Yeah.

00:56:37.380 --> 00:56:38.380
It's real fun.

00:56:38.380 --> 00:56:39.380
It's really fun.

00:56:39.380 --> 00:56:41.380
It's a great experience community where you can just chat to us as well.

00:56:41.380 --> 00:56:42.380
Cool.

00:56:42.380 --> 00:56:43.380
All right.

00:56:43.380 --> 00:56:44.380
Well, Pedram, thank you for being on the show.

00:56:44.380 --> 00:56:47.380
Make sure the work on Dagster and sharing it with us.

00:56:47.380 --> 00:56:48.380
Thank you, Michael.

00:56:48.380 --> 00:56:49.380
You bet.

00:56:49.380 --> 00:56:50.380
See you later.

00:56:50.380 --> 00:56:52.380
This has been another episode of Talk Python to Me.

00:56:52.380 --> 00:56:54.260
Thank you to our sponsors.

00:56:54.260 --> 00:56:55.620
Be sure to check out what they're offering.

00:56:55.620 --> 00:56:58.180
It really helps support the show.

00:56:58.180 --> 00:57:01.980
This episode is sponsored by Posit Connect from the makers of Shiny.

00:57:01.980 --> 00:57:06.580
Publish, share, and deploy all of your data projects that you're creating using Python.

00:57:06.580 --> 00:57:13.580
Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Quarto, Reports, Dashboards, and APIs.

00:57:13.580 --> 00:57:15.500
Posit Connect supports all of them.

00:57:15.500 --> 00:57:18.580
Try Posit Connect for free by going to talkpython.fm/posit.

00:57:18.580 --> 00:57:19.580
P-O-S-I-T.

00:57:19.580 --> 00:57:24.100
Want to level up your Python?

00:57:24.100 --> 00:57:28.180
We have one of the largest catalogs of Python video courses over at Talk Python.

00:57:28.180 --> 00:57:33.280
Our content ranges from true beginners to deeply advanced topics like memory and async.

00:57:33.280 --> 00:57:35.980
And best of all, there's not a subscription in sight.

00:57:35.980 --> 00:57:39.100
Check it out for yourself at training.talkpython.fm.

00:57:39.100 --> 00:57:43.720
Be sure to subscribe to the show, open your favorite podcast app, and search for Python.

00:57:43.720 --> 00:57:45.080
We should be right at the top.

00:57:45.080 --> 00:57:50.620
You can also find the iTunes feed at /itunes, the Google Play feed at /play, and the Direct

00:57:50.620 --> 00:57:54.660
RSS feed at /rss on talkpython.fm.

00:57:54.660 --> 00:57:57.220
We're live streaming most of our recordings these days.

00:57:57.220 --> 00:58:00.800
If you want to be part of the show and have your comments featured on the air, be sure

00:58:00.800 --> 00:58:05.740
to subscribe to our YouTube channel at talkpython.fm/youtube.

00:58:05.740 --> 00:58:07.020
This is your host, Michael Kennedy.

00:58:07.020 --> 00:58:08.180
Thanks so much for listening.

00:58:08.180 --> 00:58:09.420
I really appreciate it.

00:58:09.420 --> 00:58:11.140
Now get out there and write some Python code.

00:58:12.140 --> 00:58:15.140
[MUSIC PLAYING]

00:58:15.140 --> 00:58:18.140
[MUSIC ENDS]

00:58:18.140 --> 00:58:21.140
[MUSIC PLAYING]

00:58:21.140 --> 00:58:24.140
[MUSIC ENDS]

00:58:24.140 --> 00:58:27.140
[MUSIC PLAYING]

00:58:27.140 --> 00:58:30.140
[MUSIC ENDS]

00:58:30.140 --> 00:58:35.260
We just recorded it.

