WEBVTT

00:00:00.000 --> 00:00:04.080
We write tests to show us when there are problems with our code, but what if there are intermittent


00:00:04.080 --> 00:00:09.120
problems with the tests themselves? That can be a big hassle. In this episode, we have Gregory


00:00:09.120 --> 00:00:14.880
Kapheimer and Owen Perry on the show to share their research and advice for taming flaky tests.


00:00:14.880 --> 00:00:19.840
This is Talk Python to Me, episode 429, recorded August 10th, 2023.


00:00:19.840 --> 00:00:36.000
Welcome to Talk Python To Me, a weekly podcast on Python.


00:00:36.000 --> 00:00:37.860
This is your host, Michael Kennedy.


00:00:37.860 --> 00:00:42.760
Follow me on Mastodon, where I'm @mkennedy, and follow the podcast using @talkpython,


00:00:42.760 --> 00:00:45.040
both on fosstodon.org.


00:00:45.040 --> 00:00:47.760
Be careful with impersonating accounts on other instances.


00:00:47.760 --> 00:00:48.760
There are many.


00:00:48.760 --> 00:00:54.360
Keep up with the show and listen to over seven years of past episodes at talkpython.fm.


00:00:54.360 --> 00:00:57.800
We've started streaming most of our episodes live on YouTube.


00:00:57.800 --> 00:01:01.400
Subscribe to our YouTube channel over at talkpython.fm/youtube


00:01:01.400 --> 00:01:05.600
to get notified about upcoming shows and be part of that episode.


00:01:05.600 --> 00:01:08.100
This episode is brought to you by JetBrains,


00:01:08.100 --> 00:01:11.700
who encourage you to get work done with PyCharm.


00:01:11.700 --> 00:01:14.500
Download your free trial of PyCharm Professional at


00:01:14.500 --> 00:01:19.020
talkbython.fm/done-with-pycharm.


00:01:19.020 --> 00:01:21.260
And it's brought to you by Sentry.


00:01:21.260 --> 00:01:23.300
Don't let those errors go unnoticed.


00:01:23.300 --> 00:01:24.340
Use Sentry.


00:01:24.340 --> 00:01:27.780
Get started at talkbython.fm/sentry.


00:01:27.780 --> 00:01:30.420
Owen, Gregory, welcome to Talk Python to Me.


00:01:30.420 --> 00:01:31.240
- Hi, Michael.


00:01:31.240 --> 00:01:32.080
It's great to be on the show.


00:01:32.080 --> 00:01:33.500
Thank you for inviting us today.


00:01:33.500 --> 00:01:34.460
- Hi there, thanks for having us.


00:01:34.460 --> 00:01:36.500
- Really great to have you both on the show.


00:01:36.500 --> 00:01:38.740
It's gonna be a bit of a flaky episode though,


00:01:38.740 --> 00:01:39.580
wouldn't you say?


00:01:39.580 --> 00:01:41.300
- It's definitely going to be flaky.


00:01:41.300 --> 00:01:42.240
- Very flaky.


00:01:42.240 --> 00:01:44.340
Looking forward to talking about flaky tests


00:01:44.340 --> 00:01:46.160
And what we can do about them.


00:01:46.160 --> 00:01:51.300
It's one of these realities of writing lots of unit tasks for real world systems, right?


00:01:51.300 --> 00:01:52.800
They end up in weird places.


00:01:52.800 --> 00:01:58.420
So for better, for worse, I've implemented a lot of programs in Python and many of them have test


00:01:58.420 --> 00:02:00.540
suites with flaky test cases inside of them.


00:02:00.540 --> 00:02:02.400
So I have to confess to everyone.


00:02:02.400 --> 00:02:05.940
I myself have written programs with many flaky tests.


00:02:05.940 --> 00:02:07.860
As have I, as have I.


00:02:07.860 --> 00:02:08.300
All right.


00:02:08.300 --> 00:02:13.560
Before we get into the show itself, maybe just a little bit of background on E2.


00:02:13.580 --> 00:02:14.580
Gregory, you want to go first?


00:02:14.580 --> 00:02:17.980
Just a quick introduction about who you are, how you got into programming in Python?


00:02:17.980 --> 00:02:18.980
Sure.


00:02:18.980 --> 00:02:23.620
My name is Gregory Kapammer, and I'm a faculty member in the Department of Computer Science


00:02:23.620 --> 00:02:25.060
at Allegheny College.


00:02:25.060 --> 00:02:29.900
And I've actually been programming in Python since I took an AI course in graduate school


00:02:29.900 --> 00:02:34.980
years ago, and we had to implement all of our algorithms in Python.


00:02:34.980 --> 00:02:38.640
Stopped using Python for a short period of time, and then picked it back up again once


00:02:38.640 --> 00:02:44.320
I learned about pytest because I found it to be such an awesome test automation framework


00:02:44.320 --> 00:02:46.920
and I've been programming in Python regularly since then.


00:02:46.920 --> 00:02:47.920
That's cool.


00:02:47.920 --> 00:02:52.640
What's pretty interesting is people who don't even necessarily do Python sometimes use Python


00:02:52.640 --> 00:02:53.920
to write the tests.


00:02:53.920 --> 00:02:54.920
Yeah, absolutely.


00:02:54.920 --> 00:02:59.800
I have to say I've used a bunch of different test automation frameworks and pytest is by


00:02:59.800 --> 00:03:02.880
far and away my favorite framework out of them all.


00:03:02.880 --> 00:03:03.880
Owen, hello.


00:03:03.880 --> 00:03:07.200
Hi, I'm a PhD student at the University of Sheffield.


00:03:07.200 --> 00:03:10.800
I'm actually coming right to the end of my time as a PhD student.


00:03:10.800 --> 00:03:11.400
So...


00:03:11.400 --> 00:03:12.800
That's quite a journey, isn't it?


00:03:12.800 --> 00:03:14.100
It is quite a journey, yeah.


00:03:14.100 --> 00:03:18.900
And throughout my whole PhD, my main topic has been flaky tests.


00:03:18.900 --> 00:03:19.400
Okay.


00:03:19.400 --> 00:03:26.300
So, before I even started my PhD, I had Python experience from just odd undergraduate projects that had to be done in Python.


00:03:26.300 --> 00:03:33.700
But for all of my research, I've thought it very important to use real software, real tests written by real people


00:03:33.700 --> 00:03:37.000
to identify flaky tests, find out what causes them, that kind of thing.


00:03:37.000 --> 00:03:40.800
And all of my sort of practical work has been done with Python.


00:03:40.800 --> 00:03:46.360
So for example, I've written pytest plugins to help detect flaky tests.


00:03:46.360 --> 00:03:50.580
And as Greg said, I think pytest is a great framework.


00:03:50.580 --> 00:03:51.640
It's very extensible.


00:03:51.640 --> 00:03:54.400
It's very great for writing plugins, very good API.


00:03:54.400 --> 00:03:57.960
And yeah, I've used lots of different types of Python projects


00:03:57.960 --> 00:04:00.600
as subjects in experiments.


00:04:00.600 --> 00:04:03.720
So I've seen quite a wide array of different types of


00:04:03.720 --> 00:04:04.820
software written in Python.


00:04:05.400 --> 00:04:08.680
Have you studied a lot of other people's software, interviewed


00:04:08.680 --> 00:04:12.200
different people to see how they're encountering flaky tests?


00:04:12.200 --> 00:04:17.340
So I've not interviewed anyone exactly, but I did do a questionnaire that I sent


00:04:17.340 --> 00:04:21.640
out on Twitter, LinkedIn, that kind of thing, where we just wanted to get as


00:04:21.640 --> 00:04:25.140
many different kinds of developers talking about flaky tests.


00:04:25.140 --> 00:04:28.340
So we asked them questions like, first of all, what is a flaky test?


00:04:28.340 --> 00:04:30.240
People have slightly different definitions.


00:04:30.240 --> 00:04:33.020
And then I went into, you know, what do you think causes them?


00:04:33.060 --> 00:04:36.540
What impacts do they have on you and your sort of professional workflow?


00:04:36.540 --> 00:04:39.980
And then we've talked a little bit about what people do about them as well.


00:04:39.980 --> 00:04:40.500
Interesting.


00:04:40.500 --> 00:04:45.180
Well, maybe that's a good place to start in either of you just jump in as you see fit.


00:04:45.180 --> 00:04:47.880
So let's just start with, you know, what is a flaky test?


00:04:47.880 --> 00:04:52.140
I mean, we all know that having unit tests and broader tests, you know,


00:04:52.140 --> 00:04:55.980
integration tests and so on for our code is generally a good thing.


00:04:55.980 --> 00:04:59.400
I guess if you write them poorly enough, it's not a good thing, but you know,


00:04:59.400 --> 00:05:04.040
Mostly it's recommended advice and there's a spectrum of how strongly it's recommended.


00:05:04.040 --> 00:05:10.200
Is it extreme programming TDD recommended or is it you have to have some tests to submit a PR


00:05:10.200 --> 00:05:12.600
level of recommended, but we think it's great.


00:05:12.600 --> 00:05:17.880
But there are these negative aspects of having tests as well, right?


00:05:17.880 --> 00:05:21.400
It's easy to sell them as a positive, but they become a maintenance burden.


00:05:21.400 --> 00:05:28.680
They become duplicate code and the flakiness, I think is a particularly challenging part of it.


00:05:28.680 --> 00:05:31.080
Let's start there. What's a flaky test for you all?


00:05:31.080 --> 00:05:35.400
So I'll start off and then Owen, if you'd like to add more details, that would be awesome.


00:05:35.400 --> 00:05:42.600
I would say that a flaky test is a test case that passes or fails in a non-deterministic fashion,


00:05:42.600 --> 00:05:49.320
even when you're not changing the source code of the program or the test suite for the program.


00:05:49.320 --> 00:05:53.320
So this is a situation where the test case may sometimes pass,


00:05:53.960 --> 00:05:57.360
then it may fail and then it may start to pass again,


00:05:57.360 --> 00:05:59.280
even though as a developer,


00:05:59.280 --> 00:06:02.600
you're not making changes to the program under test


00:06:02.600 --> 00:06:05.400
or the source code of the test suite itself.


00:06:05.400 --> 00:06:06.680
- Yeah, that is tricky, right?


00:06:06.680 --> 00:06:09.040
Because there's one day the tests start failing.


00:06:09.040 --> 00:06:10.000
We didn't change anything.


00:06:10.000 --> 00:06:10.960
Nothing has changed here.


00:06:10.960 --> 00:06:11.800
Why?


00:06:11.800 --> 00:06:13.040
How could this have anything to do with it, right?


00:06:13.040 --> 00:06:13.880
- And-- - Owen?


00:06:13.880 --> 00:06:14.700
- Sorry, go ahead, Greg.


00:06:14.700 --> 00:06:16.040
- The only other thing I was going to add


00:06:16.040 --> 00:06:18.800
is that flaky test cases could manifest themselves


00:06:18.800 --> 00:06:20.780
on a developer workstation,


00:06:20.780 --> 00:06:22.520
or they could also manifest


00:06:22.520 --> 00:06:26.000
when you're running them in a continuous integration environment as well.


00:06:26.000 --> 00:06:26.600
Yeah, for sure.


00:06:26.600 --> 00:06:28.800
So to just sort of build on what Greg said there a little bit.


00:06:28.800 --> 00:06:31.720
So one interesting thing we found from that developer survey,


00:06:31.720 --> 00:06:34.680
so the definition that Greg gave just then was pretty much


00:06:34.680 --> 00:06:38.720
the definition we proposed to the respondents to the survey.


00:06:38.720 --> 00:06:40.320
And then we asked them, do you agree?


00:06:40.320 --> 00:06:42.000
If not, what's your definition?


00:06:42.000 --> 00:06:44.440
Most people agreed, but some people said,


00:06:44.440 --> 00:06:46.520
well, it doesn't just depend on the source code


00:06:46.520 --> 00:06:49.920
of the program and the code of the test, but also the execution environment.


00:06:49.920 --> 00:06:54.480
So you can have, like you can take one piece of software and it's associated


00:06:54.480 --> 00:06:56.240
test suite, run it on one computer.


00:06:56.240 --> 00:06:58.200
It passes, run it on another system.


00:06:58.200 --> 00:06:59.400
And then for whatever reason it doesn't.


00:06:59.400 --> 00:07:02.860
So nothing's changed at all except for the execution environment.


00:07:02.860 --> 00:07:06.400
And that's quite prevalent when you're talking about CI, because most of the


00:07:06.400 --> 00:07:11.960
time, these are running on cloud machines, which may not be all exactly the same in


00:07:11.960 --> 00:07:14.320
spec from the perspective of the developer.


00:07:14.320 --> 00:07:18.640
It looks as if a test could just fail, but maybe it failed because it was running on


00:07:18.640 --> 00:07:20.400
a slightly different machine that time it was run.


00:07:20.400 --> 00:07:21.280
>> I never really thought of that.


00:07:21.280 --> 00:07:23.040
Obviously, the different environments,


00:07:23.040 --> 00:07:26.680
a CI machine is very different than my MacBook.


00:07:26.680 --> 00:07:31.400
Clearly, it could be the case that the test passes on my machine,


00:07:31.400 --> 00:07:32.960
not CI or vice versa.


00:07:32.960 --> 00:07:34.520
But I hadn't really thought about,


00:07:34.520 --> 00:07:37.760
well, this time it got an AMD Cloud processor,


00:07:37.760 --> 00:07:39.400
and it was already under heavy load,


00:07:39.400 --> 00:07:42.560
and so the timing changed versus the other time it was on


00:07:42.560 --> 00:07:46.640
a premium Intel thing in the Cloud that had no other thing going on,


00:07:46.640 --> 00:07:48.560
so it behaved differently. It's pretty wild.


00:07:48.560 --> 00:07:50.040
- Your point is a good one, Michael.


00:07:50.040 --> 00:07:53.920
It's actually often the case that the speed of the CPU


00:07:53.920 --> 00:07:56.800
or the amount of memory or the amount of disk


00:07:56.800 --> 00:08:00.040
on the testing workstation can make a big difference


00:08:00.040 --> 00:08:02.520
when it comes to manifesting a flaky tests.


00:08:02.520 --> 00:08:03.960
- Yeah, I guess that makes a lot of sense,


00:08:03.960 --> 00:08:05.600
especially if it's a race condition, right?


00:08:05.600 --> 00:08:07.320
If you're having some sort of parallelism,


00:08:07.320 --> 00:08:10.240
then that could really come into memory as well, maybe.


00:08:10.240 --> 00:08:11.280
Maybe you run out of memory,


00:08:11.280 --> 00:08:12.800
you get an out of memory exception.


00:08:12.800 --> 00:08:15.140
So when we're talking about flaky tests,


00:08:15.140 --> 00:08:17.220
one thing that came to mind for me,


00:08:17.220 --> 00:08:19.180
And I want to bring it up at the start here,


00:08:19.180 --> 00:08:22.100
'cause I'm wondering if this classifies as flaky for you,


00:08:22.100 --> 00:08:25.420
if this is some other kind of not great test.


00:08:25.420 --> 00:08:28.100
It has to do with science, right?


00:08:28.100 --> 00:08:31.460
So scientific type of computing, mathematical stuff.


00:08:31.460 --> 00:08:33.060
Obviously you shouldn't say,


00:08:33.060 --> 00:08:35.420
I've got some floating point number, equal, equal,


00:08:35.420 --> 00:08:38.140
you know, some other long, precise, you know,


00:08:38.140 --> 00:08:40.060
here's my definition of the square root of two


00:08:40.060 --> 00:08:42.020
as a float equal, equal that, right?


00:08:42.020 --> 00:08:43.360
That might be too precise.


00:08:43.360 --> 00:08:45.160
But what I'm thinking about is,


00:08:45.160 --> 00:08:47.020
if you make the smallest amount of change


00:08:47.020 --> 00:08:49.820
to some algorithm or the way something works


00:08:49.820 --> 00:08:52.260
could change some, like maybe you're trying to say,


00:08:52.260 --> 00:08:55.020
does this, do we get a curve that looks like this?


00:08:55.020 --> 00:08:58.740
Or do we match some kind of criteria on, you know,


00:08:58.740 --> 00:08:59.580
statistics?


00:08:59.580 --> 00:09:01.460
It could change just a little bit,


00:09:01.460 --> 00:09:04.740
but the way that you're testing for it to be a match,


00:09:04.740 --> 00:09:06.700
it changes enough in that regard,


00:09:06.700 --> 00:09:09.660
even though effectively it kind of means the same thing.


00:09:09.660 --> 00:09:10.540
You know what I'm asking?


00:09:10.540 --> 00:09:12.220
Does that count as a flaky test for you?


00:09:12.220 --> 00:09:15.620
- So what you're talking about is a very specific category


00:09:15.620 --> 00:09:20.540
flaky test. So I would call that a flaky test. So yeah, so when you're dealing with programs,


00:09:20.540 --> 00:09:24.260
like for example, various machine learning packages that are very common in Bison, you'll


00:09:24.260 --> 00:09:29.340
see a lot of test cases that will say, assert X is equal to this within a certain tolerance


00:09:29.340 --> 00:09:34.240
range or something, or is approximately equal to something. With these kinds of tests, they


00:09:34.240 --> 00:09:40.160
are kind of inherently flaky. There is a trade-off between how strong you want the test to be.


00:09:40.160 --> 00:09:46.160
i.e. how narrow you want that acceptable band to be versus how flaky you want it to be.


00:09:46.160 --> 00:09:50.120
Sorry, how flaky you don't want it to be. So the stronger you make the test, the more


00:09:50.120 --> 00:09:53.920
flaky it's likely to be because that band is narrower. But if you just increase that


00:09:53.920 --> 00:09:58.160
tolerance then, yeah, it won't be as flaky anymore, but then maybe it won't catch as


00:09:58.160 --> 00:10:01.320
many bugs anymore because it's too relaxed of a test.


00:10:01.320 --> 00:10:02.320
Sure.


00:10:02.320 --> 00:10:06.000
And unfortunately, if you're going to reduce a test case to either a pass or a fail, which


00:10:06.000 --> 00:10:07.700
which is something that we have to do.


00:10:07.700 --> 00:10:09.080
There's no real way around that.


00:10:09.080 --> 00:10:10.860
There is, there has been done work


00:10:10.860 --> 00:10:12.860
where people have sort of tried to calculate,


00:10:12.860 --> 00:10:15.120
sorry, via, there we go.


00:10:15.120 --> 00:10:16.500
It's on a motion sensor.


00:10:16.500 --> 00:10:19.540
- Got away with the light, it's going dark for you.


00:10:19.540 --> 00:10:20.860
- Well, I was like, yeah, so there's a,


00:10:20.860 --> 00:10:22.260
there is work people have done


00:10:22.260 --> 00:10:23.900
where they try to calculate sort of


00:10:23.900 --> 00:10:27.920
what is the best sort of tolerance threshold in test cases,


00:10:27.920 --> 00:10:29.740
but there's no kind of silver bullet solution


00:10:29.740 --> 00:10:31.380
for that kind of flakiness.


00:10:31.380 --> 00:10:32.700
- Yeah, it's tricky, isn't it?


00:10:32.700 --> 00:10:33.540
- Yeah.


00:10:33.540 --> 00:10:35.740
- Yeah, Arvon out in the audience has an interesting one.


00:10:35.740 --> 00:10:40.240
maybe probably lands a little more into the realm directly of the kind of stuff that you're talking about.


00:10:40.240 --> 00:10:46.740
It says, "At one of my jobs, we were testing chained celery tasks that become flaky sometimes.


00:10:46.740 --> 00:10:51.740
Since one celery task fails for some reason, the chain task could fail as well."


00:10:51.740 --> 00:10:55.240
Those kind of external systems are probably at the heart of a lot of this, right?


00:10:55.240 --> 00:10:57.740
Yeah, I think it is often at the heart of it.


00:10:57.740 --> 00:11:02.240
And whether you're using celery or some other work processing queue,


00:11:02.240 --> 00:11:08.000
Or alternatively, if you're interacting with a document database or a relational database,


00:11:08.000 --> 00:11:13.520
in all of those occasions, when you interact with some storage or processing system in your


00:11:13.520 --> 00:11:19.040
environment, you may not have control over that part of your environment. And then once again,


00:11:19.040 --> 00:11:23.280
that's another way in which flakiness can creep into the testing process.


00:11:23.280 --> 00:11:27.920
Right. And you maybe don't care about how the Celery server is doing, but at the same time,


00:11:27.920 --> 00:11:31.760
maybe you haven't mocked that part out. You need to somehow, are you doing an end-to-end


00:11:31.760 --> 00:11:35.440
test or something, and it becomes part of the reliability of your system.


00:11:35.440 --> 00:11:38.400
Even though maybe that's like a Q and A celery server, not


00:11:38.400 --> 00:11:39.680
the production server, right?


00:11:39.680 --> 00:11:40.100
Yeah.


00:11:40.100 --> 00:11:43.900
And in fact, you brought up another really good point when it comes to mocking.


00:11:43.900 --> 00:11:48.280
There are many circumstances in which I've had to use one of the various


00:11:48.280 --> 00:11:52.880
mocking features that Python provides in order to stand up my own


00:11:52.880 --> 00:11:54.920
version of an external service.


00:11:54.920 --> 00:12:00.760
However, that is a trade-off like Owen mentioned previously, because now my


00:12:00.760 --> 00:12:07.660
test case may be less flaky and yet it's also less realistic and so therefore may not be


00:12:07.660 --> 00:12:13.660
able to catch certain types of bugs. So now we've seen another example of the trade-off


00:12:13.660 --> 00:12:20.180
associated with making a test less flaky but perhaps also making it less realistic.


00:12:20.180 --> 00:12:26.160
Yeah, I'm starting to get a sense that there's probably a precision versus stability trade-off


00:12:26.160 --> 00:12:27.940
that's always at play here.


00:12:27.940 --> 00:12:30.820
- Yeah, and obviously in the extreme end of the spectrum,


00:12:30.820 --> 00:12:35.180
you can make any test non-flaky by just deleting it, right?


00:12:35.180 --> 00:12:36.460
- Put an ignore attribute on it.


00:12:36.460 --> 00:12:37.280
- Exactly.


00:12:37.280 --> 00:12:39.780
So you've got to be careful that you're not optimizing


00:12:39.780 --> 00:12:41.460
your tests just for passing,


00:12:41.460 --> 00:12:44.100
which if you're trying to get a PR through,


00:12:44.100 --> 00:12:47.540
then that is a trap you might fall into.


00:12:47.540 --> 00:12:48.360
- Yeah, absolutely.


00:12:48.360 --> 00:12:50.300
So you talked about that survey a little bit before.


00:12:50.300 --> 00:12:52.020
Maybe you want to talk about some more of the things


00:12:52.020 --> 00:12:52.840
you learned from there,


00:12:52.840 --> 00:12:54.940
like what were some of the responses there?


00:12:54.940 --> 00:12:57.460
- So we got some really interesting ones, actually.


00:12:57.460 --> 00:13:00.720
If you want to find the paper yourself, or if anyone's listening wants to find the paper,


00:13:00.720 --> 00:13:05.440
if you just Google "surveying the developer experience of flaky tests," you should be


00:13:05.440 --> 00:13:06.800
able to find it.


00:13:06.800 --> 00:13:11.960
As I said, we also asked developers what they thought were the most common causes of flaky


00:13:11.960 --> 00:13:12.960
tests.


00:13:12.960 --> 00:13:17.160
And the cause that got the most votes was setup and teardown.


00:13:17.160 --> 00:13:23.160
So what I mean by that is flakiness being caused by a test case that either doesn't


00:13:23.160 --> 00:13:25.600
fully set up its executing environment,


00:13:25.600 --> 00:13:28.240
alternatively doesn't clean up after itself.


00:13:28.240 --> 00:13:30.000
And if it doesn't clean up after itself correctly,


00:13:30.000 --> 00:13:32.320
it could leave some global state behind


00:13:32.320 --> 00:13:34.440
that could then impact a later test case


00:13:34.440 --> 00:13:36.000
that's executed, if that makes sense.


00:13:36.000 --> 00:13:36.940
- Yeah, absolutely.


00:13:36.940 --> 00:13:40.240
The cleanup one is especially tricky, right?


00:13:40.240 --> 00:13:42.160
We kind of know about setup because you're like,


00:13:42.160 --> 00:13:45.300
oh, well, we have to do this in order for this file


00:13:45.300 --> 00:13:47.440
to exist or whatever.


00:13:47.440 --> 00:13:49.840
Teardown part, that becomes really tricky


00:13:49.840 --> 00:13:52.280
because it could have knock-on effects for tests


00:13:52.280 --> 00:14:01.280
that either pass and they shouldn't or don't pass because it didn't start fresh, right?


00:14:01.280 --> 00:14:05.640
This portion of Talk Python to Me is brought to you by JetBrains and PyCharm.


00:14:05.640 --> 00:14:09.600
Are you a data scientist or a web developer looking to take your projects to the next


00:14:09.600 --> 00:14:10.600
level?


00:14:10.600 --> 00:14:13.080
Well, I have the perfect tool for you, PyCharm.


00:14:13.080 --> 00:14:17.480
PyCharm is a powerful integrated development environment that empowers developers and data


00:14:17.480 --> 00:14:23.960
scientists like us to write clean and efficient code with ease. Whether you're analyzing complex


00:14:23.960 --> 00:14:29.400
data sets or building dynamic web applications, PyCharm has got you covered. With its intuitive


00:14:29.400 --> 00:14:34.280
interface and robust features, you can boost your productivity and bring your ideas to life faster


00:14:34.280 --> 00:14:39.240
than ever before. For data scientists, PyCharm offers seamless integration with popular libraries


00:14:39.240 --> 00:14:45.240
like NumPy, Pandas, and Matplotlib. You can explore, visualize, and manipulate data effortlessly,


00:14:45.240 --> 00:14:49.120
unlocking valuable insights with just a few lines of code.


00:14:49.120 --> 00:14:53.680
And for us web developers, PyCharm provides a rich set of tools to streamline your workflow.


00:14:53.680 --> 00:14:57.120
From intelligent code completion to advanced debugging capabilities,


00:14:57.120 --> 00:15:02.920
PyCharm helps you write clean, scalable code that powers stunning web applications.


00:15:02.920 --> 00:15:07.920
Plus, PyCharm's support for popular frameworks like Django, FastAPI, and React


00:15:07.920 --> 00:15:11.120
make it a breeze to build and deploy your web projects.


00:15:11.120 --> 00:15:16.080
It's time to say goodbye to tedious configuration and hello to rapid development.


00:15:16.080 --> 00:15:17.560
But wait, there's more!


00:15:17.560 --> 00:15:22.520
With PyCharm, you get even more advanced features like remote development, database integration,


00:15:22.520 --> 00:15:26.320
and version control, ensuring your projects stay organized and secure.


00:15:26.320 --> 00:15:29.920
So whether you're diving into data science or shaping the future of the web, PyCharm


00:15:29.920 --> 00:15:31.440
is your go-to tool.


00:15:31.440 --> 00:15:33.520
Join me and try PyCharm today.


00:15:33.520 --> 00:15:40.700
Just visit talkpython.fm/done-with-pycharm, links in your show notes, and experience the


00:15:40.700 --> 00:15:46.700
power of PyCharm first hand for three months free. PyCharm, it's how I get work done.


00:15:46.700 --> 00:15:52.940
Well, this kind of leads us into a whole other type of flaky test called a test order dependent


00:15:52.940 --> 00:15:57.820
test. When you have a test case that doesn't clean up after itself properly, then that can


00:15:57.820 --> 00:16:02.620
potentially mean that later tests that perhaps are targeting similar parts of the program where


00:16:02.620 --> 00:16:07.820
there might be some state involved, they might fail when they should pass or alternatively pass


00:16:07.820 --> 00:16:12.060
when they should fail just because the assumptions that were there when the developer wrote that test


00:16:12.060 --> 00:16:16.780
aren't being met anymore because something's changed by another test. So what that means is


00:16:16.780 --> 00:16:21.980
that if you just take an arbitrary test suite, randomize the order, shuffle it, for any large


00:16:21.980 --> 00:16:25.420
test suite I can almost guarantee that some tests are going to fail and all you've done is change


00:16:25.420 --> 00:16:29.980
the order and they're failing because somewhere a test isn't cleaning up after itself properly.


00:16:29.980 --> 00:16:35.580
Yeah or cleaning up can mean different things right? Cleaning up can mean we didn't change,


00:16:35.580 --> 00:16:40.380
we didn't take away that file we created or put back the file we deleted as part


00:16:40.380 --> 00:16:41.880
of this test scenario we're working with.


00:16:41.880 --> 00:16:46.080
But it could also be we're testing by talking to a database and we made an


00:16:46.080 --> 00:16:47.880
insert to it and didn't roll that back.


00:16:47.880 --> 00:16:51.440
Or maybe the most subtle, yes, there's two more levels here.


00:16:51.440 --> 00:16:54.360
One, you could have changed in memory state, right?


00:16:54.360 --> 00:16:56.720
You could have like, there's a shared variable, which is


00:16:56.720 --> 00:16:58.400
probably the most common reason.


00:16:58.400 --> 00:17:01.800
Like some shared state of the process just isn't in its


00:17:01.800 --> 00:17:03.700
starting or expected position.


00:17:03.900 --> 00:17:07.180
But the fourth one, I said, oh, there's three, but actually I think there's more,


00:17:07.180 --> 00:17:09.180
is you mocked out something.


00:17:09.180 --> 00:17:12.140
Like, I've mocked out what datetime.now means.


00:17:12.140 --> 00:17:13.500
I forgot to put it back.


00:17:13.500 --> 00:17:14.620
So time has stopped.


00:17:14.620 --> 00:17:15.900
Or something like that, right?


00:17:15.900 --> 00:17:21.580
Yeah, those are all really good examples of the flakiness that can appear when you have shared state.


00:17:21.580 --> 00:17:25.180
And building on what both you and Owen just said,


00:17:25.180 --> 00:17:27.820
again, I think there's another trade-off here.


00:17:27.820 --> 00:17:33.180
One of the trade-offs is connected to the efficiency of the testing process.


00:17:33.180 --> 00:17:35.680
is the flakiness of the testing process.


00:17:35.680 --> 00:17:40.180
So if you do a really good job at clearing out state from your database


00:17:40.180 --> 00:17:43.380
or resetting state in the memory of your process,


00:17:43.380 --> 00:17:45.480
that may take a longer time,


00:17:45.480 --> 00:17:49.680
but potentially reduce the amount of flakiness that manifests in your tests.


00:17:49.680 --> 00:17:52.180
And then additionally, it's worth noting that


00:17:52.180 --> 00:17:55.780
when you have test suites with really good setup


00:17:55.780 --> 00:17:59.180
and really good teardown and cleaning mechanisms,


00:17:59.180 --> 00:18:03.180
Those are also more time consuming for us to write as developers,


00:18:03.180 --> 00:18:08.140
which may mean we're spending a lot of time investing in our test suite and perhaps slightly


00:18:08.140 --> 00:18:13.580
less time actually adding new features to our program. And so there's trade-offs both in terms


00:18:13.580 --> 00:18:17.660
of developer productivity and the efficiency of the testing process.


00:18:17.660 --> 00:18:22.620
Those both matter. Which one matters more to you probably depends on your situation, right?


00:18:22.620 --> 00:18:27.340
If you're a small team and you need to move quick, the developer overhead is probably a


00:18:27.340 --> 00:18:32.940
a serious hassle. But if you're a large team and you have a hundred thousand tests, you


00:18:32.940 --> 00:18:38.580
want to get answers today, not tomorrow from your test suite, the speed of execution probably


00:18:38.580 --> 00:18:39.740
matters more at that point.


00:18:39.740 --> 00:18:44.280
Yeah, I think that's absolutely the case. And so there have been some situations where


00:18:44.280 --> 00:18:49.780
I have certain test cases that take a really long time to run. And so in pytest, I might


00:18:49.780 --> 00:18:56.260
set a marker and only run those test cases at certain points of time during development


00:18:56.260 --> 00:19:00.980
on my laptop and then always run them inside a CI.


00:19:00.980 --> 00:19:04.980
And the nice thing about removing those long running test cases is that it can make the


00:19:04.980 --> 00:19:11.180
testing process faster and I don't have to do my rigorous cleaning approach except when


00:19:11.180 --> 00:19:12.620
I am running them in CI.


00:19:12.620 --> 00:19:13.980
Yeah, that's an interesting idea.


00:19:13.980 --> 00:19:17.820
Maybe giving them tags and then coming up with a category of speed.


00:19:17.820 --> 00:19:22.500
I mean, I know I've heard of people doing like marking a test as slow, a set of tests


00:19:22.500 --> 00:19:23.780
as slow.


00:19:23.780 --> 00:19:25.460
Maybe that's not fine grained enough.


00:19:25.460 --> 00:19:29.260
been doing something like fast, less than a second, less than five seconds, less


00:19:29.260 --> 00:19:29.980
than 10 seconds.


00:19:29.980 --> 00:19:33.500
So I'm willing to run all the ones that run in three seconds or less, but not


00:19:33.500 --> 00:19:34.140
more than that.


00:19:34.140 --> 00:19:34.660
Right.


00:19:34.660 --> 00:19:37.420
So you could kind of scale it up more than just fast and slow.


00:19:37.420 --> 00:19:42.140
So on the topic of markers, there's several plugins for pytest that enable


00:19:42.140 --> 00:19:43.660
you to mark a test as flaky.


00:19:43.660 --> 00:19:44.260
Okay.


00:19:44.260 --> 00:19:49.340
Basically what that then means is that if it fails, it'll retry it some number of


00:19:49.340 --> 00:19:49.860
times.


00:19:49.860 --> 00:19:53.780
And then if it passes at least once, it will call the whole thing a pass.


00:19:53.860 --> 00:19:59.140
So while that means, yeah, you can just make your test suite pass, it's like, so for example,


00:19:59.140 --> 00:20:04.260
so in the, in the survey, we had one respondent tell us sometimes a flaky test, always a bad


00:20:04.260 --> 00:20:09.620
thing because sometimes the fact that a test is non-deterministic is showing that part


00:20:09.620 --> 00:20:12.500
of the software is non-deterministic when it shouldn't be.


00:20:12.500 --> 00:20:17.060
So if you were to follow this methodology of just rerunning all your flaky tests and


00:20:17.060 --> 00:20:20.900
ignoring them, if they pass at least once, then you'd miss out on that because you wouldn't


00:20:20.900 --> 00:20:22.500
be notified that that test failed.


00:20:22.500 --> 00:20:23.500
Yeah.


00:20:23.500 --> 00:20:26.780
Maybe it's highlighting a weakness in your infrastructure,


00:20:26.780 --> 00:20:28.460
your DevOps story.


00:20:28.460 --> 00:20:30.660
You could say, "Well, that's out of my control.


00:20:30.660 --> 00:20:31.380
It's not my problem."


00:20:31.380 --> 00:20:33.220
Or you could say, "Actually, folks,


00:20:33.220 --> 00:20:35.380
look, this is pointing out this is


00:20:35.380 --> 00:20:39.660
the least stable pillar of our uptime for our app."


00:20:39.660 --> 00:20:40.860
>> Yeah, that's a good point.


00:20:40.860 --> 00:20:43.340
The other thing since we're talking about randomness,


00:20:43.340 --> 00:20:45.020
that's important to discuss,


00:20:45.020 --> 00:20:47.900
is the use of property-based testing tools.


00:20:47.900 --> 00:20:51.180
For example, I use hypothesis in order


00:20:51.180 --> 00:20:54.740
to automatically generate inputs


00:20:54.740 --> 00:20:57.900
and then send them into my function under test.


00:20:57.900 --> 00:21:02.360
And there may be cases where hypothesis reveals a bug


00:21:02.360 --> 00:21:05.960
and that could in fact actually be a bug in my program


00:21:05.960 --> 00:21:08.860
even though I've run exactly that same test case


00:21:08.860 --> 00:21:10.500
frequently in the past.


00:21:10.500 --> 00:21:14.220
And it just happens to be the case that in that run


00:21:14.220 --> 00:21:17.660
when I was using a hypothesis property-based test,


00:21:17.660 --> 00:21:20.740
it was able to find a potential problem.


00:21:20.740 --> 00:21:25.140
So in that situation, even though that test didn't fail the last three times,


00:21:25.140 --> 00:21:30.340
this could still be a silver lining to suggest that there is a problem with my


00:21:30.340 --> 00:21:35.460
program and I need to resolve it because hypothesis has randomly generated an


00:21:35.460 --> 00:21:37.680
input that I haven't seen previously.


00:21:37.680 --> 00:21:37.860
Yeah.


00:21:37.860 --> 00:21:39.500
The hypothesis story is interesting.


00:21:39.500 --> 00:21:41.100
I was thinking about that as well.


00:21:41.100 --> 00:21:45.080
After reading some of the work that you're doing here, thinking things like


00:21:45.080 --> 00:21:49.440
hypothesis and parameterized testing and those sorts of things where they just,


00:21:49.560 --> 00:21:51.720
they naturally take some test scenario


00:21:51.720 --> 00:21:54.120
and run it over and over with a bunch of different inputs.


00:21:54.120 --> 00:21:57.800
Probably uncovers this better than one-off tests, I imagine.


00:21:57.800 --> 00:21:59.720
- And Hypothesis also has a mode


00:21:59.720 --> 00:22:03.560
that lets you run a long running fuzz testing campaign.


00:22:03.560 --> 00:22:04.760
And in those situations,


00:22:04.760 --> 00:22:06.720
it doesn't constrain its execution


00:22:06.720 --> 00:22:08.840
by a specific period of time.


00:22:08.840 --> 00:22:11.840
And I have found that when I let a fuzzing campaign go


00:22:11.840 --> 00:22:13.280
for a given function,


00:22:13.280 --> 00:22:16.480
maybe I've described its inputs with a JSON schema


00:22:16.480 --> 00:22:20.100
and I'm using the hypothesis JSON schema plugin,


00:22:20.100 --> 00:22:23.120
I might not find a bug for a long period of time.


00:22:23.120 --> 00:22:25.320
And then suddenly a bug will crop up.


00:22:25.320 --> 00:22:28.020
And I often use that as an opportunity


00:22:28.020 --> 00:22:29.860
to rethink my assumptions


00:22:29.860 --> 00:22:31.400
about the function that I'm testing.


00:22:31.400 --> 00:22:33.300
- So you said fuzzing.


00:22:33.300 --> 00:22:35.120
Tell people out there, give people a definition


00:22:35.120 --> 00:22:36.520
that they're familiar with fuzzing.


00:22:36.520 --> 00:22:39.700
- So when I think of fuzzing as a process


00:22:39.700 --> 00:22:43.040
where you're randomly generating inputs


00:22:43.040 --> 00:22:46.160
that you're going to send to the function under test


00:22:46.160 --> 00:22:57.400
And you're frequently doing that in what I would call a campaign, which means that the input generation process is going to range perhaps for an extended period of time.


00:22:57.400 --> 00:23:09.600
And you may have different goals during that fuzzing campaign, like covering more of the program under test or attempting to realize as many crash inducing inputs as is possible.


00:23:09.680 --> 00:23:11.320
- Yeah, that's such a cool idea.


00:23:11.320 --> 00:23:13.960
And that's sort of how a hypothesis works.


00:23:13.960 --> 00:23:16.460
Although I don't know, it's really meant for buzzing


00:23:16.460 --> 00:23:18.420
in the sense of just we're gonna hit it


00:23:18.420 --> 00:23:21.320
with a whole bunch of stuff at extreme scale


00:23:21.320 --> 00:23:23.220
until it breaks, but it certainly is meant


00:23:23.220 --> 00:23:25.400
to run it a lot of times with different inputs.


00:23:25.400 --> 00:23:29.160
So sort of the effect, if not necessarily the intent of it.


00:23:29.160 --> 00:23:31.320
Here's another one from the audience


00:23:31.320 --> 00:23:33.680
that also is one that I hadn't thought about from Marwan.


00:23:33.680 --> 00:23:36.320
It says, "Sometimes flakiness shows up as conflicts


00:23:36.320 --> 00:23:38.200
"to access a shared resource


00:23:38.200 --> 00:23:41.800
when more than one CI pipeline is running for same code.


00:23:41.800 --> 00:23:43.700
That's pretty wild. I hadn't really thought about that, right?


00:23:43.700 --> 00:23:45.400
But you have a lock on something.


00:23:45.400 --> 00:23:49.600
Resource availability in general is quite a common cause of flakiness.


00:23:49.600 --> 00:23:53.300
So that might be, so this resource could be a file system


00:23:53.300 --> 00:23:55.400
or a database or anything really.


00:23:55.400 --> 00:23:58.400
Or even something, even if we're not talking about CI,


00:23:58.400 --> 00:24:00.300
just on a local machine, you've got, like you said,


00:24:00.300 --> 00:24:02.900
locks and things that aren't supposed to be shared


00:24:02.900 --> 00:24:04.800
between multiple processes or whatever.


00:24:04.800 --> 00:24:07.000
So yeah, that is a relatively common cause


00:24:07.000 --> 00:24:09.700
that I've seen in the programs that I've tested.


00:24:09.700 --> 00:24:14.900
Yeah, the thing about that that stands out to me is you might have an assumption that only one process


00:24:14.900 --> 00:24:18.500
of your app is ever going to be running on a server at a time.


00:24:18.500 --> 00:24:22.500
And yet somehow, because, you know, there were multiple Git commits


00:24:22.500 --> 00:24:25.600
that you weren't even aware of, now they're running kind of in parallel.


00:24:25.600 --> 00:24:29.100
So you're in this situation that, you know, you never saw coming


00:24:29.100 --> 00:24:31.300
because you just don't run your app that way.


00:24:31.300 --> 00:24:32.000
You know what I mean?


00:24:32.000 --> 00:24:32.400
Yeah.


00:24:32.400 --> 00:24:38.080
This is actually a really good example of when a flaky test is again a silver lining


00:24:38.080 --> 00:24:44.320
because it forces you to question your assumptions about how your program will run and when it


00:24:44.320 --> 00:24:49.040
will run and how much of resources your program is going to consume.


00:24:49.040 --> 00:24:54.200
So in the situation when I never thought about my program running in multiple instances at


00:24:54.200 --> 00:25:00.600
the same time, if my tests become flaky, that may actually open up a whole new opportunity


00:25:00.600 --> 00:25:03.000
for me to refactor and improve my program.


00:25:03.000 --> 00:25:03.840
- Yeah, that's right.


00:25:03.840 --> 00:25:04.800
You're like, wait a minute,


00:25:04.800 --> 00:25:06.340
I didn't realize this was a problem,


00:25:06.340 --> 00:25:08.600
but yes, maybe it is.


00:25:08.600 --> 00:25:09.440
Let's see.


00:25:09.440 --> 00:25:12.640
So let's talk a little bit about some articles


00:25:12.640 --> 00:25:16.260
that a couple of the big tech companies wrote here.


00:25:16.260 --> 00:25:21.260
So both Google and Spotify talked about how they're using,


00:25:21.260 --> 00:25:23.360
how they're experiencing flaky tests


00:25:23.360 --> 00:25:26.880
and what they're doing to either reduce them


00:25:26.880 --> 00:25:28.720
or maybe as you put it, Gregory,


00:25:28.720 --> 00:25:31.520
some of the silver linings that they're finding in it


00:25:31.520 --> 00:25:32.980
and some of the tools that are building


00:25:32.980 --> 00:25:33.860
to help deal with that.


00:25:33.860 --> 00:25:35.680
So over at Google, they say they're running


00:25:35.680 --> 00:25:38.280
obviously a large set of tests.


00:25:38.280 --> 00:25:41.060
You could probably, like that is a massive understatement


00:25:41.060 --> 00:25:44.560
I imagine, but it says they see a continual rate


00:25:44.560 --> 00:25:48.860
of flakiness of about 1.5% on all test cases,


00:25:48.860 --> 00:25:53.020
which for them, I imagine is a lot of tests.


00:25:53.020 --> 00:25:56.360
And so you wanna talk a little bit about this,


00:25:56.360 --> 00:26:00.960
this either you guys and maybe some of the mitigation strategies they have.


00:26:00.960 --> 00:26:06.160
So from a developer perspective, so this article and others as well point out an interesting


00:26:06.160 --> 00:26:09.640
side of flakiness that when you're talking from a purely technical perspective, you don't


00:26:09.640 --> 00:26:10.800
really consider.


00:26:10.800 --> 00:26:13.920
And that's the kind of the sort of the psychological impact of them.


00:26:13.920 --> 00:26:16.360
So it's a little bit like the boy who cried wolf.


00:26:16.360 --> 00:26:21.200
So if you have a test case that's known to be flaky, you might be tempted to just put


00:26:21.200 --> 00:26:26.120
some marker on it that says ignore it or it's an expected fail or whatever.


00:26:26.120 --> 00:26:31.160
But then suppose it fails for real sometime and you're ignoring it or you have it quarantined


00:26:31.160 --> 00:26:35.080
or something, then that means you're missing out on real bugs.


00:26:35.080 --> 00:26:40.240
So as well as just being a hindrance to CI and that kind of thing, it could almost make


00:26:40.240 --> 00:26:45.260
a developer team lose the discipline to properly investigate every test failure.


00:26:45.260 --> 00:26:49.280
If they've got a test suite that's known to be full of flaky tests, then naturally you're


00:26:49.280 --> 00:26:50.600
not going to trust it as much.


00:26:50.600 --> 00:26:51.600
Yeah.


00:26:51.600 --> 00:26:53.840
So that's probably one of the biggest problems that flaky tests cause in my opinion.


00:26:53.840 --> 00:26:57.960
I think the mental aspect of it, the how much do I trust it?


00:26:57.960 --> 00:27:00.600
Do I have faith in our test suite?


00:27:00.600 --> 00:27:04.480
Do I have faith in the continuous deployment capabilities


00:27:04.480 --> 00:27:06.480
of our pipelines and things like that?


00:27:06.480 --> 00:27:08.720
That's, I think that's pretty serious.


00:27:08.720 --> 00:27:10.760
There's already a bit of a challenge, I think,


00:27:10.760 --> 00:27:13.160
on teams to have complete buy-in


00:27:13.160 --> 00:27:16.480
on making sure the software is self-evaluating.


00:27:16.480 --> 00:27:18.800
You know, like some people will check in code


00:27:18.800 --> 00:27:20.880
that breaks the build, but they're kind of like,


00:27:20.880 --> 00:27:23.640
YOLO, whatever, other people really, you know,


00:27:23.640 --> 00:27:25.260
Somehow they really want the build to work.


00:27:25.260 --> 00:27:28.720
So it's their job to kind of chase that person down and make them fix it.


00:27:28.720 --> 00:27:32.560
And it's always kind of a bit of a struggle, but that's when the tests are awesome.


00:27:32.560 --> 00:27:33.260
Right.


00:27:33.260 --> 00:27:37.320
It just changes to the code makes it sort of adds this, these breaking builds.


00:27:37.320 --> 00:27:42.240
But if the code is flaky, all of a sudden you can start to see CI as an


00:27:42.240 --> 00:27:44.040
annoyance because it tells you something's wrong.


00:27:44.040 --> 00:27:45.180
You're like, I know nothing's wrong.


00:27:45.180 --> 00:27:46.400
It's just, it'll go away.


00:27:46.400 --> 00:27:51.200
So maybe speak to the psychological bit of like how flakiness can maybe


00:27:51.200 --> 00:27:53.680
degrade people's caring about tests at all.


00:27:53.680 --> 00:28:00.540
I would say that overall, developers have the risk of losing a confidence in two types


00:28:00.540 --> 00:28:01.900
of correctness.


00:28:01.900 --> 00:28:07.880
First of all, flaky test cases may cause developers to begin to mistrust and lose confidence in


00:28:07.880 --> 00:28:09.120
the test suite.


00:28:09.120 --> 00:28:14.680
Then they also may lose confidence in the overall correctness of their program, and


00:28:14.680 --> 00:28:20.440
then may cause them to stop running test cases, which then reduces test quality and maybe


00:28:20.440 --> 00:28:24.740
even also reduces the quality of the program under test as well.


00:28:24.740 --> 00:28:31.940
So I think regrettably, it's a negative reinforcing cycle where you start to mistrust your tests


00:28:31.940 --> 00:28:33.200
so you don't run them.


00:28:33.200 --> 00:28:37.240
But then you start to lose confidence in the correctness of your program.


00:28:37.240 --> 00:28:42.340
And now you're not sure what to do because tests are failing for spurious reasons.


00:28:42.340 --> 00:28:46.920
You disable them, but then as Owen mentioned previously, you lose the opportunity to get


00:28:46.920 --> 00:28:48.480
the feedback from those tests.


00:28:48.480 --> 00:28:49.940
It goes both ways, right?


00:28:49.940 --> 00:28:51.940
you don't feel like if it says it's broken,


00:28:51.940 --> 00:28:53.180
it provides you much value


00:28:53.180 --> 00:28:55.980
'cause it might report broken even though it's working.


00:28:55.980 --> 00:28:57.340
But on the flip side,


00:28:57.340 --> 00:28:59.860
if you were doing continuous deployment,


00:28:59.860 --> 00:29:02.100
and by that I mean, I check into a branch,


00:29:02.100 --> 00:29:03.580
that branch noticed the change,


00:29:03.580 --> 00:29:05.740
automatically it rolls out the new version, right?


00:29:05.740 --> 00:29:08.340
Maybe you merge over to a production branch


00:29:08.340 --> 00:29:09.860
and then it just, it takes off.


00:29:09.860 --> 00:29:13.380
The gate to making that not go to production


00:29:13.380 --> 00:29:15.740
is the CI system that's gonna say


00:29:15.740 --> 00:29:17.100
whether or not the tests pass.


00:29:17.100 --> 00:29:20.100
If the test pass in, maybe they shouldn't have


00:29:20.100 --> 00:29:22.340
because you got this flakiness.


00:29:22.340 --> 00:29:24.460
Well, that's also not good.


00:29:24.460 --> 00:29:26.580
- That's a situation when you could have just


00:29:26.580 --> 00:29:28.900
deployed software that wasn't working.


00:29:28.900 --> 00:29:32.180
And then the flip side of that is you have a flaky build


00:29:32.180 --> 00:29:34.940
and you want to be able to release quickly,


00:29:34.940 --> 00:29:37.080
but because test cases are failing,


00:29:37.080 --> 00:29:39.220
you don't release your system.


00:29:39.220 --> 00:29:42.700
And so it can really be a hindrance to being able


00:29:42.700 --> 00:29:45.080
to quickly push your work to production


00:29:45.080 --> 00:29:47.720
because you frequently have flaky test cases


00:29:47.720 --> 00:29:50.400
that are causing you to limit the velocity


00:29:50.400 --> 00:29:51.920
of your development process.


00:29:51.920 --> 00:29:55.760
- This portion of Talk Python to Me


00:29:55.760 --> 00:29:57.560
is brought to you by Sentry.


00:29:57.560 --> 00:29:59.840
You know Sentry for their error tracking service,


00:29:59.840 --> 00:30:01.980
but did you know you can take that all the way


00:30:01.980 --> 00:30:04.320
through your multi-tiered and distributed app


00:30:04.320 --> 00:30:06.720
with their distributed tracing feature?


00:30:06.720 --> 00:30:08.840
Distributed tracing is a debugging technique


00:30:08.840 --> 00:30:11.840
that involves tracking requests of your system,


00:30:11.840 --> 00:30:14.480
starting from the very beginning, like a user action,


00:30:14.480 --> 00:30:19.220
the way to the backend, database, and third-party services. This can help you identify if the


00:30:19.220 --> 00:30:23.220
cause of an error in one project is due to the error in another.


00:30:23.220 --> 00:30:26.780
Every system can benefit from distributed tracing, but they are especially useful for


00:30:26.780 --> 00:30:31.740
microservices. In this architecture, logs won't give you the full picture, so you can't


00:30:31.740 --> 00:30:36.580
debug every request in full just by reading the logs. Distributed tracing with a platform


00:30:36.580 --> 00:30:41.740
like Sentry gives you a visual overview about which services were called during the execution


00:30:41.740 --> 00:30:43.260
of certain requests.


00:30:43.260 --> 00:30:47.880
Aside from debugging and visualizing your architecture, distributed tracing also helps


00:30:47.880 --> 00:30:50.340
you identify performance bottlenecks.


00:30:50.340 --> 00:30:54.380
Through a visual like a Gantt chart, you can see if a particular span in your stack took


00:30:54.380 --> 00:30:59.300
longer than expected and how it could be causing slowdowns in other parts of your app.


00:30:59.300 --> 00:31:04.000
Learn more and see some examples in the tracing section at docs.sentry.io.


00:31:04.000 --> 00:31:08.900
To take advantage of all the features of the Sentry platform, just create your free account.


00:31:08.900 --> 00:31:13.820
And for all of you Talk Python listeners, use the code Talk Python, all one word, and you'll


00:31:13.820 --> 00:31:17.580
activate a free month of their premium paid features.


00:31:17.580 --> 00:31:21.780
Get started today at talkpython.fm/sentry-trace.


00:31:21.780 --> 00:31:25.380
That link is in your podcast player show notes and the episode page.


00:31:25.380 --> 00:31:30.780
Thank you to Sentry for supporting Talk Python to me.


00:31:30.780 --> 00:31:31.780
Got thoughts on that?


00:31:31.780 --> 00:31:33.860
No, I think Greg's pretty much covered that pretty well.


00:31:33.860 --> 00:31:34.860
Yeah, yeah.


00:31:34.860 --> 00:31:35.860
Excellent.


00:31:35.860 --> 00:31:40.180
- Two things, two takeaways from the Google article is one,


00:31:40.180 --> 00:31:42.740
they talked about mitigation strategies.


00:31:42.740 --> 00:31:43.780
And they said, they have a tool


00:31:43.780 --> 00:31:46.380
that monitors the flakiness of tests.


00:31:46.380 --> 00:31:48.060
And if the flakiness is too high,


00:31:48.060 --> 00:31:50.520
it automatically quarantines the test,


00:31:50.520 --> 00:31:52.900
takes it out of the critical path, takes it out of CI.


00:31:52.900 --> 00:31:54.740
You know, maybe somebody notices like,


00:31:54.740 --> 00:31:56.420
hey, there's a new flaky test.


00:31:56.420 --> 00:31:58.580
We need to go find the root cause of that.


00:31:58.580 --> 00:32:00.540
But that's a pretty interesting idea, isn't it?


00:32:00.540 --> 00:32:04.600
Some sort of automation or maybe not quite totally automatic


00:32:04.600 --> 00:32:07.400
but some kind of tool that you can run that'll say,


00:32:07.400 --> 00:32:09.880
this thing has reached a point where maybe


00:32:09.880 --> 00:32:12.460
its value in the test suite is degraded


00:32:12.460 --> 00:32:14.780
because it's so flaky that we need to either fix it


00:32:14.780 --> 00:32:15.620
or just delete it.


00:32:15.620 --> 00:32:18.080
- I think the problem with quarantining tests is


00:32:18.080 --> 00:32:21.020
it only works if the development team


00:32:21.020 --> 00:32:23.660
is serious about investigating them.


00:32:23.660 --> 00:32:26.460
Otherwise, what could end up happening is quarantining


00:32:26.460 --> 00:32:29.240
becomes effectively equivalent to just deleting it.


00:32:29.240 --> 00:32:31.480
If they all end up in a special flaky bucket


00:32:31.480 --> 00:32:32.740
and no one looks at them again,


00:32:32.740 --> 00:32:35.060
the whole point of the process is kind of moot really.


00:32:35.060 --> 00:32:37.980
So doing something like that, I think can be really useful


00:32:37.980 --> 00:32:41.940
if the developers are willing to actually investigate


00:32:41.940 --> 00:32:43.180
why these tests are flaky.


00:32:43.180 --> 00:32:44.020
- That's true.


00:32:44.020 --> 00:32:45.460
If it becomes just a black box


00:32:45.460 --> 00:32:47.180
and basically a trash can for tests,


00:32:47.180 --> 00:32:48.740
then what's the point, right?


00:32:48.740 --> 00:32:49.580
- Exactly, yeah.


00:32:49.580 --> 00:32:51.180
- It kind of goes back to my talking about like,


00:32:51.180 --> 00:32:53.300
there's some people on the team that really care about


00:32:53.300 --> 00:32:55.820
the build and continuous integration and all this


00:32:55.820 --> 00:32:57.500
and other people who are just don't.


00:32:57.500 --> 00:33:00.540
So it does come back to the team mentality


00:33:00.540 --> 00:33:02.940
and people really caring about these things.


00:33:02.940 --> 00:33:05.740
But it's a cool idea, if,


00:33:05.740 --> 00:33:08.180
at least in the optimistic point of view,


00:33:08.180 --> 00:33:10.660
where assuming everyone wants to make sure


00:33:10.660 --> 00:33:12.780
these keep working and someone's gonna pay attention


00:33:12.780 --> 00:33:13.620
to this and so on.


00:33:13.620 --> 00:33:14.620
- Yeah, it's a difficult one,


00:33:14.620 --> 00:33:17.340
'cause I mean, sometimes you can just write a bad test


00:33:17.340 --> 00:33:20.100
and that test is flaky purely because it's a bad test.


00:33:20.100 --> 00:33:22.260
But other times you can write a good test


00:33:22.260 --> 00:33:24.940
that's flaky because there's a problem.


00:33:24.940 --> 00:33:27.180
Like I said before, we had one developer say


00:33:27.180 --> 00:33:28.900
that sometimes a flaky test implies


00:33:28.900 --> 00:33:31.620
that a part of the program they thought was deterministic


00:33:31.620 --> 00:33:32.800
was actually non-deterministic.


00:33:32.800 --> 00:33:36.200
So you're potentially throwing away useful information


00:33:36.200 --> 00:33:37.780
as well as potentially throwing away


00:33:37.780 --> 00:33:39.340
just poorly written tests.


00:33:39.340 --> 00:33:41.140
And it's hard to distinguish between those two.


00:33:41.140 --> 00:33:41.980
- I'm sure that it is.


00:33:41.980 --> 00:33:43.860
Yeah, I mean, identifying these,


00:33:43.860 --> 00:33:45.600
maybe not quarantine them,


00:33:45.600 --> 00:33:48.220
but identifying them is pretty valuable, I would think.


00:33:48.220 --> 00:33:50.580
And then you can see what lessons come from that, right?


00:33:50.580 --> 00:33:52.100
What you do once you've identified it,


00:33:52.100 --> 00:33:54.020
I think that is up for debate, right?


00:33:54.020 --> 00:33:54.860
- Yeah.


00:33:54.860 --> 00:33:57.420
- Okay, the other one is test flakiness,


00:33:57.420 --> 00:33:59.780
methods for identifying and dealing with flaky tests


00:33:59.780 --> 00:34:03.140
by Jason Palmer from Spotify, which is also cool.


00:34:03.140 --> 00:34:04.740
This one has pictures, which is fun.


00:34:04.740 --> 00:34:08.020
They've got like a graphical analysis of their tests


00:34:08.020 --> 00:34:10.320
and the flakiness of it and so on.


00:34:10.320 --> 00:34:12.960
They came up with a thing called flaky bot


00:34:12.960 --> 00:34:16.700
and it's a GitHub integration,


00:34:16.700 --> 00:34:18.960
GitHub bot that they can make it run


00:34:18.960 --> 00:34:21.860
and they can ask it to exercise the test really quickly


00:34:21.860 --> 00:34:23.060
and see if it's flaky.


00:34:23.060 --> 00:34:24.620
And I got the sense that it does that


00:34:24.620 --> 00:34:26.940
by just running it a bunch of different times


00:34:26.940 --> 00:34:29.800
with different delays and seeing if it always passes


00:34:29.800 --> 00:34:32.540
or if it potentially sometimes passes or fails.


00:34:32.540 --> 00:34:34.680
- So I think broadly, one of the things


00:34:34.680 --> 00:34:36.960
that is mentioned in this article


00:34:36.960 --> 00:34:39.920
and something that's done by a number of pytest plugins


00:34:39.920 --> 00:34:42.800
as well is rerunning the test suite.


00:34:42.800 --> 00:34:46.700
And so you could imagine rerunning each test case


00:34:46.700 --> 00:34:47.860
in isolation.


00:34:47.860 --> 00:34:52.040
You could also imagine picking a group of test cases


00:34:52.040 --> 00:34:55.540
and then rerunning the test cases in that group,


00:34:55.540 --> 00:34:59.660
either in a random order or in certain fixed orders.


00:34:59.660 --> 00:35:02.500
So rerunning is often a very helpful way


00:35:02.500 --> 00:35:05.580
for us to detect flaky test cases,


00:35:05.580 --> 00:35:08.020
whether we rerun the whole test suite,


00:35:08.020 --> 00:35:10.900
whether we run test cases individually,


00:35:10.900 --> 00:35:14.100
or whether we run test cases in groups.


00:35:14.100 --> 00:35:16.500
Obviously, one of the clear downsides


00:35:16.500 --> 00:35:18.900
associated with rerunning a test suite


00:35:18.900 --> 00:35:23.180
is the execution time associated with the rerunning process.


00:35:23.180 --> 00:35:24.460
- Yeah, the more you run it,


00:35:24.460 --> 00:35:27.740
the more likely you're able to detect flakiness.


00:35:27.740 --> 00:35:29.140
If it's only a little bit flaky,


00:35:29.140 --> 00:35:31.860
but at the same time, the longer that goes,


00:35:31.860 --> 00:35:34.140
the longer it takes, that's also a problem.


00:35:34.140 --> 00:35:36.020
There's another thing in here


00:35:36.020 --> 00:35:37.440
that I thought was pretty interesting,


00:35:37.440 --> 00:35:39.660
but I'm struggling to find it in this article


00:35:39.660 --> 00:35:42.540
for the second integration now.


00:35:42.540 --> 00:35:44.520
I thought, oh, end to end maybe.


00:35:44.520 --> 00:35:46.860
So in the Spotify article,


00:35:46.860 --> 00:35:49.020
they say that, in their assessment,


00:35:49.020 --> 00:35:52.140
that end-to-end tests are flaky by nature.


00:35:52.140 --> 00:35:53.180
Write fewer of them.


00:35:53.180 --> 00:35:56.780
So I get the sense, I don't know, I get the sense maybe you all sort of feel this


00:35:56.780 --> 00:35:59.420
way as well, but I don't necessarily agree with that.


00:35:59.420 --> 00:36:04.180
I think end to end tests, if they are flaky, that's telling you something about


00:36:04.180 --> 00:36:04.940
your program.


00:36:04.940 --> 00:36:08.420
It might not be really precisely narrowing in on it, but it's telling you


00:36:08.420 --> 00:36:10.100
something about your program.


00:36:10.100 --> 00:36:13.180
If you can write end to end tests that are flaky, what do you think?


00:36:13.180 --> 00:36:17.420
I think with end to end tests, I mean, sort of saying they're flaky by nature,


00:36:17.420 --> 00:36:21.020
maybe a little strong, but they're certainly more susceptible to flakiness


00:36:21.020 --> 00:36:23.860
purely because there's a hell of a lot more going on.


00:36:23.860 --> 00:36:28.540
So I think when we talk about this sort of flakiness and sort of precision trade-off,


00:36:28.540 --> 00:36:31.060
I think with end-to-end tests, you should be a little bit more forgiving with


00:36:31.060 --> 00:36:33.060
flakiness purely because there's more going on.


00:36:33.060 --> 00:36:36.860
So like, for example, for a unit test, you shouldn't really accept any flakiness


00:36:36.860 --> 00:36:39.340
because that's a very focused test case.


00:36:39.340 --> 00:36:40.980
So yeah, those are my thoughts on that.


00:36:40.980 --> 00:36:41.460
Okay.


00:36:41.460 --> 00:36:43.100
I would agree with what Owen said.


00:36:43.100 --> 00:36:47.860
I still think there is quite a bit of value end-to-end or integration testing


00:36:48.180 --> 00:36:49.900
because from my perspective,


00:36:49.900 --> 00:36:53.900
it's increasing the realism of the testing process.


00:36:53.900 --> 00:36:56.540
So I still write end-to-end test cases


00:36:56.540 --> 00:36:58.540
if I'm building a web API


00:36:58.540 --> 00:37:00.660
or even if I'm building an application,


00:37:00.660 --> 00:37:02.860
but I think I have to be willing to tolerate


00:37:02.860 --> 00:37:04.460
a little bit more flakiness


00:37:04.460 --> 00:37:08.900
and perhaps even be creative with the various strategies


00:37:08.900 --> 00:37:11.100
that I adopt when I do rerunning.


00:37:11.100 --> 00:37:14.340
Maybe I need to run some of my integration tests


00:37:14.340 --> 00:37:17.000
with really good setup and teardown


00:37:17.000 --> 00:37:19.640
to avoid pollution between test cases.


00:37:19.640 --> 00:37:22.380
Or maybe certain integration test cases


00:37:22.380 --> 00:37:25.360
have to be run completely in isolation


00:37:25.360 --> 00:37:27.640
and they can't be run while any other part


00:37:27.640 --> 00:37:29.400
of the program is being used.


00:37:29.400 --> 00:37:30.720
So in those cases,


00:37:30.720 --> 00:37:33.800
maybe my integration tests are run less frequently,


00:37:33.800 --> 00:37:37.320
but I still keep them as a part of my pytest test suite.


00:37:37.320 --> 00:37:39.080
- Yeah, interesting, both of you.


00:37:39.080 --> 00:37:41.120
For me, one of the things I do that I think


00:37:41.120 --> 00:37:43.600
is really valuable is over at Talk Python,


00:37:43.600 --> 00:37:45.780
we have the courses and that web app


00:37:45.780 --> 00:37:47.960
that serves up the courses once people buy them


00:37:47.960 --> 00:37:48.800
and all that sort of stuff.


00:37:48.800 --> 00:37:51.040
It's like 20,000 lines of Python, maybe more.


00:37:51.040 --> 00:37:52.640
These days I haven't measured it for a long time,


00:37:52.640 --> 00:37:54.600
but it's a non-trivial amount.


00:37:54.600 --> 00:37:58.900
And it's got a site map of all the pages on there.


00:37:58.900 --> 00:38:00.580
And one of the things I do for the test


00:38:00.580 --> 00:38:03.700
is just go and find every, pull the site map,


00:38:03.700 --> 00:38:05.860
look at every page on the site and just request it


00:38:05.860 --> 00:38:10.100
and make sure it doesn't 500 out or 404 or things like that.


00:38:10.100 --> 00:38:12.980
And it just, they all work, right?


00:38:12.980 --> 00:38:16.120
Now there's like 6,000 links in the sitemap.


00:38:16.120 --> 00:38:20.420
So it says, well, these 500 are all really the same thing


00:38:20.420 --> 00:38:22.260
with just different data behind it.


00:38:22.260 --> 00:38:23.580
So just pick one of those.


00:38:23.580 --> 00:38:25.700
There's a way to kind of winnow it down to, you know,


00:38:25.700 --> 00:38:28.740
20 requests and not 6,000, but that kind of stuff,


00:38:28.740 --> 00:38:33.160
your, there should be no time where there is a 404


00:38:33.160 --> 00:38:34.020
on your site.


00:38:34.020 --> 00:38:36.900
It's not an inherent flakiness of testing


00:38:36.900 --> 00:38:37.980
that there's not a 404.


00:38:37.980 --> 00:38:39.460
There should not be a 404.


00:38:39.460 --> 00:38:42.320
Same thing, there should not be a 500, my website,


00:38:42.320 --> 00:38:45.040
my server crashed, it should never crash.


00:38:45.040 --> 00:38:45.920
All right.


00:38:45.920 --> 00:38:47.600
And so those types of integration tests,


00:38:47.600 --> 00:38:49.800
I think they still add a lot of value, right?


00:38:49.800 --> 00:38:51.040
'Cause you could miss something like,


00:38:51.040 --> 00:38:53.280
well, I checked the database models, they were fine.


00:38:53.280 --> 00:38:54.280
I checked the code that works


00:38:54.280 --> 00:38:55.440
with the database models are fine,


00:38:55.440 --> 00:38:58.520
but the HTML assume there was a field


00:38:58.520 --> 00:39:00.340
in the database model we passed to it.


00:39:00.340 --> 00:39:02.680
There is value in these sort of holistic,


00:39:02.680 --> 00:39:05.280
like does it still click together story, I think.


00:39:05.280 --> 00:39:07.840
- No, I think you're making a really good point, Michael.


00:39:07.840 --> 00:39:10.400
And so what I often do, like say, for example,


00:39:10.400 --> 00:39:14.360
I'm adding a new feature or I'm adding a bug fix.


00:39:14.360 --> 00:39:17.200
What I'm going to regularly do to be confident


00:39:17.200 --> 00:39:22.040
in my changes to the system is run my integration tests


00:39:22.040 --> 00:39:25.500
maybe every once in a while and run my unit tests


00:39:25.500 --> 00:39:28.840
very frequently during the refactoring process.


00:39:28.840 --> 00:39:30.420
I can't run them all the time


00:39:30.420 --> 00:39:33.180
because they regrettably take too long to run,


00:39:33.180 --> 00:39:37.160
but I can run my integration test cases very frequently


00:39:37.160 --> 00:39:39.760
when I'm adding a bug fix or adding a new feature.


00:39:39.760 --> 00:39:41.480
And then every once in a while,


00:39:41.480 --> 00:39:44.320
do the kind of smoke tests that you mentioned,


00:39:44.320 --> 00:39:46.920
and then the integration or end-to-end testing


00:39:46.920 --> 00:39:48.280
that we've been discussing,


00:39:48.280 --> 00:39:51.760
so that ultimately I have rapid feedback


00:39:51.760 --> 00:39:53.180
that gives me confidence.


00:39:53.180 --> 00:39:56.040
And then additionally, I have longer running tests


00:39:56.040 --> 00:39:59.040
that further give me confidence that my program is working.


00:39:59.040 --> 00:40:00.120
- That's a really good point.


00:40:00.120 --> 00:40:02.240
Maybe you don't even run all your unit tests


00:40:02.240 --> 00:40:03.360
as you're writing that feature.


00:40:03.360 --> 00:40:06.220
Maybe you run a group of them that you think are related.


00:40:06.220 --> 00:40:07.700
- Yeah, so you're bringing up something


00:40:07.700 --> 00:40:10.580
that I really love about the Python ecosystem,


00:40:10.580 --> 00:40:13.700
which is the awesome coverage.py


00:40:13.700 --> 00:40:16.180
and the pytest-cov plugin.


00:40:16.180 --> 00:40:18.480
Those plugins are really good.


00:40:18.480 --> 00:40:20.740
And what's awesome about coverage.py


00:40:20.740 --> 00:40:24.900
is that it can track code coverage on a per test case basis.


00:40:24.900 --> 00:40:27.380
So one of the things that I will often do


00:40:27.380 --> 00:40:31.080
is look at what test case or what part of the system.


00:40:31.080 --> 00:40:34.360
And as you mentioned, I'll only run those test cases


00:40:34.360 --> 00:40:36.220
that are covering the parts of the system


00:40:36.220 --> 00:40:38.380
that I'm changing because that helps me


00:40:38.380 --> 00:40:42.340
to get very rapid feedback from my unit test cases


00:40:42.340 --> 00:40:43.820
while I'm adding a new feature


00:40:43.820 --> 00:40:45.420
to a certain part of my program.


00:40:45.420 --> 00:40:48.300
- I didn't realize that coverage.py would tell you


00:40:48.300 --> 00:40:51.460
that in reverse, like for this part of your program,


00:40:51.460 --> 00:40:52.820
these are the five tests.


00:40:52.820 --> 00:40:53.660
That's really cool.


00:40:53.660 --> 00:40:55.200
- Yeah, so I really like that feature.


00:40:55.200 --> 00:40:58.980
I think it was released in coverage.py 5.0


00:40:58.980 --> 00:41:02.420
and I've been using it since the feature was available.


00:41:02.420 --> 00:41:05.000
It's incredibly helpful because of the fact


00:41:05.000 --> 00:41:09.360
that you can look at specific statements in your code


00:41:09.360 --> 00:41:12.840
and then find out which test cases cover those statements


00:41:12.840 --> 00:41:16.200
and then choose to rerun those specific tests


00:41:16.200 --> 00:41:18.800
when you're repeatedly running your test suite.


00:41:18.800 --> 00:41:21.440
And I call that test suite reduction


00:41:21.440 --> 00:41:24.080
or coverage-based test suite reduction.


00:41:24.080 --> 00:41:29.080
And having what CoveragePi calls the context of coverage


00:41:29.080 --> 00:41:32.040
is in my experience, very, very helpful.


00:41:32.040 --> 00:41:33.440
- Yeah, the bigger your test suite is,


00:41:33.440 --> 00:41:34.560
the more helpful it is, right?


00:41:34.560 --> 00:41:35.400
- Absolutely.


00:41:35.400 --> 00:41:36.960
- Owen, did you find that a lot of people


00:41:36.960 --> 00:41:40.440
were using those kinds of tools to sort of limit


00:41:40.440 --> 00:41:41.480
the amount of tests they got to run?


00:41:41.480 --> 00:41:43.920
- With the sort of programs I was working with,


00:41:43.920 --> 00:41:45.320
for the purposes of my experiments,


00:41:45.320 --> 00:41:48.080
I was running the whole test suite in its entirety,


00:41:48.080 --> 00:41:49.680
multiple times to find flaky tests,


00:41:49.680 --> 00:41:52.320
but I did see evidence of that kind of thing being set up.


00:41:52.320 --> 00:41:55.320
So I think it is fairly well adopted.


00:41:55.320 --> 00:41:57.880
Once again, it's a lot more relevant


00:41:57.880 --> 00:42:00.720
to very large projects as opposed to small projects


00:42:00.720 --> 00:42:03.280
where if it only takes you 10 seconds


00:42:03.280 --> 00:42:04.600
to run the whole test suite.


00:42:04.600 --> 00:42:06.440
Obviously there's not a lot of point in doing--


00:42:06.440 --> 00:42:07.920
- Just let it run. - Yeah.


00:42:07.920 --> 00:42:10.840
But when you've got, I've dealt with test suites


00:42:10.840 --> 00:42:15.440
that take best part of six hours to run end to end.


00:42:15.440 --> 00:42:17.440
So having some kind of test selection,


00:42:17.440 --> 00:42:20.200
test reduction there is essential really.


00:42:20.200 --> 00:42:21.320
- In the winter it's nice,


00:42:21.320 --> 00:42:23.840
because then your computer can spend six hours


00:42:23.840 --> 00:42:24.920
heating the room.


00:42:24.920 --> 00:42:27.960
It puts a little less stress on the house heater,


00:42:27.960 --> 00:42:29.400
or office heater.


00:42:29.400 --> 00:42:32.160
Seriously, what do you think about the systems,


00:42:32.160 --> 00:42:35.720
like the IDEs that have their extensions are built right in,


00:42:35.720 --> 00:42:38.680
where they just constantly run the tests as you make changes.


00:42:38.680 --> 00:42:40.200
They just notice the files have changed,


00:42:40.200 --> 00:42:41.360
so we're rerunning the tests.


00:42:41.360 --> 00:42:43.440
- So I don't use that in an IDE,


00:42:43.440 --> 00:42:46.040
but I do have something like that set up


00:42:46.040 --> 00:42:48.780
that runs in a terminal window.


00:42:48.780 --> 00:42:52.080
And I found continuous testing to be quite helpful.


00:42:52.080 --> 00:42:55.400
It's really helpful in cases where maybe I forget


00:42:55.400 --> 00:42:58.680
to run my test suite while I'm refactoring my program,


00:42:58.680 --> 00:43:01.540
and it can give me immediate feedback.


00:43:01.540 --> 00:43:03.860
To go back to a comment that I made previously,


00:43:03.860 --> 00:43:06.900
you can also use different pytest plugins


00:43:06.900 --> 00:43:09.060
or use something like Hypothesis


00:43:09.060 --> 00:43:10.920
so that you can run your test suite


00:43:10.920 --> 00:43:14.100
with random inputs on a regular basis.


00:43:14.100 --> 00:43:16.780
And I have found that's another good way for me


00:43:16.780 --> 00:43:20.560
to be able to, without having to think too hard,


00:43:20.560 --> 00:43:23.420
find potential bugs in the functions that I'm testing.


00:43:23.420 --> 00:43:24.260
- Okay, interesting.


00:43:24.260 --> 00:43:26.020
Let's talk about some of the tools.


00:43:26.020 --> 00:43:28.420
So you all highlighted a couple of tools


00:43:28.420 --> 00:43:32.820
that people can use to help find flaky tests.


00:43:32.820 --> 00:43:34.440
So over at Datadog,


00:43:34.440 --> 00:43:36.820
they've got a one for flaky test management.


00:43:36.820 --> 00:43:38.020
Want to tell people about that?


00:43:38.020 --> 00:43:41.420
- So many of the tools that are provided by companies


00:43:41.420 --> 00:43:45.780
like Datadog are offering you type of dashboard


00:43:45.780 --> 00:43:48.740
that will help you to better understand


00:43:48.740 --> 00:43:50.940
the characteristics of your test suite.


00:43:50.940 --> 00:43:54.100
So you can see what are the test cases


00:43:54.100 --> 00:43:56.420
that tend to be the most flaky.


00:43:56.420 --> 00:44:02.580
I think oftentimes it's hard for us to get a big picture view of our test suite and to


00:44:02.580 --> 00:44:05.540
understand what is and is not flaky.


00:44:05.540 --> 00:44:11.060
And so therefore having a flaky test management dashboard like Datadog provides can often


00:44:11.060 --> 00:44:16.100
give me the observability or the visibility that I might miss otherwise.


00:44:16.100 --> 00:44:17.100
- That's super cool.


00:44:17.100 --> 00:44:19.820
And let's see, there's, I don't know, that's not the one I want to pull up.


00:44:19.820 --> 00:44:22.540
Also at Cypress has flaky test management.


00:44:22.540 --> 00:44:28.300
This is a really interesting approach because I normally use Cypress when I'm testing websites.


00:44:28.300 --> 00:44:29.820
And in my experience, when I'm...


00:44:29.820 --> 00:44:30.820
What is Cypress?


00:44:30.820 --> 00:44:31.820
I'm not familiar with that.


00:44:31.820 --> 00:44:32.820
Maybe people aren't as well.


00:44:32.820 --> 00:44:33.820
Give us a quick intro first.


00:44:33.820 --> 00:44:34.820
Sure.


00:44:34.820 --> 00:44:35.820
I'd love to do so.


00:44:35.820 --> 00:44:41.720
So Cypress is a tool that helps you to do testing of your web user interfaces.


00:44:41.720 --> 00:44:47.420
So if you have a web application and you want to be able to test the input to a form, or


00:44:47.420 --> 00:44:50.000
or you want to be able to test certain workflows


00:44:50.000 --> 00:44:52.140
through your web application,


00:44:52.140 --> 00:44:56.420
you can use Cypress and you write your test cases.


00:44:56.420 --> 00:45:00.560
Essentially, it's as if Cypress is running its own Chrome


00:45:00.560 --> 00:45:02.340
and it can control your test suite,


00:45:02.340 --> 00:45:04.240
it can run your test cases.


00:45:04.240 --> 00:45:07.760
When things fail, it can actually give you snapshots


00:45:07.760 --> 00:45:09.440
of what failed.


00:45:09.440 --> 00:45:13.740
It can tell you about the browser version that it was using


00:45:13.740 --> 00:45:16.880
or maybe the mobile ready viewport


00:45:16.880 --> 00:45:18.960
that it was currently running at that.


00:45:18.960 --> 00:45:21.520
And again, the nice thing about things


00:45:21.520 --> 00:45:25.420
like what Cypress provides is that it can give you


00:45:25.420 --> 00:45:28.740
some kind of flaky test case analytics,


00:45:28.740 --> 00:45:30.300
which can show you which are failing


00:45:30.300 --> 00:45:31.440
and which are passing.


00:45:31.440 --> 00:45:34.640
And it can also say, hey, these that are flaky


00:45:34.640 --> 00:45:37.100
and then break it out in terms of which ones


00:45:37.100 --> 00:45:40.140
are the most flaky versus the least flaky.


00:45:40.140 --> 00:45:44.840
Again, primarily in the context of testing web interfaces


00:45:44.840 --> 00:45:46.280
or web applications.


00:45:46.280 --> 00:45:49.880
Sounds a little bit like Selenium or Playwright, which are both nice.


00:45:49.880 --> 00:45:57.080
It is. So I have to say I've had the most flaky tests for my Selenium test cases,


00:45:57.080 --> 00:46:03.080
but when I switched to either Cypress or Playwright, Playwright as well has a way


00:46:03.080 --> 00:46:08.480
so that you don't have to do these baked-in weights inside of your test case,


00:46:08.480 --> 00:46:14.480
which is one of the sources of flakiness that Owen and I have found in a number of real-world programs.


00:46:14.480 --> 00:46:16.320
I'd say that's almost one of the most common actually.


00:46:16.320 --> 00:46:17.160
- Okay, Owen.


00:46:17.160 --> 00:46:20.480
So Gregory points out that it could be not exactly


00:46:20.480 --> 00:46:23.400
there's something wrong with your program or your code


00:46:23.400 --> 00:46:25.420
or the infrastructure your code depends upon,


00:46:25.420 --> 00:46:29.340
but maybe almost a flaky test framework itself,


00:46:29.340 --> 00:46:31.840
a flaky test runner scenario


00:46:31.840 --> 00:46:35.760
or where the flakiness is not even necessarily,


00:46:35.760 --> 00:46:39.000
it's in the observation, not in the execution.


00:46:39.000 --> 00:46:40.280
That's pretty interesting.


00:46:40.280 --> 00:46:42.480
- Yeah, the classic formula for something like that


00:46:42.480 --> 00:46:45.940
It's a test case that says launch something asynchronously,


00:46:45.940 --> 00:46:48.080
wait one second, check something.


00:46:48.080 --> 00:46:50.880
Yeah, you might think that that one second is enough.


00:46:50.880 --> 00:46:53.120
If there's a time when for whatever reason there's


00:46:53.120 --> 00:46:56.760
some background work going on or it takes a little longer than that, then.


00:46:56.760 --> 00:46:58.840
>> Then all of a sudden it needed one and a half seconds.


00:46:58.840 --> 00:47:00.560
>> Then you have a flaky test. Yeah.


00:47:00.560 --> 00:47:03.680
>> Yeah, for sure. Any of those things where you have to start something and then


00:47:03.680 --> 00:47:06.460
wait for it to something to happen remotely,


00:47:06.460 --> 00:47:08.080
that's got to be pretty sketchy.


00:47:08.080 --> 00:47:11.240
>> The usual approach is to have an explicit wait.


00:47:11.240 --> 00:47:17.040
So you'll sort of say, I'm actually going to wait until this is completed, whatever that means.


00:47:17.040 --> 00:47:25.160
But then you run into a situation where, well, what if for whatever reason, this asynchronous thing you're interacting with is timed out or frozen,


00:47:25.160 --> 00:47:27.320
then you're going to end up with a test that's waiting forever.


00:47:27.320 --> 00:47:30.640
So you have to have some kind of upper limit to how long you'll wait for.


00:47:30.640 --> 00:47:33.960
Otherwise, you may wait forever. Yeah, this test case is real slow.


00:47:33.960 --> 00:47:38.240
So once again, there's no kind of silver bullet solution, really. It's just trade offs again.


00:47:38.240 --> 00:47:48.440
Yeah, yeah, yeah. What do you all think about things like tenacity where you can go and put a decorator onto a function and just say, retry this with some scenario?


00:47:48.440 --> 00:47:53.240
Or another one that I just learned about is Henix stamina, which is cool as well.


00:47:53.240 --> 00:48:03.840
You can say, put a decorator and say, retry certain number of attempts with, you know, like some kind of back off, an exponential back off where you give it a certain amount of time.


00:48:03.840 --> 00:48:07.560
Like, for flaky tests, you see it making sense to say,


00:48:07.560 --> 00:48:11.400
well, maybe call the functions this way in some of your test cases.


00:48:11.400 --> 00:48:13.500
So I've never actually seen either of these plugins,


00:48:13.500 --> 00:48:15.160
but they do look quite interesting.


00:48:15.160 --> 00:48:15.500
Yeah.


00:48:15.500 --> 00:48:19.240
I haven't used Tenacity either, but I was aware of it.


00:48:19.240 --> 00:48:23.900
And I think you could imagine using Tenacity in two distinct locations.


00:48:23.900 --> 00:48:27.600
Maybe you want to put some of these Tenacity annotations


00:48:27.600 --> 00:48:30.200
on your multi-threaded code,


00:48:30.200 --> 00:48:34.400
and then let the test case call those annotated functions.


00:48:34.400 --> 00:48:35.240
- Yes, exactly.


00:48:35.240 --> 00:48:36.520
Don't put them in your production code,


00:48:36.520 --> 00:48:37.400
don't put them on your test,


00:48:37.400 --> 00:48:39.760
but just have like an intermediate one


00:48:39.760 --> 00:48:42.280
that you can control the back off and retry count.


00:48:42.280 --> 00:48:43.880
- Exactly, exactly.


00:48:43.880 --> 00:48:46.240
And another thing that I think is worth pointing out


00:48:46.240 --> 00:48:49.100
since we were previously discussing web testing


00:48:49.100 --> 00:48:53.580
is that there is a way in which it can be a problem


00:48:53.580 --> 00:48:55.560
with your testing framework,


00:48:55.560 --> 00:48:57.480
like you previously mentioned, Michael.


00:48:57.480 --> 00:49:02.280
So for example, Playwright does have a really nice auto weighting feature.


00:49:02.280 --> 00:49:08.620
And so when I'm testing a web application, I can use Playwright's auto weight feature,


00:49:08.620 --> 00:49:15.080
and that will help me to avoid baking in hard coded weights inside my test,


00:49:15.080 --> 00:49:21.100
because the actual testing framework itself has a way to do auto weighting.


00:49:21.100 --> 00:49:25.580
So when you say auto weighting, you can say things like, request this page,


00:49:25.740 --> 00:49:28.140
find this field, put my email address into it,


00:49:28.140 --> 00:49:30.660
click this button, test that this thing is,


00:49:30.660 --> 00:49:32.620
now the page has this thing in it.


00:49:32.620 --> 00:49:36.420
But obviously servers don't instantly respond to that,


00:49:36.420 --> 00:49:38.100
so you've got to have some sorts of delays.


00:49:38.100 --> 00:49:39.300
So you're talking about the system


00:49:39.300 --> 00:49:41.340
can kind of track that automatically?


00:49:41.340 --> 00:49:43.340
- Yeah, so Playwright can actually do


00:49:43.340 --> 00:49:44.700
some of that on its own.


00:49:44.700 --> 00:49:48.900
So for example, if you're looking for a certain element


00:49:48.900 --> 00:49:51.300
in the webpage to be available,


00:49:51.300 --> 00:49:54.140
Playwright has a way that will allow you to ensure


00:49:54.140 --> 00:49:57.140
that the element is actually attached to the DOM,


00:49:57.140 --> 00:49:59.180
that it's actually visible,


00:49:59.180 --> 00:50:01.020
that it hasn't moved around


00:50:01.020 --> 00:50:04.000
or that it's not being animated in some way.


00:50:04.000 --> 00:50:06.580
And all of those things are actually part


00:50:06.580 --> 00:50:08.560
of the testing framework,


00:50:08.560 --> 00:50:10.420
which makes it incredibly helpful


00:50:10.420 --> 00:50:14.260
because then I don't have to actually implement all of that


00:50:14.260 --> 00:50:15.900
when I'm writing my test cases.


00:50:15.900 --> 00:50:16.720
- Fantastic.


00:50:16.720 --> 00:50:18.440
We're getting a little short on time here.


00:50:18.440 --> 00:50:19.920
Let's round, I want to round it out


00:50:19.920 --> 00:50:22.380
with a little bit of a survey.


00:50:22.380 --> 00:50:28.140
If I find the right thing of some, y'all mentioned some of the, I test


00:50:28.140 --> 00:50:30.540
plugins that might be relevant here.


00:50:30.540 --> 00:50:34.920
So I'm pulling up awesome pie test, which I'll link to just an awesome list of.


00:50:34.920 --> 00:50:40.880
High test things, but you've got things like in here, like high test randomly,


00:50:40.880 --> 00:50:44.800
which lets you randomly order tests and set a seed and those kinds of things.


00:50:44.800 --> 00:50:46.100
And a bunch of other stuff.


00:50:46.100 --> 00:50:49.800
You want to pull out some of these, you maybe think are relevant or see if


00:50:49.800 --> 00:50:52.260
they're at least in your list, the ones you like, so I've used randomly


00:50:52.260 --> 00:50:57.460
before. And like how I said earlier, this could be a great way of finding those


00:50:57.460 --> 00:51:00.980
tests that not necessarily the ones that don't clean up after themselves properly,


00:51:00.980 --> 00:51:05.100
but it will certainly show you tests that are potentially impacted by other


00:51:05.100 --> 00:51:07.500
tests, not cleaning up after themselves.


00:51:07.500 --> 00:51:11.660
So I think if you take almost any large test suite and apply randomly to it, the


00:51:11.660 --> 00:51:14.860
chances are you are probably going to see some failed tests that weren't failing


00:51:14.860 --> 00:51:16.100
before you shuffled the order.


00:51:16.100 --> 00:51:20.660
So I think that's quite an interesting plugin and you can use to quickly assess


00:51:20.700 --> 00:51:23.180
if you've got all the dependent tests in your test suite.


00:51:23.180 --> 00:51:28.180
Speaking from experience, the one additional point that I would add is that when I use


00:51:28.180 --> 00:51:35.700
pytest randomly, I try to make sure I integrate it early into the lifetime of my development


00:51:35.700 --> 00:51:43.180
process. So instead of writing 947 test cases, and then trying to use pytest randomly, I


00:51:43.180 --> 00:51:49.700
tried to always make sure that pytest randomly is running in GitHub Actions very early in


00:51:49.700 --> 00:51:54.780
the development of my application so that when I only have 40 or 50 test cases, I can


00:51:54.780 --> 00:52:00.580
immediately find those dependent tests that could have flakiness and then begin to be


00:52:00.580 --> 00:52:06.660
more proactive when it comes to avoiding flakiness very early when I'm launching a new product.


00:52:06.660 --> 00:52:07.660
Yeah, that makes sense.


00:52:07.660 --> 00:52:12.500
One of my follow-up questions was going to be, would you all recommend de facto installing


00:52:12.500 --> 00:52:17.940
that into and turning that on at least unless you have a reason to disable it for new projects?


00:52:17.940 --> 00:52:19.180
I find it very helpful.


00:52:19.180 --> 00:52:22.420
one of the things that I frequently add to a new project.


00:52:22.420 --> 00:52:27.680
So when I'm, I use poetry for a lot of my package management and I have various


00:52:27.680 --> 00:52:33.040
templates that I use and I often add pytest randomly right away as one of my dev


00:52:33.040 --> 00:52:37.420
dependencies and then make sure I'm always running my test suite in a random


00:52:37.420 --> 00:52:39.940
order when I'm running it in GitHub actions.


00:52:39.940 --> 00:52:44.380
So I can't remember who I heard this off for, where I read it exactly, but I have


00:52:44.380 --> 00:52:48.940
heard that at Google, other companies as well, running the tests in a random order


00:52:48.940 --> 00:52:52.460
is actually standard practice for the reason like Greg just said.


00:52:52.460 --> 00:52:56.500
So when you start on a new project, you're starting with this sort of


00:52:56.500 --> 00:52:57.980
shuffled order test running.


00:52:57.980 --> 00:53:00.900
And now I suppose it's kind of like a technical debt, then you're kind of


00:53:00.900 --> 00:53:05.380
paying it off early rather than writing a bunch of tests and then having a big


00:53:05.380 --> 00:53:08.780
fixing effort when you realize there's a big problem with a whole bunch of them.


00:53:08.780 --> 00:53:12.660
I feel like it's a little similar to linting and you have things that go


00:53:12.660 --> 00:53:16.300
through and tell you these are the recommended issues or issues we found.


00:53:16.300 --> 00:53:18.460
We recommend fixing them for your code.


00:53:18.460 --> 00:53:22.260
And if you apply that retroactively, like rough or whatever,


00:53:22.260 --> 00:53:25.660
if you apply that to your project after it's huge,


00:53:25.660 --> 00:53:26.900
you'll get thousands of lines


00:53:26.900 --> 00:53:29.140
and nobody wants to spend the next two weeks fixing them.


00:53:29.140 --> 00:53:31.420
But if you just run that as you develop,


00:53:31.420 --> 00:53:33.940
you go, there's two little things we gotta fix, no big deal.


00:53:33.940 --> 00:53:36.180
See, it sounds similar to that effect.


00:53:36.180 --> 00:53:37.000
- I agree.


00:53:37.000 --> 00:53:38.540
- Yeah, here's another one that's interesting.


00:53:38.540 --> 00:53:41.740
Hightest.socket to disable socket calls during tests.


00:53:41.740 --> 00:53:45.140
So you heard me talk about requesting every page


00:53:45.140 --> 00:53:46.220
on the sitemap.


00:53:46.220 --> 00:53:48.020
So I'm not necessarily suggesting


00:53:48.020 --> 00:53:50.620
that you would want to just do this in general.


00:53:50.620 --> 00:53:52.340
But you know, one of the areas that seems to me


00:53:52.340 --> 00:53:55.600
that could be result in flakiness for a set of tests


00:53:55.600 --> 00:53:57.700
is like I depend on an external system.


00:53:57.700 --> 00:53:59.700
Like, oh, I thought we were mocking out the database,


00:53:59.700 --> 00:54:01.300
but I'm actually talking to the database.


00:54:01.300 --> 00:54:04.100
Or, oh, I thought we were mocking out the API call,


00:54:04.100 --> 00:54:05.300
but we're talking to it.


00:54:05.300 --> 00:54:07.340
You could probably turn that on for a moment,


00:54:07.340 --> 00:54:09.100
see which test fails and just go,


00:54:09.100 --> 00:54:11.340
well, these three were not supposed to be talking


00:54:11.340 --> 00:54:13.340
over the network, but somehow they fail


00:54:13.340 --> 00:54:15.220
when we don't let them talk to the network.


00:54:15.220 --> 00:54:16.660
So that might be worth looking into.


00:54:16.660 --> 00:54:17.500
What do you think about that?


00:54:17.500 --> 00:54:19.380
- Good point. I haven't tried that tool,


00:54:19.380 --> 00:54:21.180
but the way that you've explained it


00:54:21.180 --> 00:54:23.140
makes it really clear that there would be


00:54:23.140 --> 00:54:25.020
a lot of utility to using it.


00:54:25.020 --> 00:54:25.940
- Yeah. Let's see.


00:54:25.940 --> 00:54:27.980
There's probably a couple other ones in here.


00:54:27.980 --> 00:54:29.760
There was one right back here.


00:54:29.760 --> 00:54:33.100
It was called a pytest Picked.


00:54:33.100 --> 00:54:34.900
And I don't really know how I feel about this.


00:54:34.900 --> 00:54:36.060
I don't know if it's precise enough,


00:54:36.060 --> 00:54:38.500
but you were talking, Greg,


00:54:38.500 --> 00:54:40.460
about winnowing down the set of tests


00:54:40.460 --> 00:54:41.660
you were running using coverage.


00:54:41.660 --> 00:54:44.020
And this one says it runs test related to changes


00:54:44.020 --> 00:54:47.900
detected by version control, just unstaged files.


00:54:47.900 --> 00:54:49.700
I feel like this is a really cool idea,


00:54:49.700 --> 00:54:51.700
but it does it in the wrong order.


00:54:51.700 --> 00:54:53.900
I feel like it's looking at just the test files


00:54:53.900 --> 00:54:56.060
that are not changed or that are not committed


00:54:56.060 --> 00:54:57.260
and rerunning those.


00:54:57.260 --> 00:54:59.460
But you should look at the code covered,


00:54:59.460 --> 00:55:03.220
the changes, unstaged production files,


00:55:03.220 --> 00:55:04.980
and then use code covers to figure out


00:55:04.980 --> 00:55:07.620
which tests need to be rerun, right?


00:55:07.620 --> 00:55:09.700
It's really cool to use the idea of having


00:55:09.700 --> 00:55:12.520
the source control tell you what the changes are


00:55:12.520 --> 00:55:13.900
since your last commit.


00:55:13.900 --> 00:55:17.140
But then this is just applying it to the test files, I think.


00:55:17.140 --> 00:55:18.980
But if it could say, well, now we use coverage


00:55:18.980 --> 00:55:21.180
to figure out these tests, that would be awesome.


00:55:21.180 --> 00:55:23.820
- Yeah, and I regret that I can't remember the name of it.


00:55:23.820 --> 00:55:27.580
There is a tool that does a type of test suite reduction


00:55:27.580 --> 00:55:29.640
as a pytest plugin.


00:55:29.640 --> 00:55:31.660
And maybe I'll look it up after the show


00:55:31.660 --> 00:55:33.580
and we can include it in the show notes.


00:55:33.580 --> 00:55:35.980
Of course, the thing that you've got to be careful about


00:55:35.980 --> 00:55:38.600
is that there could be dependencies


00:55:38.600 --> 00:55:42.120
between program components that are not evidenced


00:55:42.120 --> 00:55:44.680
in the source code or the cover relationship,


00:55:44.680 --> 00:55:47.600
but maybe by access to external resources.


00:55:47.600 --> 00:55:48.840
And so in those cases,


00:55:48.840 --> 00:55:51.560
the selection process may not work as intended.


00:55:51.560 --> 00:55:53.640
- Right, this thing changed something in the database.


00:55:53.640 --> 00:55:57.180
Some other part of the code read it and that makes it crash.


00:55:57.180 --> 00:55:59.080
You didn't actually touch that code over there.


00:55:59.080 --> 00:55:59.920
Something like that.


00:55:59.920 --> 00:56:00.760
- Yeah, absolutely.


00:56:00.760 --> 00:56:02.240
- Yeah, interesting.


00:56:02.240 --> 00:56:04.800
Maybe one more, I don't know too much about this,


00:56:04.800 --> 00:56:05.840
but Bill points out, says,


00:56:05.840 --> 00:56:08.160
I remember Anthony, bracket, Sotili,


00:56:08.160 --> 00:56:11.800
had worked on a tool to detect test pollution,


00:56:11.800 --> 00:56:13.600
which is a kind of related topic.


00:56:13.600 --> 00:56:17.400
And it says a test pollution is where a test fails


00:56:17.400 --> 00:56:20.920
due to the side effects of some other test in the suite.


00:56:20.920 --> 00:56:22.200
And that's pretty interesting.


00:56:22.200 --> 00:56:24.960
So maybe that's worth something for people to look at.


00:56:24.960 --> 00:56:25.800
Have you heard of this?


00:56:25.800 --> 00:56:26.620
I haven't heard of this before,


00:56:26.620 --> 00:56:28.960
so I can't speak too much about it.


00:56:28.960 --> 00:56:31.520
- I've not heard of this specific tool,


00:56:31.520 --> 00:56:35.720
but I've seen it done in Java as a Java project.


00:56:35.720 --> 00:56:38.760
And yeah, you can do it fairly successfully


00:56:38.760 --> 00:56:40.280
and you can go quite deep with it as well.


00:56:40.280 --> 00:56:43.880
I mean, it's hard to see exactly how this one works just based on the description.


00:56:43.880 --> 00:56:47.660
But I mean, I think there was an example where it had a global variable.


00:56:47.660 --> 00:56:50.940
But I mean, obviously that's quite a trivial example.


00:56:50.940 --> 00:56:54.640
But I mean, you can get state pollution in ways you really wouldn't expect it.


00:56:54.640 --> 00:57:00.360
So for example, I've seen a test where two tests that were dependent on each other were


00:57:00.360 --> 00:57:04.000
individual parameterizations of the same test.


00:57:04.000 --> 00:57:11.000
is a dependency because in the parameterization decorator, someone had used a list object


00:57:11.000 --> 00:57:16.240
as an argument. And then in the test, they've modified that list, but then the list isn't


00:57:16.240 --> 00:57:20.720
then recreated for the next test. So then the next test gets... But that's not a global


00:57:20.720 --> 00:57:24.720
variable that's just sort of created when that file is executed.


00:57:24.720 --> 00:57:29.280
Yeah. A weird Python default value behavior. Yeah, that is...


00:57:29.280 --> 00:57:33.880
Yeah, it's quite... I've seen people complain about that quite a lot. So like when you have


00:57:33.880 --> 00:57:37.720
a function and you put a list or a mutable object as like a default


00:57:37.720 --> 00:57:40.000
argument, that's quite a common gotcha.


00:57:40.000 --> 00:57:41.360
So it's a similar kind of thing.


00:57:41.360 --> 00:57:41.540
Yeah.


00:57:41.540 --> 00:57:44.600
And it's, if you run tools, like I talked about linting, if you run tools


00:57:44.600 --> 00:57:48.100
like rough or others that will, you know, flake eight, those types of things,


00:57:48.100 --> 00:57:51.240
you know, many of them will warn you like, no, this is a bad idea.


00:57:51.240 --> 00:57:51.940
Don't do that.


00:57:51.940 --> 00:57:56.880
So maybe running, I would imagine running tools like rough and other


00:57:56.880 --> 00:57:58.560
linters that detect these issues.


00:57:58.560 --> 00:58:03.000
Might actually reduce the test flakiness by finding some of these, you know,


00:58:03.040 --> 00:58:05.320
anti-patterns that you maybe didn't catch.


00:58:05.320 --> 00:58:06.520
>> Yeah, it may well do, yeah.


00:58:06.520 --> 00:58:09.880
>> I think another thing that's important to note when we're talking about a linter


00:58:09.880 --> 00:58:16.120
like Ruff is that it's so fast to run that there's not really a big cost from a developer's


00:58:16.120 --> 00:58:17.120
perspective.


00:58:17.120 --> 00:58:18.120
>> Yeah, it's nearly instant.


00:58:18.120 --> 00:58:19.120
>> Yeah.


00:58:19.120 --> 00:58:25.000
Again, integrate it early, use it regularly, have it in your IDE, use it in CI, and it's


00:58:25.000 --> 00:58:30.280
one of those things where it might help you to avoid certain coding practices that would


00:58:30.280 --> 00:58:33.520
ultimately lead to test flakiness creeping into your system.


00:58:33.520 --> 00:58:33.780
Yeah.


00:58:33.780 --> 00:58:34.580
Really good advice.


00:58:34.580 --> 00:58:36.600
It is so fast.


00:58:36.600 --> 00:58:40.460
I ran it against, like I said, 20,000 lines of Python and it went, it just,


00:58:40.460 --> 00:58:42.400
it looked like it didn't even do anything.


00:58:42.400 --> 00:58:45.020
I thought, oh, I didn't do it right because nothing happened.


00:58:45.020 --> 00:58:49.680
You know, but it's so quick and you can put it, there's plugins for both


00:58:49.680 --> 00:58:51.980
a PyCharm and VS Code that'll just run it.


00:58:51.980 --> 00:58:55.940
And PyCharm even integrates it into its code fixes and all of its, its


00:58:55.940 --> 00:58:59.040
behaviors and just reformat code options and stuff.


00:58:59.040 --> 00:58:59.620
It's really good.


00:58:59.680 --> 00:59:03.040
I've been using Ruff recently and I really like it as well.


00:59:03.040 --> 00:59:07.200
Along with the point that you mentioned, I like the fact that you can configure it through the


00:59:07.200 --> 00:59:12.320
PyProject Tomo file, which is where I'm already putting all of my other configurations.


00:59:12.320 --> 00:59:12.640
Yeah.


00:59:12.640 --> 00:59:18.880
And then it also essentially can serve as a language server protocol implementation.


00:59:18.880 --> 00:59:22.640
So even if you don't use the two text editors that you mentioned,


00:59:22.640 --> 00:59:26.080
you can still get all of the code actions and fixes.


00:59:26.080 --> 00:59:30.640
And because it's so fast, it's really easy to use it even on big code basis.


00:59:30.640 --> 00:59:33.840
Okay, one more really quick, because I think this is a good suggestion.


00:59:33.840 --> 00:59:38.240
And this goes back to how I talked about, you know, like the call is coming from inside the


00:59:38.240 --> 00:59:43.440
house type of problem in that the error could actually be with the test framework and the


00:59:43.440 --> 00:59:49.280
test code, not just not actually your code. So Marwan points out that scoping fixtures


00:59:49.280 --> 00:59:54.400
incorrectly could be another source of flakiness. So the fixture could say,


00:59:54.400 --> 00:59:58.320
create a generator and pass it over to you. But you could say this is a class-based one


00:59:58.320 --> 01:00:04.080
instead of a test instance one. And then you get different results depending on the order


01:00:04.080 --> 01:00:07.040
and all these kinds of things, right? That's really interesting, I think.


01:00:07.040 --> 01:00:11.280
I would agree. I think that's a really good point. The other thing that I sometimes need


01:00:11.280 --> 01:00:18.160
to be very careful about is having auto-use test fixtures inside of my code, because then those


01:00:18.160 --> 01:00:23.580
Those might be applied everywhere along with other decorators that are fixtures that are


01:00:23.580 --> 01:00:25.660
just applied selectively.


01:00:25.660 --> 01:00:30.740
And then I might get a kind of non-determinant process just because of the way that various


01:00:30.740 --> 01:00:34.060
text fixtures are applied or the order in which they're applied.


01:00:34.060 --> 01:00:35.060
Absolutely.


01:00:35.060 --> 01:00:36.960
All right, Owen, last word on this.


01:00:36.960 --> 01:00:42.060
The other plugin that might be interesting is pytest xdist or running these distributed.


01:00:42.060 --> 01:00:45.100
Like, what do you think of, does that help or hurt us here?


01:00:45.100 --> 01:00:50.420
Running your tests in parallel can be obviously a great way to expose concurrency related


01:00:50.420 --> 01:00:55.080
flakiness because as you said before, when you're writing the test, you might be writing


01:00:55.080 --> 01:00:59.540
it under the assumption that you're the only one accessing certain resources or running


01:00:59.540 --> 01:01:01.020
it at a certain time.


01:01:01.020 --> 01:01:06.400
Another thing that something like this can do as well is it can also expose order dependent


01:01:06.400 --> 01:01:12.560
tests because, so the way it will work is it will create, say you're wanting to run


01:01:12.560 --> 01:01:19.520
eight tests at a time, this plugin will then create eight separate processes. But within


01:01:19.520 --> 01:01:24.880
those processes, each one has its own independent Python interpreter. So they're running independently


01:01:24.880 --> 01:01:31.160
of each other. But then you could also, by doing that, expose a test case that was expecting


01:01:31.160 --> 01:01:36.080
a previous test to run, but now isn't because it's running in a different process. And that


01:01:36.080 --> 01:01:40.640
test could then go on to fail. So that would then be another issue of inadequate setup


01:01:40.640 --> 01:01:41.640
from that test.


01:01:41.640 --> 01:01:43.360
- Yeah, this is something I should probably be running


01:01:43.360 --> 01:01:45.600
more of as well, like why not?


01:01:45.600 --> 01:01:47.400
I have 10 cores on this computer.


01:01:47.400 --> 01:01:49.200
Why don't I just have my tests run faster?


01:01:49.200 --> 01:01:51.840
Probably not 10 times faster, but it could do more


01:01:51.840 --> 01:01:54.320
than just running one thread in serial.


01:01:54.320 --> 01:01:55.200
- You could do, yeah.


01:01:55.200 --> 01:01:56.600
- But certainly running them in parallel


01:01:56.600 --> 01:01:59.640
would certainly pull up some of those ordering issues


01:01:59.640 --> 01:02:01.840
as well as resource contention issues.


01:02:01.840 --> 01:02:03.920
- Yeah, so as well as providing a speed up,


01:02:03.920 --> 01:02:06.480
it's also great because it exposed some problems


01:02:06.480 --> 01:02:07.560
in your test suite as well.


01:02:07.560 --> 01:02:08.400
- Absolutely.


01:02:08.400 --> 01:02:10.600
All right, guys, I think we're gonna have to leave it there


01:02:10.600 --> 01:02:13.460
for the time we got, but excellent stuff.


01:02:13.460 --> 01:02:15.440
And there's a lot of detail here, isn't there?


01:02:15.440 --> 01:02:16.280
As you dig into it.


01:02:16.280 --> 01:02:18.080
- Yeah, I think flaky tests are something


01:02:18.080 --> 01:02:21.440
that all of us as developers have encountered.


01:02:21.440 --> 01:02:24.960
We recognize that they limit us as developers,


01:02:24.960 --> 01:02:26.440
but also there's something that


01:02:26.440 --> 01:02:30.200
if we can automatically detect them or mitigate them


01:02:30.200 --> 01:02:33.820
in some way, we can remove that hassle from developers.


01:02:33.820 --> 01:02:35.440
So I think what we would like to do,


01:02:35.440 --> 01:02:37.520
both as researchers and developers,


01:02:37.520 --> 01:02:40.880
is allow people who write pytest test suites


01:02:40.880 --> 01:02:45.080
to be more productive and to write better tests


01:02:45.080 --> 01:02:46.160
that are less flaky.


01:02:46.160 --> 01:02:46.980
- Excellent.


01:02:46.980 --> 01:02:48.840
All right, before we wrap up the show,


01:02:48.840 --> 01:02:51.280
I'll just ask you one quick question


01:02:51.280 --> 01:02:52.400
I usually do at the end,


01:02:52.400 --> 01:02:56.600
and that is you've got a flaky test related project


01:02:56.600 --> 01:02:58.960
on PyPI, some library, some package


01:02:58.960 --> 01:03:00.380
you wanna recommend to people,


01:03:00.380 --> 01:03:02.860
or it could be something other than flaky related,


01:03:02.860 --> 01:03:04.160
but something you wanna recommend,


01:03:04.160 --> 01:03:05.680
some package you've come across lately.


01:03:05.680 --> 01:03:07.760
I was actually going to recommend something


01:03:07.760 --> 01:03:10.320
that's not connected to flaky test cases.


01:03:10.320 --> 01:03:11.160
- Go for it.


01:03:11.160 --> 01:03:12.380
- So a lot of the work that I do


01:03:12.380 --> 01:03:15.480
involves various types of processing


01:03:15.480 --> 01:03:19.300
of the abstract syntax tree of a Python program.


01:03:19.300 --> 01:03:22.640
And so I thought I might first call out the AST package


01:03:22.640 --> 01:03:25.960
that's actually a part of Python,


01:03:25.960 --> 01:03:29.880
which is built in and an incredibly useful tool,


01:03:29.880 --> 01:03:33.240
which isn't available in a lot of programming languages.


01:03:33.240 --> 01:03:36.120
The other two packages which are on PyPI,


01:03:36.120 --> 01:03:39.340
which I'll share is number one, libcst,


01:03:39.340 --> 01:03:41.220
which implements something that's called


01:03:41.220 --> 01:03:43.480
a concrete syntax tree.


01:03:43.480 --> 01:03:46.440
And it's a super useful tool when you want to be able


01:03:46.440 --> 01:03:49.420
to make changes to Python code


01:03:49.420 --> 01:03:52.000
or detect patterns in Python code.


01:03:52.000 --> 01:03:55.340
And you want to be able to fully preserve things


01:03:55.340 --> 01:03:57.680
like the blank space in the code


01:03:57.680 --> 01:03:59.560
and the comment strings in the code


01:03:59.560 --> 01:04:00.720
and things of that nature.


01:04:00.720 --> 01:04:04.760
Lib CST is actually the foundation for another tool


01:04:04.760 --> 01:04:06.400
which is called Fixit.


01:04:06.400 --> 01:04:09.920
And Fixit is a little bit like Ruff,


01:04:09.920 --> 01:04:12.800
except that it allows you to very easily


01:04:12.800 --> 01:04:15.120
write your own linting rules.


01:04:15.120 --> 01:04:17.320
And then finally, the last thing that I would share


01:04:17.320 --> 01:04:19.160
on this same theme, Michael,


01:04:19.160 --> 01:04:22.120
is that there's a really fun to use tool


01:04:22.120 --> 01:04:26.640
by someone who is a core member of the Django project,


01:04:26.640 --> 01:04:29.960
and it's called PyAST-Grep.


01:04:29.960 --> 01:04:33.800
And it actually lets you write XPath expressions.


01:04:33.800 --> 01:04:36.840
And then you can use those XPath expressions


01:04:36.840 --> 01:04:40.600
to essentially query the abstract syntax tree


01:04:40.600 --> 01:04:41.800
of your Python program.


01:04:41.800 --> 01:04:42.880
- Incredible, okay.


01:04:42.880 --> 01:04:45.760
Looks like, I guess syntax trees


01:04:45.760 --> 01:04:47.440
are a little bit like XML, aren't they?


01:04:47.440 --> 01:04:48.720
Okay.


01:04:48.720 --> 01:04:50.840
- And if anybody has to do work


01:04:50.840 --> 01:04:54.180
where they're building an automated refactoring tool,


01:04:54.180 --> 01:04:56.760
or they're building a new linting tool,


01:04:56.760 --> 01:04:59.620
or various types of program analysis tools,


01:04:59.620 --> 01:05:01.940
The packages that I've mentioned might be very helpful.


01:05:01.940 --> 01:05:03.340
Thank you, that was a bunch of good ones.


01:05:03.340 --> 01:05:05.060
Owen, you got anything you want to give a shout out to?


01:05:05.060 --> 01:05:09.340
That's actually a bit spooky because I was also about to recommend libcst as well.


01:05:09.340 --> 01:05:15.180
So one small library I've used a few times is radon.


01:05:15.180 --> 01:05:19.220
So that's R-A-D-O-N, I believe it's spelled.


01:05:19.220 --> 01:05:23.300
So this will basically calculate a load of code metrics for you.


01:05:23.300 --> 01:05:24.300
Oh, nice. Okay.


01:05:24.300 --> 01:05:26.860
So these are from relatively simple things like


01:05:26.860 --> 01:05:32.020
number of lines while taking into account comments and that kind of things to more


01:05:32.020 --> 01:05:33.020
complex metrics.


01:05:33.020 --> 01:05:37.260
So there's this maintainability index, which is basically like a weighted sum of a bunch


01:05:37.260 --> 01:05:38.900
of other code metrics.


01:05:38.900 --> 01:05:39.900
I really like that one.


01:05:39.900 --> 01:05:40.900
Yeah.


01:05:40.900 --> 01:05:44.580
It's like it combines and says, well, it's like complexity is this line length is that


01:05:44.580 --> 01:05:46.220
function length, like all that kind of stuff.


01:05:46.220 --> 01:05:47.220
Right.


01:05:47.220 --> 01:05:51.140
And I've actually found sort of empirically there is appears to be some correlation in


01:05:51.140 --> 01:05:55.200
in some cases to between having a high,


01:05:55.200 --> 01:05:56.980
sorry, or having a poor maintainability


01:05:56.980 --> 01:05:58.600
to having very complex code,


01:05:58.600 --> 01:06:00.420
having very complex test case code


01:06:00.420 --> 01:06:02.180
and that test case actually being flaky,


01:06:02.180 --> 01:06:03.020
which is interesting.


01:06:03.020 --> 01:06:03.840
- Yeah, I can believe it.


01:06:03.840 --> 01:06:05.160
Okay, that's also a cool,


01:06:05.160 --> 01:06:06.940
I hadn't heard of Radon, that's neat.


01:06:06.940 --> 01:06:08.940
All right, guys, thank you for being on the show.


01:06:08.940 --> 01:06:11.500
It's been a super interesting conversation.


01:06:11.500 --> 01:06:12.560
Final call to action,


01:06:12.560 --> 01:06:16.020
people either have flaky tests and want to get out of them


01:06:16.020 --> 01:06:18.660
or they want to avoid having them in the first place.


01:06:18.660 --> 01:06:19.900
What do you tell them?


01:06:19.900 --> 01:06:20.900
What are your parting thoughts?


01:06:20.900 --> 01:06:23.060
So my quick parting thought is as follows.


01:06:23.060 --> 01:06:26.620
We'll have some links in the show notes to various papers


01:06:26.620 --> 01:06:30.940
and tools that Owen and our colleagues and I have developed.


01:06:30.940 --> 01:06:33.220
And we hope that people will try them out.


01:06:33.220 --> 01:06:36.160
It would also be awesome if people can get in contact


01:06:36.160 --> 01:06:40.260
with us and share some of their flaky test case war stories.


01:06:40.260 --> 01:06:43.040
We would love to learn from you and partner with you


01:06:43.040 --> 01:06:45.940
to help you solve some of the flaky test case challenges


01:06:45.940 --> 01:06:46.780
that you have.


01:06:46.780 --> 01:06:48.100
Owen, what else do you want to add?


01:06:48.100 --> 01:06:49.280
- I think that's pretty much it for me.


01:06:49.280 --> 01:06:53.800
I'd say probably most important thing to do would just be to stick with testing.


01:06:53.800 --> 01:06:58.000
Don't let flaky tests put you off test-driven development or anything like that,


01:06:58.000 --> 01:07:00.080
because it's better than not testing.


01:07:00.080 --> 01:07:02.480
Yeah, indeed. All right. Well, thanks, guys. Thanks for being on the show.


01:07:02.480 --> 01:07:03.120
Thank you.


01:07:03.120 --> 01:07:04.120
Thank you.


01:07:04.120 --> 01:07:07.120
This has been another episode of Talk Python to Me.


01:07:07.120 --> 01:07:09.120
Thank you to our sponsors.


01:07:09.120 --> 01:07:12.120
Be sure to check out what they're offering. It really helps support the show.


01:07:12.120 --> 01:07:17.120
The folks over at JetBrains encourage you to get work done with PyCharm.


01:07:17.120 --> 01:07:22.120
PyCharm Professional understands complex projects across multiple languages and technologies,


01:07:22.120 --> 01:07:28.120
so you can stay productive while you're writing Python code and other code like HTML or SQL.


01:07:28.120 --> 01:07:33.120
Download your free trial at talkpython.fm/donewithpycharm.


01:07:33.120 --> 01:07:37.920
Take some stress out of your life. Get notified immediately about errors and


01:07:37.920 --> 01:07:42.360
performance issues in your web or mobile applications with Sentry. Just visit


01:07:42.360 --> 01:07:47.480
talkpython.fm/sentry and get started for free. And be sure to use the


01:07:47.480 --> 01:07:52.600
promo code "talkpython" all one word. Want to level up your Python? We have one of the


01:07:52.600 --> 01:07:57.000
largest catalogs of Python video courses over at Talk Python. Our content ranges


01:07:57.000 --> 01:08:01.600
from true beginners to deeply advanced topics like memory and async. And best of


01:08:01.600 --> 01:08:06.640
all, there's not a subscription in sight. Check it out for yourself at training.talkpython.fm.


01:08:06.640 --> 01:08:11.280
Be sure to subscribe to the show, open your favorite podcast app, and search for Python.


01:08:11.280 --> 01:08:16.560
We should be right at the top. You can also find the iTunes feed at /iTunes, the Google Play feed


01:08:16.560 --> 01:08:24.080
at /play, and the Direct RSS feed at /rss on talkpython.fm. We're live streaming most of our


01:08:24.080 --> 01:08:28.240
recordings these days. If you want to be part of the show and have your comments featured on the


01:08:28.240 --> 01:08:34.240
air, be sure to subscribe to our YouTube channel at talkpython.fm/youtube. This is your host,


01:08:34.240 --> 01:08:38.160
Michael Kennedy. Thanks so much for listening. I really appreciate it. Now get out there and


01:08:38.160 --> 01:08:39.520
write some Python code.


01:08:39.520 --> 01:08:42.880
[MUSIC PLAYING]


01:08:42.880 --> 01:08:57.880
[Music]


01:08:57.880 --> 01:09:07.880
[BLANK_AUDIO]

