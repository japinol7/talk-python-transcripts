WEBVTT

00:00:00.040 --> 00:00:04.000
When you run your code in the cloud, how much do you know about where it runs?

00:00:04.000 --> 00:00:09.300
I mean the hardware it runs on and the data center it runs in. There are just a

00:00:09.300 --> 00:00:13.040
couple of hyperscale cloud providers in the world. This episode is a unique

00:00:13.040 --> 00:00:18.320
chance to get a deep look inside one of them, Microsoft Azure. Azure is comprised

00:00:18.320 --> 00:00:23.520
of over 200 physical data centers with hundreds of thousands of servers in each

00:00:23.520 --> 00:00:28.760
one of those. A look at how code runs on them is fascinating. Our guide for this

00:00:28.760 --> 00:00:33.480
journey will be Mark Russinovich. Mark is the CTO of Microsoft Azure and a

00:00:33.480 --> 00:00:38.000
technical fellow, Microsoft's senior most technical position. He's also a bit of a

00:00:38.000 --> 00:00:41.680
programming hero of mine. Even if you don't host your code in the cloud, I

00:00:41.680 --> 00:00:45.920
think you'll enjoy this conversation. Let's dive in. This is Talk Python to Me

00:00:45.920 --> 00:00:55.200
episode 445 recorded on-site at Microsoft Ignite in Seattle, November 16th, 2023.

00:00:55.200 --> 00:01:06.920
[Music]

00:01:06.920 --> 00:01:11.640
Welcome to Talk Python to Me, a weekly podcast on Python. This is your host

00:01:11.640 --> 00:01:15.720
Michael Kennedy. Follow me on Mastodon where I'm @mkennedy and follow the

00:01:15.720 --> 00:01:21.200
podcast using @talkpython both on fosstodon.org. Keep up with the show and

00:01:21.200 --> 00:01:26.160
listen to over seven years of past episodes at talkpython.fm. We've started

00:01:26.160 --> 00:01:30.360
streaming most of our episodes live on YouTube. Subscribe to our YouTube channel

00:01:30.360 --> 00:01:35.440
over at talkpython.fm/youtube to get notified about upcoming shows and be

00:01:35.440 --> 00:01:40.960
part of that episode. This episode is sponsored by Posit Connect from the

00:01:40.960 --> 00:01:44.880
makers of Shiny. Publish, share, and deploy all of your data projects that you're

00:01:44.880 --> 00:01:50.400
creating using Python. Streamlit, Dash, Shiny, Bokeh, FastAPI, Flask, Reports,

00:01:50.400 --> 00:01:55.400
Dashboards, and APIs. Posit Connect supports all of them. Try Posit Connect

00:01:55.400 --> 00:02:01.760
for free by going to talkpython.fm/posit. P-O-S-I-T. And it's brought to you by the

00:02:01.760 --> 00:02:05.920
PyBites Developer Mindset Program. PyBytes core mission is to help you break

00:02:05.920 --> 00:02:09.480
the vicious cycle of tutorial paralysis through developing real-world

00:02:09.480 --> 00:02:13.240
applications. The PyBites Developer Mindset Program will help you build the

00:02:13.240 --> 00:02:17.440
confidence you need to become a highly effective developer. Check it out at

00:02:17.440 --> 00:02:24.200
talkpython.fm/pdm. Mark, welcome to Talk Python. Thanks. Thanks, Michael.

00:02:24.200 --> 00:02:28.240
Yeah, it's fantastic to have you here. I've been a fan of your work for a really long

00:02:28.240 --> 00:02:32.520
time. We're gonna have a really cool look inside of Azure and there's not very

00:02:32.520 --> 00:02:36.200
many hyperscale clouds in the world. You can probably count them on your hands, right?

00:02:36.200 --> 00:02:41.600
And so I think as developers, Python developers generally, will be really

00:02:41.600 --> 00:02:45.280
interested to just kind of get a sense of when we run on the cloud, what exactly

00:02:45.280 --> 00:02:49.280
does that mean because it's been a journey. Yeah, for sure. Before we dive

00:02:49.280 --> 00:02:52.880
into that though and some other cool things you're up to, tell people quick

00:02:52.880 --> 00:02:57.200
about it, a bit about yourself. Sure. I'm CTO and Technical Fellow at Microsoft,

00:02:57.200 --> 00:03:01.080
CTO of Azure. I've been in Azure since 2010. Prior to that, I was in Windows. I

00:03:01.080 --> 00:03:04.960
joined Microsoft in 2006 when it acquired my software company and my

00:03:04.960 --> 00:03:10.880
freeware website, Winternals and Sysinternals, respectively. And since 2010,

00:03:10.880 --> 00:03:14.000
I've effectively been in the same role the entire time, which is overseeing

00:03:14.000 --> 00:03:17.440
technical strategy and architecture for the Azure platform. And the skill of that

00:03:17.440 --> 00:03:21.200
is quite something. So it'll be great to get in that. That's awesome.

00:03:21.200 --> 00:03:27.520
You first came on my radar probably in the late 90s, early aughts, through the

00:03:27.520 --> 00:03:31.440
Sysinternals thing, not through Microsoft. You brought that up. So tell people a bit

00:03:31.440 --> 00:03:35.480
about Sysinternals. It was like if you wanted to see inside your, what your app

00:03:35.480 --> 00:03:38.760
is doing on Windows, you go to Sysinternals, right? Tell us about that.

00:03:38.760 --> 00:03:43.080
Sysinternals grew out of my just love of understanding the way things work and I

00:03:43.080 --> 00:03:48.680
was doing a lot of work on Windows internals actually in my PhD program

00:03:48.680 --> 00:03:53.080
where I was trying to figure out how to get operating systems to be able to save

00:03:53.080 --> 00:03:57.120
their state and then come back in case of a failure. So I learned the internals

00:03:57.120 --> 00:04:03.120
of Windows 3.1 and then Windows 95 and then Windows NT and started to think

00:04:03.120 --> 00:04:06.680
about cool ways that I could understand the way things worked underneath the

00:04:06.680 --> 00:04:10.920
hood. So actually the first Sysinternals tool was something called Control2Cap,

00:04:10.920 --> 00:04:15.640
which swaps the caps lock and key, you know, the control key because I came from

00:04:15.640 --> 00:04:19.720
a Unix background. And who needs caps lock? Yeah, exactly. No one should be that.

00:04:19.720 --> 00:04:26.160
I'm not yelling at people very much. So the second tool that I wrote was actually

00:04:26.160 --> 00:04:31.520
called NTFS DOS to bring NTFS to DOS, but the native Windows NT tools that

00:04:31.520 --> 00:04:35.760
Bryce Cogswell, who I met in grad school, and I co-wrote together were RegMon and

00:04:35.760 --> 00:04:39.440
FileMon. They were like the originals and RegMon allowed you to watch

00:04:39.440 --> 00:04:42.920
registry activity, FileMon file system activity. We later merged them into

00:04:42.920 --> 00:04:48.560
Process Monitor after we joined Microsoft. But we decided to make those

00:04:48.560 --> 00:04:53.040
tools available for free and so we started the NTinternals.com

00:04:53.040 --> 00:04:57.800
website, which then Microsoft's lawyers said don't use NT, so we switched over to

00:04:57.800 --> 00:05:02.360
renaming it Sysinternals. And then Bryce was like, hey, some of the tools that

00:05:02.360 --> 00:05:05.480
we've made we should sell. So we wrote a tool that would allow you to mount a

00:05:05.480 --> 00:05:10.280
dead NT system through a serial cable as if it was a local drive on the recovery

00:05:10.280 --> 00:05:14.160
system. And he said if we make a read/write version we should sell it. So

00:05:14.160 --> 00:05:18.760
he started, he went and set up a credit account, you know, credit card account on

00:05:18.760 --> 00:05:26.440
e-commerce.net or something like that. And we started selling the

00:05:26.440 --> 00:05:29.720
software and that grew into what became Winternals, a commercial software

00:05:29.720 --> 00:05:33.240
company. But Sysinternals and Winternals, like I said, were both acquired at the

00:05:33.240 --> 00:05:38.520
time. I joined Microsoft in 2006. But Bryce and I continued to work on

00:05:38.520 --> 00:05:43.240
Sysinternals. He worked on him until he retired from Microsoft and retired just

00:05:43.240 --> 00:05:48.000
in general four years later. And then I've continued. I have now a couple

00:05:48.000 --> 00:05:52.320
people, three people, working on Sysinternals engineering systems, keeping

00:05:52.320 --> 00:05:55.960
them healthy and the build pipelines working, and then adding features to

00:05:55.960 --> 00:05:59.680
them. And I still code on them and still add features. Like just at a Zoomit

00:05:59.680 --> 00:06:04.240
release a few days ago I added some, I blur and highlight to Zoomit. Yeah,

00:06:04.240 --> 00:06:08.200
people are doing presentations on Windows. With Zoomit you can say let me

00:06:08.200 --> 00:06:11.480
quickly draw on the screen, not PowerPoint, but just whatever happens to

00:06:11.480 --> 00:06:15.600
be on your screen, which is really nice. I love my Macs these days, but boy I wish

00:06:15.600 --> 00:06:19.680
Zoomit existed on the Mac. Actually, people have asked for Zoomit for Mac and

00:06:19.680 --> 00:06:24.280
I'd like to make a Zoomit for Mac. And now with Copilot, I wonder how good

00:06:24.280 --> 00:06:28.240
it is at writing Mac apps because I don't want to spend all the time to

00:06:28.240 --> 00:06:31.960
learn how to write a Mac apps just to write Zoomit, but if Copilot can help

00:06:31.960 --> 00:06:35.200
maybe it'll, you know, something that I can do in my, you know, you know, spare time.

00:06:35.200 --> 00:06:39.080
I don't know if it'll do it, but it'll get you close. It's crazy how these LLMs

00:06:39.080 --> 00:06:42.160
are writing code for us these days. And we're gonna talk a bit about maybe how

00:06:42.160 --> 00:06:47.920
some of those run. Yeah. And so on, I mean, Azure is doing tons of stuff with large

00:06:47.920 --> 00:06:50.880
language models and you all have some, you know, we're here at the Microsoft

00:06:50.880 --> 00:06:55.080
Ignite conference, you've got some big announcements. But yeah, I was such a fan

00:06:55.080 --> 00:07:00.640
still of Sysmon and I use that for all sorts of things still. So, super cool.

00:07:00.640 --> 00:07:04.480
Now, before we jump in, I kind of want to talk about some of your big announcements.

00:07:04.480 --> 00:07:08.600
Yeah. Because they really caught me off guard here. I'm like, yes, this is exciting.

00:07:08.600 --> 00:07:12.280
But maybe since we're gonna talk a decent amount about Azure, the internals

00:07:12.280 --> 00:07:16.200
of hardware, how our code runs, just give us a quick history of Azure. You know, when

00:07:16.200 --> 00:07:19.400
did this whole thing get started? Yeah, Azure started right as I joined

00:07:19.400 --> 00:07:22.840
Microsoft in 2006. There was a group of people, including Dave Cutler, one of the

00:07:22.840 --> 00:07:26.600
people that I've looked up to because Dave was the original architect behind

00:07:26.600 --> 00:07:31.520
the VMS operating system and then Windows NT, which is now underlying

00:07:31.520 --> 00:07:37.800
Windows. He and some other people were just at the suggestion of Ray Ozzie. This

00:07:37.800 --> 00:07:44.160
is back when services was a big thing. And Ray sent a memo to the company,

00:07:44.160 --> 00:07:47.920
kind of echoing Bill Gates's internet memo, saying it's software and services

00:07:47.920 --> 00:07:52.480
now. And they said, how can we build a data center scale type platform to make

00:07:52.480 --> 00:07:57.120
it easier for Microsoft to develop services? And so this was called Project

00:07:57.120 --> 00:08:01.680
Red Dog, which was incubating for a while. And then 2008, they publicly launched it

00:08:01.680 --> 00:08:05.280
because Steve said, we need to make this available to third parties as well as

00:08:05.280 --> 00:08:11.040
Windows Azure. And, sorry, 2008 they announced the preview of it. 2010, it

00:08:11.040 --> 00:08:16.320
commercially launched publicly in February, and I joined in July. And a few

00:08:16.320 --> 00:08:19.560
years later, with the rise of open source software and so many enterprise

00:08:19.560 --> 00:08:25.000
customers wanting to have Linux, we re-branded it Microsoft Azure. And we

00:08:25.000 --> 00:08:28.680
also... Can I run Linux on Windows Azure? I don't know if that makes any sense. Yeah, and one of the

00:08:28.680 --> 00:08:35.240
first things I'd done, worked on with Corey Sanders, was being asked, hey,

00:08:35.240 --> 00:08:38.480
we've got platform as a service. We have this thing called cloud services, this

00:08:38.480 --> 00:08:42.160
model for how you write apps. But our enterprise customers were saying, I can't

00:08:42.160 --> 00:08:49.160
move my existing IT stuff to Azure because it just needs VMs. And so

00:08:49.160 --> 00:08:54.680
the first thing we did was, hey, we should get IaaS capability in Azure. And so in

00:08:54.680 --> 00:08:58.400
2012, we launched the preview of IaaS for Azure. And that's really when the

00:08:58.400 --> 00:09:01.840
business started to take off because enterprises then could, with minimal

00:09:01.840 --> 00:09:07.080
effort, start to move. Well, that's like doing what we do in our data center, but in your data center.

00:09:07.080 --> 00:09:12.200
Yeah, exactly. Now no one even thinks about it. Exactly. So IaaS has continued

00:09:12.200 --> 00:09:16.040
to evolve, PaaS has continued to evolve. Cloud services was designed in a world

00:09:16.040 --> 00:09:19.760
without containers. Now we've got containerization, the rise of Kubernetes,

00:09:19.760 --> 00:09:23.720
and then application models on top of containers. And so Azure's evolved and

00:09:23.720 --> 00:09:27.600
actually, I think, led some of that evolution of cloud-native computing up

00:09:27.600 --> 00:09:31.920
into containers and abstractions. But it's been a long, long journey towards

00:09:31.920 --> 00:09:35.840
that. I mean, I think one of the things is I've always believed that ultimately

00:09:35.840 --> 00:09:39.680
cloud should be about making it easy for developers to say, here's what I want, and

00:09:39.680 --> 00:09:43.800
then the cloud takes care of the rest. And we're moving towards it relentlessly.

00:09:43.800 --> 00:09:48.480
That time when you'll really be able to do that. Yeah, so you don't have to

00:09:48.480 --> 00:09:53.240
know DevOps, you don't have to know distributed architectures, you just give

00:09:53.240 --> 00:09:58.480
you guys the code. Yeah, which is beautiful. It's beautiful. Now, real quickly, just

00:09:58.480 --> 00:10:03.160
give us a scale. Like, think of how many data centers, how many servers, how many

00:10:03.160 --> 00:10:08.680
miles, fiber. It's kind of astonishing. It is pretty flabbergasting. And the numbers

00:10:08.680 --> 00:10:13.320
continue to grow exponentially. I'll just give you, because I remember when I

00:10:13.320 --> 00:10:18.360
first started in Azure, I was asked to give a talk at the Azure All Hands about

00:10:18.360 --> 00:10:21.960
architecture and some of the announcements we had coming. And the All

00:10:21.960 --> 00:10:27.440
Hands was two rooms with the partition removed in our on-campus

00:10:27.440 --> 00:10:32.720
conference room meeting center. A total of about 500 people. That was

00:10:32.720 --> 00:10:37.240
all of the Azure team in 2010. And really nobody outside the Azure team knew

00:10:37.240 --> 00:10:40.120
anything about Azure. Really, the world didn't know about Azure.

00:10:40.120 --> 00:10:44.640
Inside, right? Yeah, inside. So effectively, that was like most, at least half the people in the

00:10:44.640 --> 00:10:48.280
world that knew anything about Azure was in those two rooms. And today,

00:10:48.280 --> 00:10:52.360
you know, Scott Guthrie's organization, Cloud and AI, all of it's working on

00:10:52.360 --> 00:10:57.920
Azure. And that's tens of thousands of people. So at least, you know, a good

00:10:57.920 --> 00:11:02.880
percentage, majority percentage even, of the company is working directly on

00:11:02.880 --> 00:11:06.280
things that come under the Azure umbrella. So it's come a long way from

00:11:06.280 --> 00:11:09.920
that perspective. And you talked about physical scale. Back then, when we

00:11:09.920 --> 00:11:15.320
originally launched Azure in two regions, it was like 40,000 servers. Like 20,000

00:11:15.320 --> 00:11:19.640
in one, 20,000 in the other. That's a lot of servers. Yeah, but back, but you know, it was

00:11:19.640 --> 00:11:25.000
like, let's, that is kind of cloud scale back then. Now, we are at millions

00:11:25.000 --> 00:11:30.160
of servers. And when it comes to data centers, we've got 60 regions around the

00:11:30.160 --> 00:11:36.640
world, 60 plus regions. And each of those consists of one, in many cases, multiple

00:11:36.640 --> 00:11:40.600
data centers. And we're still building out, we're launching a data, like two

00:11:40.600 --> 00:11:44.280
data centers every week, I think is the number that we're launching. Wow, that's

00:11:44.280 --> 00:11:48.080
crazy. And these could be slotted into one of these regions or it could be, yeah,

00:11:48.080 --> 00:11:52.320
a new region. Yeah, yeah. Incredible, incredible. And so the big announcement

00:11:52.320 --> 00:11:54.920
that I wanted to ask you about, just before we run out of time, then we'll

00:11:54.920 --> 00:11:59.520
dive into some of that, that sort of how does your code run story is Azure Cobalt.

00:11:59.520 --> 00:12:04.240
Yeah, that's a new processor you guys announced. And you know, listeners, I'm a

00:12:04.240 --> 00:12:08.240
big fan of Apple Silicon and how it sort of changed the computing landscape for

00:12:08.240 --> 00:12:12.480
power and speed on like little laptops and stuff. And this is kind of that idea

00:12:12.480 --> 00:12:15.440
but for the data center, right? Tell us about that. It is that idea. I think

00:12:15.440 --> 00:12:20.600
having a processor that can be designed really with our specifications. If you

00:12:20.600 --> 00:12:24.960
take a look at Intel and AMD processors, they're fantastic processors. They're

00:12:24.960 --> 00:12:28.400
very versatile. They're taking requirements from lots of different

00:12:28.400 --> 00:12:33.200
sources. And so we're just, we're a voice. We're a significant voice when it comes

00:12:33.200 --> 00:12:37.040
to saying we'd like your processors to do these things. When we have our own,

00:12:37.040 --> 00:12:40.640
we've got the ability to just decide unilaterally what we'd like it to do

00:12:40.640 --> 00:12:44.560
based off of what we see and can vertically integrate it into our systems.

00:12:44.560 --> 00:12:49.440
We can put it on SSEs and integrate it with memory and GPUs. And so that is

00:12:49.440 --> 00:12:55.080
kind of the reason that we've done that verticalization for processors.

00:12:55.080 --> 00:12:59.000
That's not to say that the other processors aren't going to be significant.

00:12:59.000 --> 00:13:01.480
It's going to be probably a blend of the offerings.

00:13:01.480 --> 00:13:04.560
They'll have different capabilities that ours won't have. There are customers that

00:13:04.560 --> 00:13:08.600
want the specific features that they've got or performance speeds and feeds that

00:13:08.600 --> 00:13:11.840
they've got because they're not all going to look the same. And so I think

00:13:11.840 --> 00:13:15.840
it's just better optionality for everybody. Well I can tell you as

00:13:15.840 --> 00:13:21.160
somebody who tries to run Linux on that thing, it's hit and miss if there's

00:13:21.160 --> 00:13:25.320
even an ARM version available. More often than not, there's not. And so there's

00:13:25.320 --> 00:13:28.640
certainly not going to be an insane rush to just drop everything because

00:13:28.640 --> 00:13:33.120
there's a lot of code that's written for x86. And optimized. That's the

00:13:33.120 --> 00:13:37.360
other thing too. Yeah, for sure. So on my list here of things I was going to

00:13:37.360 --> 00:13:40.600
ask you is, well what about ARM in the data center? Well that is ARM.

00:13:40.600 --> 00:13:44.680
Exactly. I'm like, well okay, so you guys beat me to the punch.

00:13:44.680 --> 00:13:48.640
This portion of Talk Python to Me is brought to you by Posit, the makers of

00:13:48.640 --> 00:13:54.640
Shiny, formerly RStudio, and especially Shiny for Python. Let me ask you a

00:13:54.640 --> 00:13:58.760
question. Are you building awesome things? Of course you are. You're a developer or

00:13:58.760 --> 00:14:03.180
data scientist. That's what we do. And you should check out Posit Connect. Posit

00:14:03.180 --> 00:14:07.400
Connect is a way for you to publish, share, and deploy all the data products

00:14:07.400 --> 00:14:12.360
that you're building using Python. People ask me the same question all the time.

00:14:12.360 --> 00:14:16.360
Michael, I have some cool data science project or notebook that I built. How do

00:14:16.360 --> 00:14:21.040
I share it with my users, stakeholders, teammates? Do I need to learn FastAPI or

00:14:21.040 --> 00:14:26.720
Flask or maybe Vue or React.js? Hold on now. Those are cool technologies and I'm

00:14:26.720 --> 00:14:30.440
sure you'd benefit from them, but maybe stay focused on the data project? Let

00:14:30.440 --> 00:14:34.400
Posit Connect handle that side of things. With Posit Connect you can rapidly and

00:14:34.400 --> 00:14:39.280
securely deploy the things you build in Python. Streamlit, Dash, Shiny, Bokeh,

00:14:39.280 --> 00:14:45.520
FastAPI, Flask, Quadro, Reports, Dashboards, and APIs. Posit Connect supports all of

00:14:45.520 --> 00:14:50.120
them. And Posit Connect comes with all the bells and whistles to satisfy IT and

00:14:50.120 --> 00:14:54.360
other enterprise requirements. Make deployment the easiest step in your

00:14:54.360 --> 00:14:58.240
workflow with Posit Connect. For a limited time you can try Posit Connect

00:14:58.240 --> 00:15:02.640
for free for three months by going to talkpython.fm/posit. That's

00:15:02.640 --> 00:15:08.720
talkpython.fm/posit. The link is in your podcast player show notes. Thank you to

00:15:08.720 --> 00:15:13.680
the team at Posit for supporting Talk Python. Awesome. Now one of the things

00:15:13.680 --> 00:15:17.640
I wanted to kind of maybe have you go through for listeners that I think is

00:15:17.640 --> 00:15:22.040
just super interesting is sort of the evolution of the hardware where our code

00:15:22.040 --> 00:15:25.560
runs throughout the data center. So you talked about in some of your talks

00:15:25.560 --> 00:15:29.520
like the data center generations and what is that like six or seven, eight

00:15:29.520 --> 00:15:33.280
maybe different variations. I'll kind of give you some prompts from them

00:15:33.280 --> 00:15:36.680
and all, but one of the things that I was thinking about when I was looking at this

00:15:36.680 --> 00:15:41.080
is you know do you have a bunch of small servers or do you have like or do you

00:15:41.080 --> 00:15:46.840
partition up really large servers right? What's the right flow for that?

00:15:46.840 --> 00:15:50.720
So one of the things that you've seen since the start of cloud back when we

00:15:50.720 --> 00:15:55.440
launched Azure there was one server type. Yeah. And it we had different virtual

00:15:55.440 --> 00:15:59.440
machines offerings but they were just all different sizes that could fit on

00:15:59.440 --> 00:16:04.320
that one server. It was a 32 core Dell Optron with I think 32 gig of RAM. Yeah.

00:16:04.320 --> 00:16:09.600
And so that was the server back then. What we've seen is more workloads come

00:16:09.600 --> 00:16:12.440
to the cloud that have different requirements. Some require large memory,

00:16:12.440 --> 00:16:17.040
some require more compute, some require GPUs, some require InfiniBand back-end

00:16:17.040 --> 00:16:21.200
networking for high-performance computing. And so we've there's been a

00:16:21.200 --> 00:16:27.560
drastic diversification of the server hardware in Azure that's being offered

00:16:27.560 --> 00:16:31.880
and current at any one point in time. And I think you'll continue to see that. So

00:16:31.880 --> 00:16:36.800
the old you know it's just a pizza box, it's it's a low low-end commodity

00:16:36.800 --> 00:16:41.920
server kind of that's the cloud vision back in 2010. Now it's the cloud

00:16:41.920 --> 00:16:46.680
contains specialized servers for specific applications. And when it comes

00:16:46.680 --> 00:16:52.320
to large servers back in 2014 we started to introduce very large servers. The kind

00:16:52.320 --> 00:16:56.080
that you know people that were cloud purists back in 2010 would have been

00:16:56.080 --> 00:17:00.040
like no don't allow that. It's all about the cheap. It's all about the cheap and scale

00:17:00.040 --> 00:17:05.360
out. Is scale up servers for SAP workloads in memory database workloads.

00:17:05.360 --> 00:17:11.720
So we introduced a machine that we nicknamed Godzilla which had 512 gig of

00:17:11.720 --> 00:17:17.080
RAM in 2014 which was like an astonishing number. And we've continued

00:17:17.080 --> 00:17:21.440
to as SAP workloads have gotten bigger and bigger and more has migrated to the

00:17:21.440 --> 00:17:25.160
cloud created bigger and bigger and bigger and bigger machines. In fact I'm

00:17:25.160 --> 00:17:28.640
showing here at Ignite the latest generation of the SAP scale up machines

00:17:28.640 --> 00:17:31.560
that we're offering. It's not yet they're not yet public but I'm going to show a

00:17:31.560 --> 00:17:37.360
demo of them of one of them call that I'm calling nicknaming super mega

00:17:37.360 --> 00:17:41.800
Godzilla beast because we've gone through so many iterations. So this this

00:17:41.800 --> 00:17:47.160
one is super is the new. Yeah you're running low on agitator. Yeah and I don't

00:17:47.160 --> 00:17:49.720
know what I'll come up with next. But anyway we're at super mega Godzilla

00:17:49.720 --> 00:17:55.960
beast as the current generation which has 1790 cores. Wow. And 32 terabytes of

00:17:55.960 --> 00:18:03.280
RAM. 32 terabytes of RAM. Incredible. So do you do things to like pin VMs to

00:18:03.280 --> 00:18:07.480
certain cores so that they get better cache hits and stuff like that rather

00:18:07.480 --> 00:18:11.480
than it just kind of mosh around. That's especially important with NUMA

00:18:11.480 --> 00:18:15.000
architectures where you've got memory that has different latencies to different

00:18:15.000 --> 00:18:19.640
sockets. As you want to have the VM that's using certain cores on a socket

00:18:19.640 --> 00:18:24.880
have memory that is close to it close to that socket. So that's and that's just

00:18:24.880 --> 00:18:29.720
part of Hyper-V scheduling is doing that kind of assignment which we have under

00:18:29.720 --> 00:18:33.240
the hood. And again it's like the control plane at the very top says launch a

00:18:33.240 --> 00:18:38.200
virtual machine of this size of this skew. Then there's a resource manager the

00:18:38.200 --> 00:18:41.480
Azure allocator that goes and figures out this is the best server to put that

00:18:41.480 --> 00:18:47.240
on. It has enough space and will reduce minimize fragmentation and places on

00:18:47.240 --> 00:18:51.600
there and then Hyper-V underneath is saying okay these are the cores to assign

00:18:51.600 --> 00:18:54.760
it to. Here's the RAM to give it. Excellent and how much of that can you

00:18:54.760 --> 00:18:59.720
ask for? You can ask for the whole machine. You can. For a full machine you know

00:18:59.720 --> 00:19:04.280
full server virtual machine sizes. Wow okay there's probably not too many of

00:19:04.280 --> 00:19:08.800
those in. Yeah. But some people using them. Yeah. Like on the SAP ones because

00:19:08.800 --> 00:19:12.640
they're designed for SAP. I think for those kinds of what the current

00:19:12.640 --> 00:19:15.960
generations I think we offer just two size like either half of it or the whole

00:19:15.960 --> 00:19:22.100
or the whole thing. Incredible. Wow okay how much of a chunk of a rack does that

00:19:22.100 --> 00:19:26.480
take? It's basically the whole rack. Yeah pretty much. Top to bottom. Yeah it's

00:19:26.480 --> 00:19:31.400
like a 10 kilowatt server. Yeah a little power plan on the side there. As you

00:19:31.400 --> 00:19:35.480
kind of talk through the history of sort of how your code ran it was more more

00:19:35.480 --> 00:19:40.280
colo as you said like the more smaller smaller ones and then as you got bigger

00:19:40.280 --> 00:19:44.560
and bigger on some of this you started working on things like well how do we

00:19:44.560 --> 00:19:49.040
let the servers run hotter and have the air cool them rather than more actively

00:19:49.040 --> 00:19:54.560
cooled and then it even gets to a almost more just remove big chunks of it and

00:19:54.560 --> 00:19:57.960
let them fail and then when enough of it has failed take them out you want to

00:19:57.960 --> 00:20:02.600
kind of talk about. Yeah we're still yeah good question because we're we're still

00:20:02.600 --> 00:20:07.360
exploring this space towards higher efficiency lower energy consumption more

00:20:07.360 --> 00:20:11.160
sustainable. One of the experiments that came out of Microsoft Research was

00:20:11.160 --> 00:20:14.760
Project Natick which is taking a bunch of servers putting it or a rack of

00:20:14.760 --> 00:20:20.280
servers putting it in a container that has nitrous oxide gas in it so it's an

00:20:20.280 --> 00:20:25.320
inert gas and dropping it into the ocean floor and letting it be cooled ambiently

00:20:25.320 --> 00:20:30.520
through the water there. Not water on the inside but the outside. Yeah so giant

00:20:30.520 --> 00:20:34.760
heat sink. And the there was potential benefits of that and and it's still

00:20:34.760 --> 00:20:38.560
something that might get revived at some point but what we found coming out of

00:20:38.560 --> 00:20:44.080
that was if the parts are in an inert environment they have one eighth the

00:20:44.080 --> 00:20:48.480
failure rates as ones that are in air environment and with particulate matter

00:20:48.480 --> 00:20:53.360
and corrosive materials in the air. So we started exploring liquid cooling both

00:20:53.360 --> 00:20:57.840
for that as well as potential energy savings and more sustainable cooling and

00:20:57.840 --> 00:21:02.440
then air cooled. We've explored a two-phase liquid immersion we had a

00:21:02.440 --> 00:21:06.600
pilot running there's some regulations that's changed around the the kinds of

00:21:06.600 --> 00:21:10.520
fluids that have made us take a look at a different direction so we. Is that kind

00:21:10.520 --> 00:21:14.320
of like the chlor what you would get in like an air conditioner or some of the

00:21:14.320 --> 00:21:18.560
stuff they. They're called the forever chemicals or materials. The ones we're

00:21:18.560 --> 00:21:23.920
using actually aren't but the regulation is a little broad and so we're just

00:21:23.920 --> 00:21:27.840
steering clear and it might be revisited at some point but we're also they have

00:21:27.840 --> 00:21:32.040
been exploring liquid cooling kind of traditional liquid cooling cold plate

00:21:32.040 --> 00:21:37.120
cold and some people listening probably like me are gamers and have liquid

00:21:37.120 --> 00:21:41.560
cooled GPUs or your CPUs at home in their gaming rigs which allow them to

00:21:41.560 --> 00:21:44.880
get overclocked and it's the same thing we're doing in our data centers. In fact

00:21:44.880 --> 00:21:48.320
one of the things Satya showed in the keynote was something called sidekick

00:21:48.320 --> 00:21:52.720
which is a cabinet that it allows us to take liquid cold plate liquid cooling

00:21:52.720 --> 00:21:59.440
into an existing data center air cooled data center where the Maya 100 AI

00:21:59.440 --> 00:22:04.320
accelerators are in the cabinet sitting right next to it and the cooling pipes

00:22:04.320 --> 00:22:10.240
are going into the Maya cabinet to cool the accelerators themselves and so that

00:22:10.240 --> 00:22:14.440
is. Do they manage like some big metal plate and then the metal plate is liquid

00:22:14.440 --> 00:22:17.640
cooled or something like that? Yeah it's effectively that there's a plate on top

00:22:17.640 --> 00:22:21.440
of the processor and then liquid is going through that. So I'm gonna actually

00:22:21.440 --> 00:22:26.520
show pictures of the inside of the Maya system tomorrow in my AI innovation

00:22:26.520 --> 00:22:31.720
closing keynote but that is I think the takeaway here is that at the scale we're

00:22:31.720 --> 00:22:35.720
at and with the efficiency gains that you might get from even a few percentage

00:22:35.720 --> 00:22:40.480
we're exploring everything at the same time like single phase liquid immersion

00:22:40.480 --> 00:22:44.640
cooling still exploring that and then how to do cold plate more efficiently.

00:22:44.640 --> 00:22:48.720
I'll also be showing something called micro fluidics we're exploring which is

00:22:48.720 --> 00:22:52.960
much more efficient than just pure liquid cold plate which a cold plate is

00:22:52.960 --> 00:22:56.240
just putting the plate like you just said right on top of the processor and

00:22:56.240 --> 00:23:03.160
so the water's taking the heat away but if we can put the liquid right into the

00:23:03.160 --> 00:23:08.000
processor like. Are we talking channels in the process? Around the processor. Okay.

00:23:08.000 --> 00:23:11.680
Just flow it right on top of it and so that's what something we're calling

00:23:11.680 --> 00:23:15.040
micro fluidics and I'll show that and talk a little bit about that tomorrow

00:23:15.040 --> 00:23:20.200
too offers much more efficient cooling and it's not it's not prime time yet but

00:23:20.200 --> 00:23:23.160
looks incredibly promising. That looks awesome.

00:23:23.160 --> 00:23:28.200
This portion of Talk Python to Me is brought to you by the PyBite Python

00:23:28.200 --> 00:23:33.680
Developer Mindset Program. It's run by my two friends and frequent guests Bob

00:23:33.680 --> 00:23:37.600
Delderbos and Julian Sequeira and instead of me telling you about it let's

00:23:37.600 --> 00:23:43.120
hear them describe their program. Happy New Year! As we step into 2024 it's time

00:23:43.120 --> 00:23:47.680
to reflect. Think back to last year what did you achieve with Python? If you're

00:23:47.680 --> 00:23:50.680
feeling like you haven't made the progress you wanted and procrastination

00:23:50.680 --> 00:23:55.960
got the best of you it's not too late. This year can be different. This year can

00:23:55.960 --> 00:24:01.000
be your year of Python mastery. At PyBites we understand the journey of

00:24:01.000 --> 00:24:05.720
learning Python. Our coaching program is tailor-made to help you break through

00:24:05.720 --> 00:24:11.120
barriers and truly excel. Don't let another year slip by with unmet goals.

00:24:11.120 --> 00:24:17.000
Join us at PyBites and let's make 2024 the year you conquer Python. Check out

00:24:17.000 --> 00:24:22.160
PDM Today, our flagship coaching program, and let's chat about your Python journey.

00:24:22.160 --> 00:24:28.640
Apply for the Python Developer Mindset today. It's quick and free to apply. The

00:24:28.640 --> 00:24:32.440
link is in your podcast player show notes. Thanks to PyBites for sponsoring

00:24:32.440 --> 00:24:36.760
the show. One of the things I saw in the opening keynote, I don't know if it fits

00:24:36.760 --> 00:24:40.040
into what you were just talking about or if it's also another of things, where

00:24:40.040 --> 00:24:44.520
actually had the whole motherboard submerged and then even just the entire

00:24:44.520 --> 00:24:48.280
computer is just underwater. So that was two-phase liquid immersion cooling like

00:24:48.280 --> 00:24:52.320
I mentioned. Yeah. Just dunking the whole thing in the dielectric fluid. And you

00:24:52.320 --> 00:24:55.720
had it boil at a low temperature I guess because the phase change is like an

00:24:55.720 --> 00:25:00.200
extremely energy intense aka heat exchange. Yeah and it's called, yeah,

00:25:00.200 --> 00:25:04.400
that's it's actually two-phase because of the boiling it goes, it phase changes

00:25:04.400 --> 00:25:09.120
into gas and then condenses again back into liquid. So it's that was the idea

00:25:09.120 --> 00:25:13.120
behind two phases. I see. Instead of just running a radiator you like almost

00:25:13.120 --> 00:25:17.560
condense it back somewhere else and then bring it back around. Yeah. Okay. Cool. So

00:25:17.560 --> 00:25:21.840
if we go and run our Python codes whether it's pass or I has or whatever,

00:25:21.840 --> 00:25:25.120
what's the chance it's hitting that or is this this kind of cutting-edge stuff

00:25:25.120 --> 00:25:30.680
reserved for high energy AI training? Is it more like if we ask ChatGPT it's

00:25:30.680 --> 00:25:36.000
liquid cooled? Our standard data centers right now are air-cooled. So

00:25:36.000 --> 00:25:40.920
it's air-cooled servers. This Maya part is liquid cooled. So when that you know

00:25:40.920 --> 00:25:44.000
in our first summer we've got some of our own workloads now starting to

00:25:44.000 --> 00:25:47.880
leverage Maya. Do you put on your own workloads first just in case? Yeah.

00:25:47.880 --> 00:25:52.360
Well it's just to see it like shake it out. Yeah. Yeah. Yeah. Piloting it. Yeah. You're not

00:25:52.360 --> 00:25:57.760
offering that up to the big customers just right away. And so Maya I don't know

00:25:57.760 --> 00:26:03.720
how many people know about this either. This is one of the GPU training systems

00:26:03.720 --> 00:26:08.120
you guys have. I mean for those who don't know OpenAI and ChatGPT run on Azure

00:26:08.120 --> 00:26:12.880
which probably takes a couple of cores, a couple GPUs to make happen. Yeah. I want

00:26:12.880 --> 00:26:18.320
to talk about that. Yeah. So the right now our fleet, a large-scale AI

00:26:18.320 --> 00:26:23.200
supercomputing fleet is made up of NVIDIA parts. So the previous generation

00:26:23.200 --> 00:26:27.320
was well the original generation that we trained GPT-3 on with OpenAI was

00:26:27.320 --> 00:26:32.240
NVIDIA V100s and then we introduced A100s which is what GPT-4 was trained on.

00:26:32.240 --> 00:26:37.680
And these are graphics cards like 4080s or something but specifically for AI

00:26:37.680 --> 00:26:41.320
right? Yeah. Okay. That's right. And then the current generation of supercomputer

00:26:41.320 --> 00:26:44.520
we're building for OpenAI training their next generation of their model that's

00:26:44.520 --> 00:26:51.320
a NVIDIA H100 GPUs. Then Maya is our own custom AI accelerator. So it's not a GPU.

00:26:51.320 --> 00:26:56.720
You know one of the aspects of NVIDIA's parts has been their GPU base. So

00:26:56.720 --> 00:27:01.560
they're also can do texture mapping for example. But you don't need that if

00:27:01.560 --> 00:27:05.560
you're just doing pure AI workloads. So back to that specialization right?

00:27:05.560 --> 00:27:08.800
So if you could build it just for the one thing maybe you build it

00:27:08.800 --> 00:27:12.000
slightly different. That's right. So Maya is just designed purely for

00:27:12.000 --> 00:27:17.640
matrix operations used in in fact low precision matrix operations used for AI

00:27:17.640 --> 00:27:22.440
training and inference. And so that is the specialized part that we've created

00:27:22.440 --> 00:27:27.400
called Maya 100 the first generation of that. Well if I think of like some of

00:27:27.400 --> 00:27:31.320
the stuff presented at the opening keynote and stuff here I think the word

00:27:31.320 --> 00:27:37.960
AI was said a record number of times. Yeah I don't think there was a topic there that AI wasn't a part of.

00:27:37.960 --> 00:27:42.840
And so how much is this changing things for you guys? Like 12 months ago or

00:27:42.840 --> 00:27:47.840
something Chachi P appeared on the scene. I mean it's changing it's literally

00:27:47.840 --> 00:27:51.640
changing everything. You know Jensen was saying this is the biggest thing since

00:27:51.640 --> 00:27:56.480
the internet. Yeah Jensen being the CEO of NVIDIA. Yeah yeah who was on stage with Satya at the

00:27:56.480 --> 00:28:01.680
keynote. It is changing everything. It's changing not just the product offerings.

00:28:01.680 --> 00:28:05.560
So the way that we you know have integrate AI into the products using

00:28:05.560 --> 00:28:09.160
Copilot. It's changing the way we develop the products as well. And the way that we

00:28:09.160 --> 00:28:13.080
run our systems inside already. So for example incident management. We've got

00:28:13.080 --> 00:28:17.800
Copilot built you know our own Copilot internally built into that. So somebody

00:28:17.800 --> 00:28:22.040
that's responding to an issue and our production systems can say okay so what's

00:28:22.040 --> 00:28:26.680
going on? What's that what happened with this? Yeah show me the graph of this. You

00:28:26.680 --> 00:28:29.880
know just be able to use human language to get caught up on what's going on.

00:28:29.880 --> 00:28:34.120
People tell me it's just statistics. Just prediction. It doesn't feel like

00:28:34.120 --> 00:28:39.000
prediction. You know the people that say that I think are missing

00:28:39.000 --> 00:28:44.560
the scale of the statistics. And because we're probably predicting a little bit.

00:28:44.560 --> 00:28:48.160
Thinking about what are you gonna say next. That's what we're we're statistical.

00:28:48.160 --> 00:28:54.720
And so it's just once you get statistics large that at a large

00:28:54.720 --> 00:28:58.440
enough scale that you start to see something that looks like what we call

00:28:58.440 --> 00:29:03.240
intelligence. Yeah yeah it's it's really incredible. I'm starting to use it to

00:29:03.240 --> 00:29:07.960
just write my git commit logs for me. You know push a button and says oh you

00:29:07.960 --> 00:29:11.600
added error handling to the background task. So in case this fails you'll be

00:29:11.600 --> 00:29:15.280
more resilient and it'll keep running like that's better than I could have got.

00:29:15.280 --> 00:29:19.760
But then I crash you know and yeah you just push the button and it's just

00:29:19.760 --> 00:29:23.920
magic. It's magical. Yeah it really is. I mean it's not it it's called

00:29:23.920 --> 00:29:26.800
copilot for a reason because we're not at the point yet where you can just let

00:29:26.800 --> 00:29:30.760
it do what it does autonomously. You need to check its work. Like you need to look

00:29:30.760 --> 00:29:33.640
at and say oops you know that time it screwed it up. It didn't get it quite

00:29:33.640 --> 00:29:39.840
right or I need to add more context to this than it had or it extracted. So but

00:29:39.840 --> 00:29:44.160
as far as accelerating work it's just a game-changer. Yeah it really really is.

00:29:44.160 --> 00:29:47.880
So before we run out of time I want to ask you just a couple more things bit of

00:29:47.880 --> 00:29:52.800
a diversion. So Python we saw Python appear in the keynote. They were showing

00:29:52.800 --> 00:29:56.600
off I can't remember who it wasn't Satya it was whoever followed him. They look we

00:29:56.600 --> 00:30:01.760
want to show off our new sharing of the insanely large GPUs for machine learning.

00:30:01.760 --> 00:30:05.040
Let's pull up some Python and a Jupyter notebook and I'll just check that out.

00:30:05.040 --> 00:30:09.840
And you're like where are we again? Yeah really interesting. So you know you said

00:30:09.840 --> 00:30:12.880
you're using a little bit of Python yourself like what's Python look like in

00:30:12.880 --> 00:30:16.480
your world? Well so the reason that I'm using Python is I took a sabbatical this

00:30:16.480 --> 00:30:20.560
summer and so I was like I'm gonna do some AI research. So I got connected with

00:30:20.560 --> 00:30:23.960
an AI researcher. In fact I'm gonna talk about this at my keynote tomorrow some

00:30:23.960 --> 00:30:28.640
of the work that came out of it. The obviously AI is completely Python these

00:30:28.640 --> 00:30:32.760
days. Yeah almost entirely. So I was I spent the whole summer and I still am

00:30:32.760 --> 00:30:37.120
spending my time in Python Jupyter notebooks and then Python scripts when

00:30:37.120 --> 00:30:42.520
you want to do some some run for for a final result. So I hadn't used really

00:30:42.520 --> 00:30:45.720
Python before other than in passing. I mean it's a very language that's very

00:30:45.720 --> 00:30:49.680
easy to pick up. Yeah there's a there's a t-shirt that says I learned Python it

00:30:49.680 --> 00:30:52.680
was a good weekend. Yeah it's a bit of a joke but it's a good joke you know. Yeah

00:30:52.680 --> 00:30:56.960
and I think that's what makes it so powerful is that it's so easy to pick up.

00:30:56.960 --> 00:31:01.880
But what's made it even easier for me to pick it up I'd say that I'm a

00:31:01.880 --> 00:31:06.400
mediocre Python programmer but I'm using Copilot and that's made me an expert

00:31:06.400 --> 00:31:12.000
Python coder. How do I do this? Yeah and it's like I've never I don't go to

00:31:12.000 --> 00:31:14.920
stack over you know it's a question I don't go to stack over flow for

00:31:14.920 --> 00:31:19.720
questions. I haven't had to get a book on Python. I basically just either ask

00:31:19.720 --> 00:31:24.920
Copilot explicitly like how do I do this or write me this or I put it in the

00:31:24.920 --> 00:31:29.440
function or in the comment and it gets it done for me. And there's a occasionally

00:31:29.440 --> 00:31:32.680
I'll have to go hand met edit it and figure out what's going on but for the

00:31:32.680 --> 00:31:37.200
most part it is writing almost all my code. And so my goal is how can I just

00:31:37.200 --> 00:31:40.880
not have it write everything for me. So that is kind of become the way that I

00:31:40.880 --> 00:31:46.600
program in Python and I think Python and AI and it the knowledge of Copilot for

00:31:46.600 --> 00:31:52.440
Python because OpenAI obviously for their own purposes has made GPT-4 and

00:31:52.440 --> 00:31:57.840
GPT-3.5 before it really know Python. I hadn't really thought of that connection but of course they

00:31:57.840 --> 00:32:02.600
wanted to answer Python questions I'm sure. For themselves. So I think when it

00:32:02.600 --> 00:32:07.560
comes to seeing what AI can do for programming Python is at the forefront

00:32:07.560 --> 00:32:10.960
of that. What was your impression of it? I mean I'm sure you've probably seen it

00:32:10.960 --> 00:32:14.440
before but like what's your impression working in it coming from a curly brace

00:32:14.440 --> 00:32:19.040
semicolon type language like C++ or something like this weird they drop a

00:32:19.040 --> 00:32:22.920
bunch of parentheses they have these tab space these four spaces rules. Well it's

00:32:22.920 --> 00:32:31.080
you know the YAML versus JSON. But I mean I've gotten used to it it's not a it's

00:32:31.080 --> 00:32:37.080
not a big deal. I find it's it's less verbose than C. There's less typing. Less

00:32:37.080 --> 00:32:41.240
symbol noise. You get the essence. Yeah. Yeah. Yeah I kind of had that experience as well

00:32:41.240 --> 00:32:44.520
coming from a C-based language like wow this is really weird and then after I

00:32:44.520 --> 00:32:48.280
went back to C# I'm like but this is also weird and I kind of like this

00:32:48.280 --> 00:32:52.560
clarity over here so now what do I do with life? And then you go back and like

00:32:52.560 --> 00:32:56.600
semicolons are annoying now. Yes exactly. Like I thought they were needed they're

00:32:56.600 --> 00:33:01.120
not needed what's going on? Another thing that I think you know maybe people

00:33:01.120 --> 00:33:05.960
really enjoy hearing a bit about and I'm a big fan of. You wrote a three-part

00:33:05.960 --> 00:33:11.280
series of novels about computer hackers called Zero Day. Mm-hmm. Really good. I read

00:33:11.280 --> 00:33:15.920
all of them back when they came out and so much of this like computer mystery

00:33:15.920 --> 00:33:19.280
stuff is like oh I'm they're using vb6 I'm gonna get their IP address. You're like

00:33:19.280 --> 00:33:24.960
wait what? Those words are meaningful but the sense is not right? And you know your

00:33:24.960 --> 00:33:28.400
books are like a lot of sort of spy stuff but also a lot of really cool

00:33:28.400 --> 00:33:33.080
legit reasonably possible computer stuff. Yeah tell people a quick bit about

00:33:33.080 --> 00:33:38.420
that. I love cyber I love thrillers growing up techno thrillers. I read

00:33:38.420 --> 00:33:42.180
Andromeda Strain when I was like in seventh grade. I was like this book is so

00:33:42.180 --> 00:33:46.860
cool because it's like I'm learning science plus yeah it's actually you know

00:33:46.860 --> 00:33:51.640
it's really exciting. So I've always wanted to write one and then coming into

00:33:51.640 --> 00:33:56.800
after into the late to 1990s when you started to see some of these large-scale

00:33:56.800 --> 00:34:01.800
viruses I was just thinking this is such a powerful weapon for somebody to cause

00:34:01.800 --> 00:34:05.960
destruction and then 9/11 happened I'm like all right logical next step is

00:34:05.960 --> 00:34:10.080
leveraging a cyber weapon to do something that with the same goals and

00:34:10.080 --> 00:34:15.840
so that's what led me to write Zero Day which is taking that idea of using a

00:34:15.840 --> 00:34:19.760
cyber weapon for terrorism. Then I was like oh that book was really well

00:34:19.760 --> 00:34:23.080
received I had a lot of fun doing it so let me write the next one and I wanted

00:34:23.080 --> 00:34:27.880
to continue in this theme with the same characters Darryl Hagen and Jeff Akin

00:34:27.880 --> 00:34:32.280
and say what what's something else what's other another cyber security

00:34:32.280 --> 00:34:35.520
angle that I can take a look at in the second one so the second one was state

00:34:35.520 --> 00:34:40.560
sponsored cyber espionage and actually the ironic thing is I'd already had Iran

00:34:40.560 --> 00:34:44.240
in the book story I'd already had China in the story I had people trying to

00:34:44.240 --> 00:34:48.560
figure out how to get Iran a nuclear weapon and then Stuxnet happened right

00:34:48.560 --> 00:34:52.440
when I was still writing the book I'm like okay this is like a stolen part of my

00:34:52.440 --> 00:34:57.080
plot line. So I had to change the book a little to acknowledge Stuxnet

00:34:57.080 --> 00:35:00.600
happening and then the third one was about insider threats which I think is

00:35:00.600 --> 00:35:04.720
one of the toughest threats to deal with in this case it was a long-range plot

00:35:04.720 --> 00:35:09.160
from some people that wanted to compromise stock exchange and kind of a

00:35:09.160 --> 00:35:13.520
mixture of high-frequency trading and insider threat with cyber security

00:35:13.520 --> 00:35:17.600
systems was the third one called Rogue Code. Mm-hmm yeah so they were all really

00:35:17.600 --> 00:35:20.720
good I really enjoyed it. Were you a fan of Mr. Robot did you ever watch that

00:35:20.720 --> 00:35:24.920
series? I did I love that series. Yeah oh my gosh again it's it seemed pretty

00:35:24.920 --> 00:35:29.360
plausible. Yeah I really like that imagine a lot of people out there have seen

00:35:29.360 --> 00:35:33.680
Mr. Robot as well if they want you know that kind of idea but in just a

00:35:33.680 --> 00:35:37.440
series they can binge. Yeah cool. Maybe we should wrap up our chat here but just a

00:35:37.440 --> 00:35:42.560
quick of like some of the future things you talked about like rapidly deploying

00:35:42.560 --> 00:35:46.800
some of these data centers and some of these ballast systems maybe just give us

00:35:46.800 --> 00:35:51.120
a sense of like and even like disaggregated rack architecture do you

00:35:51.120 --> 00:35:56.320
have instead of having a GPU alongside a server like a rack of GPUs and then

00:35:56.320 --> 00:36:00.240
optical connections to a rack of servers like give us a sense of some of the

00:36:00.240 --> 00:36:03.240
stuff where it's going. Yeah so that's some of the stuff that we're exploring

00:36:03.240 --> 00:36:07.800
like I mentioned we're taking a look at lots of different ways to re-architect

00:36:07.800 --> 00:36:10.920
the data center to be more efficient and one of the ways that you get efficient

00:36:10.920 --> 00:36:15.600
is by and in reduced fragmentation is by having larger pools to allocate

00:36:15.600 --> 00:36:18.720
resources from. If you think about allocating a virtual machine on a server

00:36:18.720 --> 00:36:22.440
how much RAM can you give it at most? Well as much as sitting on the server.

00:36:22.440 --> 00:36:26.480
How many GPUs can you attach to it? Well as most as are attached to that server.

00:36:26.480 --> 00:36:30.920
Right how many PCI slots you got? Yeah so but if you think about I've got a large

00:36:30.920 --> 00:36:36.200
resource pool it's a whole group of GPUs then I could be able to give it as many

00:36:36.200 --> 00:36:41.760
GPUs as you want. 50? Yeah ask for 50. Exactly the benefits of pooling for a

00:36:41.760 --> 00:36:44.400
resource allocation are that you reduce fragmentation and you get more

00:36:44.400 --> 00:36:48.360
flexibility so we've been trying to explore how we can do this from rack

00:36:48.360 --> 00:36:52.080
scale disaggregation of saying there's a whole bunch of SSDs at the top of the

00:36:52.080 --> 00:36:56.480
rack then there's a bunch of GPUs and then there's a bunch of CPU cores and

00:36:56.480 --> 00:36:59.880
let's just compose the system dynamically. There's a bunch of

00:36:59.880 --> 00:37:04.760
challenges from a resiliency perspective like how do you prevent one failure of

00:37:04.760 --> 00:37:09.080
the GPU part of the system bringing down the whole rack for example. There's

00:37:09.080 --> 00:37:13.400
latency and bandwidth challenges like how do you when you're sitting there on

00:37:13.400 --> 00:37:16.680
the PCI bus you get a whole bunch of bandwidth and you get very low latency.

00:37:16.680 --> 00:37:21.000
If you're going across the rack you might have the same bandwidth you might

00:37:21.000 --> 00:37:24.640
have lower bandwidth just because you can't deliver that much bandwidth out of

00:37:24.640 --> 00:37:28.800
the GPUs. Right all the systems are optimized to make assumptions about these

00:37:28.800 --> 00:37:32.560
numbers. Exactly and your latency is gonna be higher and so some workloads

00:37:32.560 --> 00:37:35.760
can't tolerate their latency so we've been exploring disaggregated memory

00:37:35.760 --> 00:37:39.080
disaggregated GPUs I've shown demos of both of them there's we're still

00:37:39.080 --> 00:37:42.680
exploring those we're not you know it's not ready for production. Which is harder.

00:37:42.680 --> 00:37:47.360
Yeah disaggregated memory or GPUs? Yeah I would guess memory but I have a zero

00:37:47.360 --> 00:37:52.000
experience. Memory is challenging because there are certain GPU workloads that

00:37:52.000 --> 00:37:57.120
aren't so latency sensitive like AI training. Sure like a batch job sort of thing.

00:37:57.120 --> 00:38:02.880
Yeah but when it comes to memory you almost always see the latency and so

00:38:02.880 --> 00:38:07.480
what we think we can do is get remote memory down to NUMA you know speaking of

00:38:07.480 --> 00:38:12.320
non-uniform memory architecture latency down to that level and a lot of

00:38:12.320 --> 00:38:16.400
applications can tolerate that. Okay. And so we have a memory tiering where you've

00:38:16.400 --> 00:38:19.160
got closed memory that's on the system and then remote memory which is like

00:38:19.160 --> 00:38:23.840
NUMA latency. Kind of like a L2 cache but like a bigger idea of it. Very very cool

00:38:23.840 --> 00:38:28.600
I think you guys are doing such neat stuff and when you see these hyperscale

00:38:28.600 --> 00:38:34.640
clouds I think a lot of what people see is the the insane dashboard of choices

00:38:34.640 --> 00:38:41.080
like do I do yeah I do routing do I do firewalls do I do VPCs do I do like paths

00:38:41.080 --> 00:38:45.400
I as what do I do but oftentimes don't really think about like well you're

00:38:45.400 --> 00:38:49.440
getting a slice of this giant server and you know maybe someday to live under the

00:38:49.440 --> 00:38:53.940
ocean or whatever right so it's really cool to yeah. What you're seeing is the

00:38:53.940 --> 00:38:57.040
cloud it started with a few basic building blocks and then we started to

00:38:57.040 --> 00:39:01.080
explore lots of different directions of creating lots of different paths

00:39:01.080 --> 00:39:05.400
services and past services for compute and then different data offerings I

00:39:05.400 --> 00:39:11.040
think this space and again coming to the workload you get this is it high do

00:39:11.040 --> 00:39:14.960
you need key value store do you need a vectorized database do you need and and

00:39:14.960 --> 00:39:19.520
do you need any of those to be extreme performance because then if you need

00:39:19.520 --> 00:39:24.400
extreme performance go for the design for purpose vectorized database if you

00:39:24.400 --> 00:39:29.240
want key value with vectorization but it's okay if the vectorization isn't the

00:39:29.240 --> 00:39:34.160
fastest possible you know you can go use this other offering so that's why the

00:39:34.160 --> 00:39:38.960
the list of options is continue to expand is just because every workload

00:39:38.960 --> 00:39:42.160
says I need this and that's the most important thing to me and everyone says

00:39:42.160 --> 00:39:45.280
no I need this that's the most important thing and others are like I don't care

00:39:45.280 --> 00:39:50.240
well as it becomes the mainframe of the world right there's a lot of different

00:39:50.240 --> 00:39:54.120
types of apps running on it yeah yeah awesome all right mark final call to

00:39:54.120 --> 00:39:57.480
action people maybe want to learn more about some of the stuff we saw here see

00:39:57.480 --> 00:40:01.120
some pictures but or maybe also just do more with Azure what do you say so a

00:40:01.120 --> 00:40:05.080
couple things one is I've been doing a series of Azure innovation talks at

00:40:05.080 --> 00:40:08.320
building ignite session so go back to the last build and you'll see the most

00:40:08.320 --> 00:40:11.800
recent one of those and then at this ignite I'm doing one on that's just

00:40:11.800 --> 00:40:17.520
looking at AI related innovation so that's on Friday tomorrow here at ignite

00:40:17.520 --> 00:40:21.520
and it'll be available on demand so that's awesome yeah I'll grab the links

00:40:21.520 --> 00:40:24.280
to some of those put them in the show notes for people excellent well thanks

00:40:24.280 --> 00:40:27.000
so much for being on the show all right thanks for having me yeah

00:40:27.000 --> 00:40:32.080
this has been another episode of talk Python to me thank you to our sponsors

00:40:32.080 --> 00:40:34.900
be sure to check out what they're offering it really helps support the

00:40:34.900 --> 00:40:40.400
show this episode is sponsored by posit connect from the makers of shiny publish

00:40:40.400 --> 00:40:43.560
share and deploy all of your data projects that you're creating using

00:40:43.560 --> 00:40:49.920
Python streamlit - shiny bokeh FastAPI flask quattro reports dashboards and

00:40:49.920 --> 00:40:55.160
APIs posit connect supports all of them try posit connect for free by going to

00:40:55.160 --> 00:41:01.880
talk Python FM slash posit POS IT are you ready to level up your Python career

00:41:01.880 --> 00:41:06.400
and could you use a little bit of personal and individualized guidance to

00:41:06.400 --> 00:41:11.680
do so check out the pie bites Python developer mindset program at talk Python

00:41:11.680 --> 00:41:16.760
dot FM slash PDM want to level up your Python we have one of the largest

00:41:16.760 --> 00:41:21.560
catalogs of Python video courses over at talk Python our content ranges from true

00:41:21.560 --> 00:41:26.080
beginners to deeply advanced topics like memory and async and best of all there's

00:41:26.080 --> 00:41:29.760
not a subscription in sight check it out for yourself at training dot talk Python

00:41:29.760 --> 00:41:34.240
dot FM be sure to subscribe to the show open your favorite podcast app and

00:41:34.240 --> 00:41:38.520
search for Python we should be right at the top you can also find the iTunes feed

00:41:38.520 --> 00:41:43.400
at /itunes the Google Play feed at /play and the direct RSS feed at

00:41:43.400 --> 00:41:48.280
/rss on talk Python dot FM we're live streaming most of our recordings

00:41:48.280 --> 00:41:51.600
these days if you want to be part of the show and have your comments featured on

00:41:51.600 --> 00:41:55.560
the air be sure to subscribe to our YouTube channel at talk Python dot FM

00:41:55.560 --> 00:42:00.160
slash YouTube this is your host Michael Kennedy thanks so much for listening I

00:42:00.160 --> 00:42:05.000
really appreciate it now get out there and write some Python code

00:42:05.000 --> 00:42:07.060
you

00:42:22.080 --> 00:42:24.080
[MUSIC]

