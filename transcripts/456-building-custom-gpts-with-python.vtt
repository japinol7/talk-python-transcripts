WEBVTT

00:00:00.000 --> 00:00:02.400
Do you know what custom GPTs are?

00:00:02.400 --> 00:00:05.160
They're configurable and shareable chat experiences

00:00:05.160 --> 00:00:07.600
with the name, logo, custom instructions,

00:00:07.600 --> 00:00:10.540
conversation starters, access to open AI tools

00:00:10.540 --> 00:00:13.040
and custom API actions.

00:00:13.040 --> 00:00:15.600
And you can build them with Python.

00:00:15.600 --> 00:00:17.640
Ian Moyer has been doing just that

00:00:17.640 --> 00:00:20.680
and is here to share his experience building them.

00:00:20.680 --> 00:00:21.720
This is "Talk Python to Me,"

00:00:21.720 --> 00:00:26.720
episode 456, recorded January 22nd, 2024.

00:00:26.720 --> 00:00:29.300
(upbeat music)

00:00:31.880 --> 00:00:42.100
Welcome to "Talk Python to Me,"

00:00:42.100 --> 00:00:44.040
a weekly podcast on Python.

00:00:44.040 --> 00:00:45.820
This is your host, Michael Kennedy.

00:00:45.820 --> 00:00:48.180
Follow me on Mastodon, where I'm @mkennedy

00:00:48.180 --> 00:00:50.900
and follow the podcast using @talkpython,

00:00:50.900 --> 00:00:53.460
both on fosstodon.org.

00:00:53.460 --> 00:00:55.580
Keep up with the show and listen to over seven years

00:00:55.580 --> 00:00:58.700
of past episodes @talkpython.fm.

00:00:58.700 --> 00:01:02.180
We've started streaming most of our episodes live on YouTube.

00:01:02.180 --> 00:01:03.400
Subscribe to our YouTube channel

00:01:03.400 --> 00:01:05.820
over at talkpython.fm/youtube

00:01:05.820 --> 00:01:08.000
to get notified about upcoming shows

00:01:08.000 --> 00:01:10.340
and be part of that episode.

00:01:10.340 --> 00:01:12.580
This episode is sponsored by Sentry.

00:01:12.580 --> 00:01:14.180
Don't let those errors go unnoticed.

00:01:14.180 --> 00:01:15.100
Use Sentry.

00:01:15.100 --> 00:01:18.400
Get started at talkpython.fm/sentry.

00:01:18.400 --> 00:01:21.260
And it's also brought to you by Neo4j.

00:01:21.260 --> 00:01:24.100
It's time to stop asking relational databases

00:01:24.100 --> 00:01:25.940
to do more than they were made for.

00:01:25.940 --> 00:01:28.280
Check out the sample FastAPI project

00:01:28.280 --> 00:01:32.540
and see what Neo4j, a native graph database, can do for you.

00:01:32.540 --> 00:01:36.800
Find out more at talkpython.fm/neo4j.

00:01:36.800 --> 00:01:39.980
Ian, welcome to Talk Python to Me.

00:01:39.980 --> 00:01:41.340
- Hey, hey, Michael, good to see you again.

00:01:41.340 --> 00:01:43.300
- Yeah, great to see you again.

00:01:43.300 --> 00:01:44.940
It has been a little while.

00:01:44.940 --> 00:01:46.580
It seems like not so long ago,

00:01:46.580 --> 00:01:50.420
and yet when I pull up the episode that we did together,

00:01:50.420 --> 00:01:55.060
sure enough, it says March 7th, 2018.

00:01:55.060 --> 00:01:55.900
Wow.

00:01:55.900 --> 00:01:56.740
- The years are short.

00:01:56.740 --> 00:01:58.300
The years are short, they go by really fast.

00:01:58.300 --> 00:01:59.340
- They sure do.

00:01:59.340 --> 00:02:02.420
So back then, we were talking about Python

00:02:02.420 --> 00:02:04.400
and biology and genomics,

00:02:04.400 --> 00:02:08.200
and it sounds like you're still doing genetic-type things

00:02:08.200 --> 00:02:11.220
and still doing Python and all that kind of stuff.

00:02:11.220 --> 00:02:12.740
- For sure, yeah, definitely.

00:02:12.740 --> 00:02:14.700
I work for a company called Genome Oncology.

00:02:14.700 --> 00:02:17.220
We do precision oncology software,

00:02:17.220 --> 00:02:19.780
helping folks make sense of genomics

00:02:19.780 --> 00:02:20.980
and trying to help cancer patients.

00:02:20.980 --> 00:02:21.820
- That's awesome.

00:02:21.820 --> 00:02:25.420
There's different levels of helping people with software.

00:02:25.420 --> 00:02:28.660
On one level, we probably have ad retargeting.

00:02:28.660 --> 00:02:33.420
On the other, we've got medical benefits

00:02:33.420 --> 00:02:36.260
and looking for helping people

00:02:36.260 --> 00:02:38.700
who are suffering socially or whatever.

00:02:38.700 --> 00:02:41.060
So it's gotta feel good to write software

00:02:41.060 --> 00:02:43.460
that is making a difference in people's lives.

00:02:43.460 --> 00:02:44.300
- That's right.

00:02:44.300 --> 00:02:46.860
I did spend a lot of the 2000s making e-commerce websites,

00:02:46.860 --> 00:02:48.980
and that wasn't exactly the most fulfilling thing.

00:02:48.980 --> 00:02:51.040
I learned a lot, but it wasn't as exciting

00:02:51.040 --> 00:02:51.880
as what I'm doing now,

00:02:51.880 --> 00:02:53.940
or at least as fulfilling as what I'm doing now.

00:02:53.940 --> 00:02:55.580
- What were those earlier websites in Python?

00:02:55.580 --> 00:02:57.660
- That was all Java for the most part.

00:02:57.660 --> 00:02:59.940
And finally with this company,

00:02:59.940 --> 00:03:03.100
knocked out a prototype in Django a few years ago.

00:03:03.100 --> 00:03:05.140
And my boss at the time was like,

00:03:05.140 --> 00:03:06.660
"You did that so fast,

00:03:06.660 --> 00:03:08.280
"you should do some more stuff in Python."

00:03:08.280 --> 00:03:10.580
So that's kind of how it evolved.

00:03:10.580 --> 00:03:14.140
And now basically most of our core backend is Python,

00:03:14.140 --> 00:03:17.460
and we use a little bit of Svelte for the user interfaces.

00:03:17.460 --> 00:03:18.300
- Beautiful.

00:03:18.300 --> 00:03:21.860
It's easy to forget, like five years ago, 10 years ago,

00:03:21.860 --> 00:03:24.020
people were questioning whether Python

00:03:24.020 --> 00:03:25.100
should be something you should use.

00:03:25.100 --> 00:03:26.060
Is it a real language?

00:03:26.060 --> 00:03:26.900
Do you really use it?

00:03:26.900 --> 00:03:27.980
Is it safe to use?

00:03:27.980 --> 00:03:31.580
Maybe you should use a Java or a C# or something like that,

00:03:31.580 --> 00:03:33.540
because this is a real project.

00:03:33.540 --> 00:03:34.380
It's interesting.

00:03:34.380 --> 00:03:36.100
You don't hear that nearly as much anymore, do you?

00:03:36.100 --> 00:03:37.380
- I grew up with Boston sports fans,

00:03:37.380 --> 00:03:39.720
and it was like being a Boston sports fan was terrible

00:03:39.720 --> 00:03:40.560
for the longest time.

00:03:40.560 --> 00:03:41.380
And now it's like,

00:03:41.380 --> 00:03:43.620
"Okay, we don't wanna hear about your problems right now."

00:03:43.620 --> 00:03:44.500
And same thing with Python.

00:03:44.500 --> 00:03:45.700
It's like, "I like Python."

00:03:45.700 --> 00:03:46.540
It's like, "Yeah, great.

00:03:46.540 --> 00:03:47.940
"So does everybody else in the world."

00:03:47.940 --> 00:03:50.380
So yeah, it's really not the issue anymore.

00:03:50.380 --> 00:03:51.820
Now it's not the cool thing to play with.

00:03:51.820 --> 00:03:54.140
So now you gotta go to Rust or something else.

00:03:54.140 --> 00:03:55.180
- You know what's shiny?

00:03:55.180 --> 00:03:56.620
LLMs are shiny.

00:03:56.620 --> 00:03:58.300
- LLMs are very shiny, for sure.

00:03:58.300 --> 00:03:59.980
- Yeah, we can talk about them today.

00:03:59.980 --> 00:04:01.100
- Yeah, that sounds great.

00:04:01.100 --> 00:04:01.940
Let's do it.

00:04:01.940 --> 00:04:04.780
- First of all, we're gonna talk about building applications

00:04:04.780 --> 00:04:08.540
that are basically powered by LLMs that you plug into.

00:04:08.540 --> 00:04:09.380
Right? - Yep.

00:04:09.380 --> 00:04:13.820
- Before we get into creating LLMs, just for you,

00:04:13.820 --> 00:04:16.940
where do LLMs play a role for you

00:04:16.940 --> 00:04:18.700
in software development these days?

00:04:18.700 --> 00:04:19.540
- Sure.

00:04:19.540 --> 00:04:21.060
So, you know, like everybody else,

00:04:21.060 --> 00:04:22.780
I mean, I had been playing with,

00:04:22.780 --> 00:04:24.980
so I do natural language processing as part of my job.

00:04:24.980 --> 00:04:25.820
Right?

00:04:25.820 --> 00:04:27.220
So using spaCy was a big part

00:04:27.220 --> 00:04:30.260
of the information extraction stack that we use,

00:04:30.260 --> 00:04:32.060
'cause we have to deal with a lot of medical data

00:04:32.060 --> 00:04:33.740
and medical data is just unstructured

00:04:33.740 --> 00:04:36.580
and has to be cleaned up before it can be used.

00:04:36.580 --> 00:04:37.540
That was my exposure.

00:04:37.540 --> 00:04:41.540
I had seen GPTs and the idea of like generating text,

00:04:41.540 --> 00:04:42.540
just starting from that,

00:04:42.540 --> 00:04:44.500
didn't really make much sense to me at the time.

00:04:44.500 --> 00:04:45.820
But then obviously like everybody else,

00:04:45.820 --> 00:04:48.380
when chatGPT came out, I was like, "Oh, I get this now."

00:04:48.380 --> 00:04:50.300
Like this thing does, you know,

00:04:50.300 --> 00:04:52.220
it can basically learn in the context

00:04:52.220 --> 00:04:54.060
and it can actually produce something that's interesting

00:04:54.060 --> 00:04:56.340
and you can use it for things like information extraction.

00:04:56.340 --> 00:04:58.500
So just like everybody else, I kind of woke up to them,

00:04:58.500 --> 00:05:01.100
you know, around that time that they got released

00:05:01.100 --> 00:05:02.340
and I use them all the time.

00:05:02.340 --> 00:05:03.180
Right?

00:05:03.180 --> 00:05:05.020
So chatGPT4 is really what I use.

00:05:05.020 --> 00:05:07.540
I would recommend, if you can afford the $20 a month,

00:05:07.540 --> 00:05:11.220
it's still the best model that there is as of January 2024.

00:05:11.220 --> 00:05:12.220
And I use that for coding.

00:05:12.220 --> 00:05:15.420
I don't really like the coding tools, the copilots,

00:05:15.420 --> 00:05:16.380
but there, you know,

00:05:16.380 --> 00:05:18.060
there's definitely folks that swear by them.

00:05:18.060 --> 00:05:20.980
So my workflow is more of, I have a problem,

00:05:20.980 --> 00:05:22.940
work with the chatbot to try to like, you know,

00:05:22.940 --> 00:05:25.140
think through all the edge cases and then,

00:05:25.140 --> 00:05:27.700
and then think through the test case, the tests.

00:05:27.700 --> 00:05:29.140
And then I think through the code, right?

00:05:29.140 --> 00:05:31.180
And then the actual typing of the code,

00:05:31.180 --> 00:05:33.100
yeah, I'll have it do a lot of the boilerplate stuff,

00:05:33.100 --> 00:05:35.660
but then kind of shaping the APIs and things like that.

00:05:35.660 --> 00:05:37.180
I kind of like to do that myself still.

00:05:37.180 --> 00:05:39.140
I'm kind of old school, old school.

00:05:39.140 --> 00:05:40.340
- I guess I'm old school as well.

00:05:40.340 --> 00:05:42.180
'Cause I'm like right there with you.

00:05:42.180 --> 00:05:45.420
But for me, I don't generally run copilot

00:05:45.420 --> 00:05:48.220
or those kinds of things in my editors.

00:05:48.220 --> 00:05:51.180
I do have some features turned on,

00:05:51.180 --> 00:05:54.180
but primarily it's just really nice autocomplete.

00:05:54.180 --> 00:05:55.020
You know what I mean?

00:05:55.020 --> 00:05:56.860
Like, it seems like it almost just knows

00:05:56.860 --> 00:05:59.500
what I want to type anyway, and that's getting better.

00:05:59.500 --> 00:06:01.580
I don't know if anyone's noticed recently,

00:06:01.580 --> 00:06:03.380
one of the recent releases of PyCharm,

00:06:03.380 --> 00:06:06.380
it starts to autocomplete whole lines.

00:06:06.380 --> 00:06:07.620
And I don't know where it's getting this from.

00:06:07.620 --> 00:06:10.260
And I think I have the AI features turned off.

00:06:10.260 --> 00:06:11.940
At least it says I have no license.

00:06:11.940 --> 00:06:13.500
Guessing that means they're turned off.

00:06:13.500 --> 00:06:16.220
So it must be something more built into it.

00:06:16.220 --> 00:06:17.340
That's pretty excellent.

00:06:17.340 --> 00:06:19.580
But for me, I find I'm pretty content

00:06:19.580 --> 00:06:20.860
to just sit and write code.

00:06:20.860 --> 00:06:24.500
However, the more specific the unknowns are,

00:06:24.500 --> 00:06:25.860
the more willing I'm like,

00:06:25.860 --> 00:06:28.000
"Oh, I need to go to ChatGPT for this."

00:06:28.000 --> 00:06:30.420
Like, for example, like, how do you use Pydantic?

00:06:30.420 --> 00:06:33.780
Like, well, I'll probably just go look at a quick code sample

00:06:33.780 --> 00:06:35.420
and see that so I can understand it.

00:06:35.420 --> 00:06:38.640
But if it's, I have this time string

00:06:38.640 --> 00:06:41.740
with the date like this, the month like this,

00:06:41.740 --> 00:06:43.660
and then it has the time zone like that.

00:06:43.660 --> 00:06:44.760
How do I parse that?

00:06:44.760 --> 00:06:47.620
Or how do I generate another one like that in Python?

00:06:47.620 --> 00:06:48.740
And here's the answer.

00:06:48.740 --> 00:06:51.020
Or I have this giant weird string,

00:06:51.020 --> 00:06:52.460
and I want this part of it

00:06:52.460 --> 00:06:55.220
as extracted with a regular expression.

00:06:55.220 --> 00:06:56.060
And I want a regular expression.

00:06:56.060 --> 00:06:56.900
- Regular expression, I was just gonna say that.

00:06:56.900 --> 00:06:57.740
- Oh my gosh.

00:06:57.740 --> 00:06:59.020
- You don't have to write another one of those.

00:06:59.020 --> 00:06:59.860
Yeah, it's great.

00:06:59.860 --> 00:07:00.680
- Yeah, it's pretty much like,

00:07:00.680 --> 00:07:02.140
do you need to detect the end of a line

00:07:02.140 --> 00:07:03.380
straight to ChatGPT?

00:07:03.380 --> 00:07:04.220
Not really, but you know,

00:07:04.220 --> 00:07:06.700
it's like almost any level of chat,

00:07:06.700 --> 00:07:07.540
a regular expression.

00:07:07.540 --> 00:07:09.100
I'm like, well, I need some AI for this.

00:07:09.100 --> 00:07:11.540
'Cause this is not time well spent for me.

00:07:11.540 --> 00:07:12.900
But yeah, it's interesting.

00:07:12.900 --> 00:07:14.420
- Yeah, one big tip I would give people though

00:07:14.420 --> 00:07:16.660
is that these chatbots, they wanna please you.

00:07:16.660 --> 00:07:19.180
So you have to ask it to criticize you.

00:07:19.180 --> 00:07:20.820
You have to say, here's some piece of code,

00:07:20.820 --> 00:07:22.300
tell me all the ways it's wrong.

00:07:22.300 --> 00:07:25.460
And you have to also ask for lots of different examples

00:07:25.460 --> 00:07:27.740
because it just starts to get more creative,

00:07:27.740 --> 00:07:28.780
more things that it says.

00:07:28.780 --> 00:07:30.380
It really thinks by talking,

00:07:30.380 --> 00:07:31.980
which is a really weird thing to consider.

00:07:31.980 --> 00:07:34.180
But it's definitely some things to keep in mind

00:07:34.180 --> 00:07:35.020
when you're working with these things.

00:07:35.020 --> 00:07:37.260
- And they do have these really weird things.

00:07:37.260 --> 00:07:40.100
Like if you compliment them, or if you ask it,

00:07:40.100 --> 00:07:42.300
you sort of tell it, like, I really want you to tell me.

00:07:42.300 --> 00:07:43.820
It actually makes a difference, right?

00:07:43.820 --> 00:07:45.140
It's not just like a search engine.

00:07:45.140 --> 00:07:46.340
Like, well, of course, what does it care?

00:07:46.340 --> 00:07:48.140
Just you put these keywords in and they come out.

00:07:48.140 --> 00:07:49.620
Like, no, you've kind of got to like,

00:07:49.620 --> 00:07:51.140
know how to talk to it just a little bit.

00:07:51.140 --> 00:07:52.780
- I've seen people threatening them,

00:07:52.780 --> 00:07:55.740
or like saying that someone's being held ransom,

00:07:55.740 --> 00:07:58.780
or, you know, I like to say, my boss is really mad at me.

00:07:58.780 --> 00:07:59.940
Like, help me out here, right?

00:07:59.940 --> 00:08:01.540
And like, see if it'll generate some better code.

00:08:01.540 --> 00:08:03.420
- You're not being a good user.

00:08:03.420 --> 00:08:04.700
You're trying to trick me.

00:08:04.700 --> 00:08:07.100
I've been a good chatbot and you've been a bad user

00:08:07.100 --> 00:08:08.380
and I'm not gonna help you anymore.

00:08:08.380 --> 00:08:09.220
- Yeah, right.

00:08:09.220 --> 00:08:11.700
So it was actually basically a conversation

00:08:11.700 --> 00:08:12.980
from being in the early days.

00:08:12.980 --> 00:08:14.020
- Yeah, the Sydney episode.

00:08:14.020 --> 00:08:15.380
Yeah, that was crazy, right?

00:08:15.380 --> 00:08:16.220
Super funny.

00:08:16.220 --> 00:08:17.180
- How funny?

00:08:17.180 --> 00:08:18.780
All right, well, I'm sure a lot of people out there

00:08:18.780 --> 00:08:20.580
are using AI these days.

00:08:20.580 --> 00:08:23.100
I think I saw a quote from, I think it was from GitHub,

00:08:23.100 --> 00:08:26.660
saying over 50% of developers are using Copilot.

00:08:26.660 --> 00:08:27.500
- For sure.

00:08:27.500 --> 00:08:29.820
- Which, crazy, but I mean, not that surprising.

00:08:29.820 --> 00:08:31.780
50% of the people are using Autocomplete.

00:08:31.780 --> 00:08:34.020
So I guess it kind of, kind of like that, right?

00:08:34.020 --> 00:08:34.860
- They're great tools.

00:08:34.860 --> 00:08:35.700
They're gonna keep evolving.

00:08:35.700 --> 00:08:36.780
There's some other ones I'm keeping an eye on.

00:08:36.780 --> 00:08:38.220
There's one called Console,

00:08:38.220 --> 00:08:39.540
which just takes a different approach.

00:08:39.540 --> 00:08:41.260
They use some stronger models.

00:08:41.260 --> 00:08:43.820
And then there's a website called Find, P-H-I-N-D,

00:08:43.820 --> 00:08:45.580
that allows you to do some searching,

00:08:45.580 --> 00:08:47.140
that they've built their own custom model.

00:08:47.140 --> 00:08:48.660
Really interesting companies that are doing

00:08:48.660 --> 00:08:49.860
some really cool things.

00:08:49.860 --> 00:08:52.860
And then Perplexity is like the search replacement

00:08:52.860 --> 00:08:54.900
that a lot of folks are very excited about using

00:08:54.900 --> 00:08:55.740
instead of Google.

00:08:55.740 --> 00:08:57.460
So there's a lot of different tools out there.

00:08:57.460 --> 00:08:59.660
You could spend all your day just kind of playing around

00:08:59.660 --> 00:09:01.340
and learning these things where you got to actually

00:09:01.340 --> 00:09:02.460
kind of get some stuff done too.

00:09:02.460 --> 00:09:04.140
- Yeah, you gotta pick something and go, right?

00:09:04.140 --> 00:09:08.140
Because with all the churn and growth and experimentation

00:09:08.140 --> 00:09:09.940
you got, you probably could try a new tool every day

00:09:09.940 --> 00:09:12.260
and still not try them all, you know,

00:09:12.260 --> 00:09:13.580
just be falling farther behind.

00:09:13.580 --> 00:09:15.260
So you gotta pick something and go.

00:09:15.260 --> 00:09:16.100
- And go, yep.

00:09:16.100 --> 00:09:18.860
- Let's talk about writing some code.

00:09:18.860 --> 00:09:20.220
- Yeah, the next thing you're gonna do

00:09:20.220 --> 00:09:24.940
after you use a chatbot is to hit an API.

00:09:24.940 --> 00:09:26.580
Like if you're gonna program an app

00:09:26.580 --> 00:09:28.540
and that app is gonna have LLM inside of it,

00:09:28.540 --> 00:09:30.100
large language models inside of it,

00:09:30.100 --> 00:09:32.260
APIs are pretty much the next step, right?

00:09:32.260 --> 00:09:35.540
So OpenAI has different models that are available.

00:09:35.540 --> 00:09:37.220
This is a webpage that I just saw recently

00:09:37.220 --> 00:09:39.300
that'll actually compare the different models

00:09:39.300 --> 00:09:40.140
that are out there.

00:09:40.140 --> 00:09:42.220
So there's obviously the big guy, which is OpenAI,

00:09:42.220 --> 00:09:44.420
and you can get that through Azure as well

00:09:44.420 --> 00:09:46.340
if you have a Microsoft arrangement.

00:09:46.340 --> 00:09:48.940
And there's some security reasons or HIPAA compliance

00:09:48.940 --> 00:09:51.860
and some other reasons that you might want to talk

00:09:51.860 --> 00:09:54.140
through Azure instead of going directly to OpenAI.

00:09:54.140 --> 00:09:57.540
I defer to your IT department about that.

00:09:57.540 --> 00:10:00.260
Google has Gemini, which they just released the pro version,

00:10:00.260 --> 00:10:03.500
which I believe is as strong as 3.5, roughly.

00:10:03.500 --> 00:10:06.420
That is interesting because if you don't care

00:10:06.420 --> 00:10:07.860
about them training on your data,

00:10:07.860 --> 00:10:10.100
if like whatever you're doing is just like

00:10:10.100 --> 00:10:11.580
not super proprietary or something

00:10:11.580 --> 00:10:12.700
you're trying to keep secret,

00:10:12.700 --> 00:10:15.460
they're offering free API access,

00:10:15.460 --> 00:10:17.620
I believe 60 words per minute, right?

00:10:17.620 --> 00:10:19.500
So basically one a second, you can call this thing

00:10:19.500 --> 00:10:21.020
and there's no charge.

00:10:21.020 --> 00:10:23.460
So I don't know how long that's gonna last.

00:10:23.460 --> 00:10:24.620
So if you have an interesting project

00:10:24.620 --> 00:10:26.660
that you wanna use in a large language model for,

00:10:26.660 --> 00:10:27.700
you might wanna look at that.

00:10:27.700 --> 00:10:29.820
- Yeah, especially if it's already open data

00:10:29.820 --> 00:10:30.700
that you're playing with.

00:10:30.700 --> 00:10:31.540
- Exactly, right.

00:10:31.540 --> 00:10:33.300
- Or data you've somehow published to the web

00:10:33.300 --> 00:10:36.180
that has certainly been consumed by these things.

00:10:36.180 --> 00:10:37.860
And these models are gonna train on it, right?

00:10:37.860 --> 00:10:38.820
That's the trade, right?

00:10:38.820 --> 00:10:40.700
They're trying to get more tokens,

00:10:40.700 --> 00:10:41.540
is what they call it, right?

00:10:41.540 --> 00:10:43.620
The tokens are what they need to actually

00:10:43.620 --> 00:10:44.700
make these models smarter.

00:10:44.700 --> 00:10:47.260
So everyone's just hunting for more tokens

00:10:47.260 --> 00:10:49.300
and I think this is part of their strategy for that.

00:10:49.300 --> 00:10:52.020
And then there's also a CLAUDE by Anthropic.

00:10:52.020 --> 00:10:53.580
And then after that, you get into the,

00:10:53.580 --> 00:10:55.820
kind of the open source APIs as well.

00:10:55.820 --> 00:10:58.220
- There's some really powerful open source ones out there.

00:10:58.220 --> 00:11:01.260
Yeah, so this website, yeah, this is DocsBot,

00:11:01.260 --> 00:11:03.540
for people listening, DocsBot.ai.

00:11:03.540 --> 00:11:05.980
Is its sole purpose just to tell you

00:11:05.980 --> 00:11:07.380
price comparisons and stuff like that?

00:11:07.380 --> 00:11:08.620
Or does it have more than that?

00:11:08.620 --> 00:11:10.620
- I assume this company's got some product,

00:11:10.620 --> 00:11:11.740
unfortunately I don't know what it is.

00:11:11.740 --> 00:11:13.700
I saw this link that they put out there

00:11:13.700 --> 00:11:14.900
and it's a calculator.

00:11:14.900 --> 00:11:16.940
So you basically can put your, what tokens,

00:11:16.940 --> 00:11:18.500
how many tokens, there's input tokens

00:11:18.500 --> 00:11:19.700
and there's output tokens, right?

00:11:19.700 --> 00:11:22.860
So they're gonna charge more on the output tokens.

00:11:22.860 --> 00:11:23.980
That's for the most part.

00:11:23.980 --> 00:11:26.780
Some of the libraries, some of the models are more equal.

00:11:26.780 --> 00:11:28.140
And then they, what they do is,

00:11:28.140 --> 00:11:30.220
if you can figure out like roughly how big a message

00:11:30.220 --> 00:11:32.420
is gonna be, both the input and the output,

00:11:32.420 --> 00:11:33.500
how many calls you're gonna make,

00:11:33.500 --> 00:11:37.060
you can use that to then calculate basically the cost.

00:11:37.060 --> 00:11:39.700
And the cost is always at like tokens per thousand,

00:11:39.700 --> 00:11:42.140
you know, or dollars or pennies really,

00:11:42.140 --> 00:11:43.820
pennies per thousand tokens.

00:11:43.820 --> 00:11:45.700
And then it's just a math equation at that point.

00:11:45.700 --> 00:11:47.900
And what you'll find is calling GPT-4

00:11:47.900 --> 00:11:49.380
is gonna be super expensive.

00:11:49.380 --> 00:11:51.980
And then calling, you know, a small seven,

00:11:51.980 --> 00:11:53.740
what's called a 7B model from Mistral

00:11:53.740 --> 00:11:55.060
is gonna be the cheapest.

00:11:55.060 --> 00:11:57.220
And you're just gonna look for these different providers.

00:11:57.220 --> 00:11:59.180
- Wow, the prices really are different.

00:11:59.180 --> 00:12:02.340
Like for example, opening an Azure GPT-4

00:12:02.340 --> 00:12:05.500
is three, a little over three cents per call.

00:12:05.500 --> 00:12:10.500
Whereas GPT-3, five turbo is one 10th of one cent.

00:12:10.500 --> 00:12:13.460
It's a big difference there.

00:12:13.460 --> 00:12:16.980
11 cents versus $3 to have a conversation with it.

00:12:16.980 --> 00:12:18.580
- Yes, it's a very, very wide difference.

00:12:18.580 --> 00:12:20.020
And it's all based on, you know,

00:12:20.020 --> 00:12:21.900
how much compute do these models take, right?

00:12:21.900 --> 00:12:25.100
'Cause the bigger the model, the more accurate it is.

00:12:25.100 --> 00:12:27.060
But also the more expensive it is for them to run it.

00:12:27.060 --> 00:12:29.420
So that's why there's such a cost difference.

00:12:31.060 --> 00:12:32.460
- This portion of Talk Python to Me

00:12:32.460 --> 00:12:33.820
is brought to you by Sentry.

00:12:33.820 --> 00:12:36.340
In the last episode, I told you about how we use Sentry

00:12:36.340 --> 00:12:38.500
to solve a tricky problem.

00:12:38.500 --> 00:12:40.740
This time, I wanna talk about making your front end

00:12:40.740 --> 00:12:43.980
and backend code work more tightly together.

00:12:43.980 --> 00:12:46.620
If you're having a hard time getting a complete picture

00:12:46.620 --> 00:12:49.380
of how your app is working and how requests flow

00:12:49.380 --> 00:12:51.380
from the front end JavaScript app,

00:12:51.380 --> 00:12:52.820
back to your Python services,

00:12:52.820 --> 00:12:56.020
down into database calls for errors and performance,

00:12:56.020 --> 00:12:59.680
you should definitely check out Sentry's distributed tracing.

00:12:59.680 --> 00:13:00.860
With distributed tracing,

00:13:00.860 --> 00:13:03.140
you'll be able to track your software's performance,

00:13:03.140 --> 00:13:05.880
measure metrics like throughput and latency,

00:13:05.880 --> 00:13:09.680
and display the impact of errors across multiple systems.

00:13:09.680 --> 00:13:11.220
Distributed tracing makes Sentry

00:13:11.220 --> 00:13:13.940
a more complete performance monitoring solution,

00:13:13.940 --> 00:13:15.420
helping you diagnose problems

00:13:15.420 --> 00:13:19.020
and measure your application's overall health more quickly.

00:13:19.020 --> 00:13:20.780
Tracing in Sentry provides insights

00:13:20.780 --> 00:13:24.580
such as what occurred for a specific event or issue,

00:13:24.580 --> 00:13:27.780
the conditions that cause bottlenecks or latency issues,

00:13:27.780 --> 00:13:30.760
and the endpoints and operations that consume the most time.

00:13:30.760 --> 00:13:32.400
Help your front end and backend teams

00:13:32.400 --> 00:13:34.400
work seamlessly together.

00:13:34.400 --> 00:13:36.160
Check out Sentry's distributed tracing

00:13:36.160 --> 00:13:40.060
at talkpython.fm/sentry-trace.

00:13:40.060 --> 00:13:43.820
That's talkpython.fm/sentry-trace.

00:13:43.820 --> 00:13:44.840
And when you sign up,

00:13:44.840 --> 00:13:48.800
please use our code TALKPYTHON, all caps, no spaces,

00:13:48.800 --> 00:13:50.120
to get more features

00:13:50.120 --> 00:13:52.180
and let them know that you came from us.

00:13:52.180 --> 00:13:54.380
Thank you to Sentry for supporting the show.

00:13:55.860 --> 00:13:57.320
- Yeah, I recently interviewed,

00:13:57.320 --> 00:13:58.760
just released a while ago,

00:13:58.760 --> 00:14:00.440
interviewed because of time shifting,

00:14:00.440 --> 00:14:03.400
on podcast, Mark Russinovich, CTO of Azure,

00:14:03.400 --> 00:14:06.120
and we talked about all the crazy stuff that they're doing

00:14:06.120 --> 00:14:07.640
for coming up with the,

00:14:07.640 --> 00:14:09.240
just running these computers

00:14:09.240 --> 00:14:10.480
that handle all of this compute,

00:14:10.480 --> 00:14:12.120
and it's really a lot.

00:14:12.120 --> 00:14:13.680
- There was a GPU shortage for a while.

00:14:13.680 --> 00:14:14.960
I don't know if that's still going on.

00:14:14.960 --> 00:14:16.240
And obviously, you know,

00:14:16.240 --> 00:14:18.680
the big companies are buying hundreds of thousands

00:14:18.680 --> 00:14:21.280
of these GPUs to get the scale they need.

00:14:21.280 --> 00:14:22.120
- Yeah.

00:14:22.120 --> 00:14:24.040
- And so once you figure out which API you want to use,

00:14:24.040 --> 00:14:26.060
then you want to talk about the library.

00:14:26.060 --> 00:14:26.900
So now, you know,

00:14:26.900 --> 00:14:28.100
most of these providers,

00:14:28.100 --> 00:14:28.940
they have, you know,

00:14:28.940 --> 00:14:30.500
a Python library that they offer.

00:14:30.500 --> 00:14:31.500
I know OpenAI does,

00:14:31.500 --> 00:14:34.140
and Google, Gemini does,

00:14:34.140 --> 00:14:35.980
but there's also open source ones, right?

00:14:35.980 --> 00:14:38.940
'Cause they're not very complicated to talk to.

00:14:38.940 --> 00:14:41.140
It's just basically HTTP requests.

00:14:41.140 --> 00:14:42.520
So it's just really a matter of like,

00:14:42.520 --> 00:14:44.620
what's the ergonomics you're looking for as a developer

00:14:44.620 --> 00:14:46.780
to interact with these things.

00:14:46.780 --> 00:14:47.860
And most importantly,

00:14:47.860 --> 00:14:49.500
make sure you're maintaining optionality, right?

00:14:49.500 --> 00:14:51.880
Like, it's great to do a prototype

00:14:51.880 --> 00:14:53.660
with one of these models,

00:14:53.660 --> 00:14:54.980
but recognize you might want to switch

00:14:54.980 --> 00:14:57.240
either for cost reasons or performance reasons

00:14:57.240 --> 00:14:58.700
or what have you.

00:14:58.700 --> 00:14:59.540
And, you know,

00:14:59.540 --> 00:15:00.360
LangChain, for instance,

00:15:00.360 --> 00:15:03.180
has a ton of the providers as part of,

00:15:03.180 --> 00:15:06.500
you basically are just switching a few arguments

00:15:06.500 --> 00:15:08.540
when you're switching between them.

00:15:08.540 --> 00:15:10.500
And then Simon Willison has, you know,

00:15:10.500 --> 00:15:11.340
a Python fame,

00:15:11.340 --> 00:15:14.300
has an LLM project where he's defined,

00:15:14.300 --> 00:15:15.740
you know, basically a set of,

00:15:15.740 --> 00:15:18.140
and it's really clean just the way he's organized it,

00:15:18.140 --> 00:15:20.220
because you can just add plugins as you need them, right?

00:15:20.220 --> 00:15:21.700
So you don't have to install

00:15:21.700 --> 00:15:23.040
all the different libraries that are out there.

00:15:23.040 --> 00:15:24.420
And I think LangChain is kind of following

00:15:24.420 --> 00:15:25.260
a similar approach.

00:15:25.260 --> 00:15:28.820
I think they're coming up with a LangChain core capability

00:15:28.820 --> 00:15:30.120
where you can just kind of bring in things

00:15:30.120 --> 00:15:31.300
as you need them.

00:15:31.300 --> 00:15:34.940
And so the idea is you're now coding against these libraries

00:15:34.940 --> 00:15:37.980
and you're trying to bring together, you know,

00:15:37.980 --> 00:15:39.660
the text you need to have analyzed

00:15:39.660 --> 00:15:41.140
or whatever your use case is,

00:15:41.140 --> 00:15:43.340
and then it'll come back with the generation.

00:15:43.340 --> 00:15:45.180
And you can also not just use them on the cloud,

00:15:45.180 --> 00:15:46.780
you can use open source ones as well

00:15:46.780 --> 00:15:48.660
and run them locally on your local computer.

00:15:48.660 --> 00:15:50.220
- I'd never really thought about

00:15:50.220 --> 00:15:52.780
my architectural considerations,

00:15:52.780 --> 00:15:54.600
I guess, of these sorts of things.

00:15:54.600 --> 00:15:56.360
But of course you want to set up

00:15:56.360 --> 00:15:58.200
some kind of abstraction layer

00:15:58.200 --> 00:16:02.080
so you're not completely tied into some provider.

00:16:02.080 --> 00:16:03.920
It could be that it becomes too expensive.

00:16:03.920 --> 00:16:05.240
It could be that it becomes too slow,

00:16:05.240 --> 00:16:07.680
but it also might just be something that's better.

00:16:07.680 --> 00:16:09.600
It could be something else that comes along that's better

00:16:09.600 --> 00:16:11.240
and you're like, "Eh, we could switch.

00:16:11.240 --> 00:16:12.840
"It's 25% better."

00:16:12.840 --> 00:16:16.140
But it's like a week to pull all the details

00:16:16.140 --> 00:16:18.360
of this one LLM out and put the new ones in,

00:16:18.360 --> 00:16:19.640
and so it's not worth it, right?

00:16:19.640 --> 00:16:23.220
So you like having, being tied to a particular database

00:16:23.220 --> 00:16:25.620
rather than more general, it's a similar idea.

00:16:25.620 --> 00:16:27.380
- And especially at this moment in time, right?

00:16:27.380 --> 00:16:29.140
Every couple months, something,

00:16:29.140 --> 00:16:30.820
so something from the bottom up

00:16:30.820 --> 00:16:32.100
is getting better and better.

00:16:32.100 --> 00:16:34.740
Meaning, you know, LLAMA came out a year ago,

00:16:34.740 --> 00:16:37.100
and then LLAMA 2 and Mistral and Mixtral,

00:16:37.100 --> 00:16:39.460
and LLAMA 3 is gonna be coming out later

00:16:39.460 --> 00:16:40.540
this year, we believe.

00:16:40.540 --> 00:16:42.940
And so those models, which are smaller

00:16:42.940 --> 00:16:45.100
and cheaper and easier to use,

00:16:45.100 --> 00:16:46.880
or not easier to use, but they're just cheaper,

00:16:46.880 --> 00:16:48.820
is those things are happening all the time.

00:16:48.820 --> 00:16:50.800
So being able to be flexible and nimble

00:16:50.800 --> 00:16:52.340
and kind of change where you are

00:16:52.340 --> 00:16:54.440
is gonna be crucial, at least for the next couple years.

00:16:54.440 --> 00:16:56.480
- Yeah, the example that I gave was databases, right?

00:16:56.480 --> 00:17:00.080
And databases have been kind of a known commodity

00:17:00.080 --> 00:17:02.440
since the '80s, or what, 1980s?

00:17:02.440 --> 00:17:04.300
And of course, there's new ones that come along,

00:17:04.300 --> 00:17:05.780
but they're kind of all the same,

00:17:05.780 --> 00:17:08.040
and you know, we got, there was MySQL,

00:17:08.040 --> 00:17:11.080
now there's Postgres that people love, and right?

00:17:11.080 --> 00:17:14.360
So that is changing way, way slower than this.

00:17:14.360 --> 00:17:15.780
And people are like, "Well, we gotta think about

00:17:15.780 --> 00:17:17.840
"those kinds of, like, don't get tied into that."

00:17:17.840 --> 00:17:19.740
Well, it's way less stable.

00:17:19.740 --> 00:17:21.240
- Right, and people, you know,

00:17:21.240 --> 00:17:23.360
create layers of abstraction there, too,

00:17:23.360 --> 00:17:25.080
is where you got SQLAlchemy,

00:17:25.080 --> 00:17:28.720
and then, you know, Sebastian from FastAPI has SQLModel.

00:17:28.720 --> 00:17:30.820
That's a layer on top of SQLAlchemy, you know,

00:17:30.820 --> 00:17:32.100
and then there's also, you know,

00:17:32.100 --> 00:17:34.900
folks that just like writing clean, fancy SQL,

00:17:34.900 --> 00:17:36.720
and you can, you know, hopefully be able to port that

00:17:36.720 --> 00:17:38.040
from database to database as well.

00:17:38.040 --> 00:17:41.080
So it's the same principles, separation of concerns,

00:17:41.080 --> 00:17:42.600
so you can kind of be flexible.

00:17:42.600 --> 00:17:44.600
- All right, so you talked about LangChain.

00:17:44.600 --> 00:17:47.080
Just give us a sense, real quick, of what LangChain is.

00:17:47.080 --> 00:17:49.840
- This was a great project from a timing perspective.

00:17:49.840 --> 00:17:51.320
I believe they kind of invented it

00:17:51.320 --> 00:17:53.680
and released it right around the time ChatGPT came out.

00:17:53.680 --> 00:17:56.400
It's a very comprehensive library with lots of,

00:17:56.400 --> 00:17:58.240
I mean, the best part about LangChain, to me,

00:17:58.240 --> 00:18:00.480
is the documentation and the code samples, right?

00:18:00.480 --> 00:18:02.360
Because if you want to learn how to interact

00:18:02.360 --> 00:18:04.320
with a different large language model

00:18:04.320 --> 00:18:06.280
or work with a vector database,

00:18:06.280 --> 00:18:08.000
there's another library called LamaIndex

00:18:08.000 --> 00:18:09.960
that does a really good job at this as well.

00:18:09.960 --> 00:18:12.320
They have tons and tons of documentation and examples,

00:18:12.320 --> 00:18:15.120
so you can kind of look at those and try to understand it.

00:18:15.120 --> 00:18:17.400
The chaining part really came from the idea of like,

00:18:17.400 --> 00:18:20.200
okay, I prompt the large language model, it gives a response,

00:18:20.200 --> 00:18:22.800
now I'm gonna take that response and prompt it,

00:18:22.800 --> 00:18:25.880
again, with a new prompt using that output.

00:18:25.880 --> 00:18:27.240
The challenge with that

00:18:27.240 --> 00:18:29.000
is the reliability of these models, right?

00:18:29.000 --> 00:18:30.680
They're not gonna get close,

00:18:30.680 --> 00:18:34.080
they're not close to 100% accurate on these types of tasks.

00:18:34.080 --> 00:18:36.280
The idea of agents as well is another thing

00:18:36.280 --> 00:18:38.320
that you might build with a LangChain.

00:18:38.320 --> 00:18:42.160
And the idea there is basically the agent is getting a task,

00:18:42.160 --> 00:18:44.120
coming up with a plan for that task,

00:18:44.120 --> 00:18:46.600
and then kind of stepping through those tasks

00:18:46.600 --> 00:18:48.120
to get the job done.

00:18:48.120 --> 00:18:49.400
Once again, we're just not there yet

00:18:49.400 --> 00:18:52.120
as far as those technologies,

00:18:52.120 --> 00:18:54.280
just because of the reliability.

00:18:54.280 --> 00:18:56.320
And then there's also a bunch of security concerns

00:18:56.320 --> 00:18:57.760
that are out there too,

00:18:57.760 --> 00:18:59.160
that you should definitely be aware of.

00:18:59.160 --> 00:19:01.120
Like one term to Google

00:19:01.120 --> 00:19:03.520
and make sure you understand is prompt injection.

00:19:03.520 --> 00:19:05.720
Right, so Simon, once again, he's got a great blog.

00:19:05.720 --> 00:19:07.120
He's got a great blog article,

00:19:07.120 --> 00:19:09.200
or just even that tag on his blog

00:19:09.200 --> 00:19:11.840
is tons of articles around prompt injection.

00:19:11.840 --> 00:19:14.680
And prompt injection is basically the idea,

00:19:14.680 --> 00:19:17.320
you have an app, a user says something in the app,

00:19:17.320 --> 00:19:20.400
or like types into the, to the, whatever the input is,

00:19:20.400 --> 00:19:22.240
and whatever texts that they're sending through,

00:19:22.240 --> 00:19:23.400
just like with SQL injection,

00:19:23.400 --> 00:19:25.120
they kind of hijacks the conversation

00:19:25.120 --> 00:19:26.960
and causes the large language model

00:19:26.960 --> 00:19:28.000
to kind of do a different thing.

00:19:28.000 --> 00:19:30.080
- Little Bobby Llama, we call him.

00:19:30.080 --> 00:19:31.720
Instead of little Bobby Diggles.

00:19:31.720 --> 00:19:33.440
- And then the other wild one is like, you know,

00:19:33.440 --> 00:19:35.400
people are putting stuff up on the internet

00:19:35.400 --> 00:19:38.560
so that when the large language model browses for webpages

00:19:38.560 --> 00:19:41.160
and brings back text, it's, you know, reading the HTML

00:19:41.160 --> 00:19:42.920
or reading the text in the HTML,

00:19:42.920 --> 00:19:44.320
and it's causing the large language model

00:19:44.320 --> 00:19:46.040
to behave in some unexpected way.

00:19:46.040 --> 00:19:49.400
So there's lots of crazy challenges out there.

00:19:49.400 --> 00:19:51.160
- I'm sure there's a lot of adversarial stuff

00:19:51.160 --> 00:19:52.360
happening to these things

00:19:52.360 --> 00:19:54.520
as they're both trying to gather data

00:19:54.520 --> 00:19:56.120
and then trying to run, right?

00:19:56.120 --> 00:19:59.720
I saw the most insane, I guess it was an article,

00:19:59.720 --> 00:20:01.280
I saw it on RSS somewhere.

00:20:01.280 --> 00:20:03.800
And it was saying that on Amazon,

00:20:03.800 --> 00:20:06.600
there's all these knockoff brands that are trying to,

00:20:06.600 --> 00:20:09.440
you know, instead of Gucci, you have a Gucci

00:20:09.440 --> 00:20:11.040
or I don't know, whatever, right?

00:20:11.040 --> 00:20:13.640
And they're getting so lazy.

00:20:13.640 --> 00:20:14.520
I don't know what the right word is,

00:20:14.520 --> 00:20:17.640
that they're using LLMs to try to write a description

00:20:17.640 --> 00:20:20.960
that is sort of in the style of Gucci, let's say.

00:20:20.960 --> 00:20:22.040
And it'll come back and say,

00:20:22.040 --> 00:20:24.440
I'm sorry, I'm a large language model.

00:20:24.440 --> 00:20:27.080
I'm not, my rules forbid me

00:20:27.080 --> 00:20:31.320
from doing brand trademark violation.

00:20:31.320 --> 00:20:33.720
That's what the Amazon listing says on Amazon.

00:20:33.720 --> 00:20:35.600
They just take it and they just straight pump it straight,

00:20:35.600 --> 00:20:37.520
whatever it says, it just goes straight into Amazon.

00:20:37.520 --> 00:20:38.440
- Yeah, you have to like Google, like,

00:20:38.440 --> 00:20:40.400
sorry, I'm not, sorry, as a large language model

00:20:40.400 --> 00:20:41.280
or sorry as a whatever.

00:20:41.280 --> 00:20:42.120
Yeah, you can find all. - Exactly.

00:20:42.120 --> 00:20:44.720
And there's like, the product listings are full of that.

00:20:44.720 --> 00:20:45.880
It's amazing.

00:20:45.880 --> 00:20:47.040
It's amazing. - It's crazy.

00:20:47.040 --> 00:20:49.640
- Certainly the reliability of that is, you know,

00:20:49.640 --> 00:20:50.960
they could probably use some testing

00:20:50.960 --> 00:20:52.480
and those kinds of things.

00:20:52.480 --> 00:20:53.560
- For sure. - Someone out there asks,

00:20:53.560 --> 00:20:56.160
like, I wonder if for local LLM models,

00:20:56.160 --> 00:20:57.960
there's a similar site as DocSpot

00:20:57.960 --> 00:21:00.200
that show you like what you need to run it locally.

00:21:00.200 --> 00:21:02.240
So that's an interesting question also,

00:21:02.240 --> 00:21:04.400
segue to maybe talk about like some local stuff.

00:21:04.400 --> 00:21:06.640
- LLM Studio, this is a new product.

00:21:06.640 --> 00:21:08.560
I honestly haven't had a chance to like really dig in

00:21:08.560 --> 00:21:09.960
and understand who created this.

00:21:09.960 --> 00:21:12.200
And, you know, make sure that the privacy stuff

00:21:12.200 --> 00:21:14.600
is up to snuff, but I've played around with it locally.

00:21:14.600 --> 00:21:15.640
It seems to work great.

00:21:15.640 --> 00:21:17.240
It's really slick, really nice user interface.

00:21:17.240 --> 00:21:19.360
So if you're just wanting to get your feet wet

00:21:19.360 --> 00:21:21.240
and try to understand some of these models,

00:21:21.240 --> 00:21:22.920
I'd download that and check it out.

00:21:22.920 --> 00:21:25.280
There's a ton of models up on Hugging Face.

00:21:25.280 --> 00:21:27.320
This product seems to just basically link right

00:21:27.320 --> 00:21:31.280
into the Hugging Face interface and grabs models.

00:21:31.280 --> 00:21:33.160
And so some of the models you wanna look for

00:21:33.160 --> 00:21:35.080
are right now as in January, right?

00:21:35.080 --> 00:21:39.240
There's Mistral 7B, you know, M-I-S-T-R-A-L.

00:21:39.240 --> 00:21:41.080
There's another one called Phi 2.

00:21:41.080 --> 00:21:42.760
Those are two of the smaller models

00:21:42.760 --> 00:21:44.840
that should run pretty well on, you know,

00:21:44.840 --> 00:21:48.800
like a commercial grade GPU or an M1 or an M2 Mac,

00:21:48.800 --> 00:21:51.960
if that's what you have and start playing with them.

00:21:51.960 --> 00:21:54.760
And they're quantized, which means they're just kind of

00:21:54.760 --> 00:21:56.720
made a little, take a little bit less space,

00:21:56.720 --> 00:21:58.960
which is good from like a virtual RAM

00:21:58.960 --> 00:22:00.880
with regards to these GPUs.

00:22:00.880 --> 00:22:03.640
And, you know, there's a account on Hugging Face

00:22:03.640 --> 00:22:04.600
called The Bloke.

00:22:04.600 --> 00:22:07.200
If you look for him, you'll see all his different fine,

00:22:07.200 --> 00:22:09.360
his different fine tunes and things like that.

00:22:09.360 --> 00:22:11.200
And there's a group called Noose,

00:22:11.200 --> 00:22:13.520
I think is how you pronounce it, N-O-U-S.

00:22:13.520 --> 00:22:15.600
And they've got some of the fine tunes

00:22:15.600 --> 00:22:18.280
that are basically the highest performing ones

00:22:18.280 --> 00:22:19.280
that are out there.

00:22:19.280 --> 00:22:21.040
So if you're really looking for a high performing

00:22:21.040 --> 00:22:22.960
local model that can actually, you know,

00:22:22.960 --> 00:22:24.880
help you with code or reasoning,

00:22:24.880 --> 00:22:26.680
those are definitely the way to get started.

00:22:26.680 --> 00:22:28.320
- Yeah, this one seems pretty nice.

00:22:28.320 --> 00:22:30.840
I also haven't played with it, I just learned about it,

00:22:30.840 --> 00:22:32.520
but it's looking really good.

00:22:32.520 --> 00:22:35.840
I had played with, what was it, GPT for All,

00:22:35.840 --> 00:22:36.680
I think is what it was.

00:22:36.680 --> 00:22:37.520
- Yep, yep.

00:22:37.520 --> 00:22:38.640
- It was the one that I played with.

00:22:38.640 --> 00:22:39.880
Somehow this looks like,

00:22:39.880 --> 00:22:41.440
looks a little bit nicer than that for some,

00:22:41.440 --> 00:22:43.400
I don't know how different it really is, but.

00:22:43.400 --> 00:22:46.160
- I mean, it's all the idea of like downloading these files

00:22:46.160 --> 00:22:47.720
and running them locally.

00:22:47.720 --> 00:22:48.960
And these are just user interfaces

00:22:48.960 --> 00:22:50.320
that make it a little easier.

00:22:50.320 --> 00:22:52.800
The original project that made this stuff kind of possible

00:22:52.800 --> 00:22:54.920
was a project called Llama CPP.

00:22:54.920 --> 00:22:58.600
There's a Python library that can work with that directly.

00:22:58.600 --> 00:23:00.720
There's another project called Llama File,

00:23:00.720 --> 00:23:02.760
where if you download the whole thing,

00:23:02.760 --> 00:23:04.800
it actually runs no matter where you are.

00:23:04.800 --> 00:23:07.880
I think it runs on Mac and Linux and Windows and BSD

00:23:07.880 --> 00:23:09.000
or whatever it is.

00:23:09.000 --> 00:23:11.400
And it's, I mean, it's an amazing technology

00:23:11.400 --> 00:23:13.800
that this one put together, it's really impressive.

00:23:13.800 --> 00:23:17.240
And then, you can actually just use Google Colab too, right?

00:23:17.240 --> 00:23:19.800
So Google Colab has some GPUs with it.

00:23:19.800 --> 00:23:22.360
I think if you upgrade it to the $10 a month version,

00:23:22.360 --> 00:23:25.680
I think you get some better GPUs access.

00:23:25.680 --> 00:23:27.920
So if you actually wanna get a hand of like running,

00:23:27.920 --> 00:23:29.120
and so this is a little bit different, right?

00:23:29.120 --> 00:23:30.880
So instead of calling an API,

00:23:30.880 --> 00:23:32.320
when you're using Google Colab,

00:23:32.320 --> 00:23:34.520
you can actually use a library called Hugging Face,

00:23:34.520 --> 00:23:36.560
and then you can actually load these things directly

00:23:36.560 --> 00:23:39.920
into your memory, and then into your actual Python

00:23:39.920 --> 00:23:41.960
environment, and then you're working with it directly.

00:23:41.960 --> 00:23:44.080
So it just takes a little bit of work

00:23:44.080 --> 00:23:46.120
to make sure you're running it on the GPU,

00:23:46.120 --> 00:23:47.400
'cause if you're running it on the CPU,

00:23:47.400 --> 00:23:48.640
it's gonna be a lot slower.

00:23:48.640 --> 00:23:50.080
- Yeah, it definitely makes a big difference.

00:23:50.080 --> 00:23:53.080
There's a tool that I use that for a long time

00:23:53.080 --> 00:23:55.840
ran on the CPU, and they rewrote it to run on the GPU.

00:23:55.840 --> 00:23:59.240
Even on my M2 Pro, it was like three times faster

00:23:59.240 --> 00:24:00.440
or something, yeah. - For sure.

00:24:00.440 --> 00:24:01.880
- Yeah, it makes a big difference.

00:24:01.880 --> 00:24:06.160
- So with the LM Studio, let's you run the LMs offline

00:24:06.160 --> 00:24:08.880
and use models through an OpenAI,

00:24:08.880 --> 00:24:09.720
that's what I was looking for,

00:24:09.720 --> 00:24:12.440
the OpenAI compatible local server.

00:24:12.440 --> 00:24:13.560
- Right. - You could basically

00:24:13.560 --> 00:24:15.120
get an API for any of these,

00:24:15.120 --> 00:24:16.600
and then start programming against it, right?

00:24:16.600 --> 00:24:19.200
- Exactly right, and it's basically the same interface,

00:24:19.200 --> 00:24:22.720
right, so same APIs for posting in response

00:24:22.720 --> 00:24:25.800
of the JSON schema that's going back and forth.

00:24:25.800 --> 00:24:27.760
So you're programming against that interface,

00:24:27.760 --> 00:24:30.400
and then you basically port it and move it to another,

00:24:30.400 --> 00:24:33.120
to the OpenAI models if you wanted to as well.

00:24:33.120 --> 00:24:35.240
So everyone's kind of coalescing around OpenAI

00:24:35.240 --> 00:24:37.240
as kind of like the quote unquote standard,

00:24:37.240 --> 00:24:39.280
but there's nothing, you know, there's really no,

00:24:39.280 --> 00:24:41.720
there's no mode around that standard as well, right,

00:24:41.720 --> 00:24:44.360
'cause anybody can kind of adopt it and use it.

00:24:44.360 --> 00:24:47.720
- There's not like a W3C committee choosing.

00:24:47.720 --> 00:24:48.560
- Correct.

00:24:48.560 --> 00:24:51.080
- The market will choose for us, let's go.

00:24:51.080 --> 00:24:52.160
- It seems to be working out well,

00:24:52.160 --> 00:24:55.880
and that's another benefit of Simon's LLM project, right,

00:24:55.880 --> 00:24:57.680
he's got the ability to kind of switch back and forth

00:24:57.680 --> 00:25:00.720
between these different libraries and APIs as well.

00:25:00.720 --> 00:25:02.680
- This LLM studio says,

00:25:02.680 --> 00:25:05.680
"This app does not collect data nor monitor your actions.

00:25:05.680 --> 00:25:07.560
"Your data stays local on your machine,

00:25:07.560 --> 00:25:08.480
"free for personal use."

00:25:08.480 --> 00:25:09.560
All that sounds great.

00:25:09.560 --> 00:25:11.320
"For business use, please get in touch."

00:25:11.320 --> 00:25:12.880
I always just like these like,

00:25:12.880 --> 00:25:15.560
if you gotta ask, it's too much type of like.

00:25:15.560 --> 00:25:16.480
- Probably, yeah.

00:25:16.480 --> 00:25:17.480
I'm using it for personal use,

00:25:17.480 --> 00:25:19.960
just so if anybody's watching, yes, just plain.

00:25:19.960 --> 00:25:21.400
- Either they just haven't thought it through

00:25:21.400 --> 00:25:22.960
and they just don't wanna talk about it yet,

00:25:22.960 --> 00:25:24.480
or it's really expensive.

00:25:24.480 --> 00:25:25.880
I just probably imagine it's probably,

00:25:25.880 --> 00:25:27.360
it's like, ah, we haven't figured out a business model,

00:25:27.360 --> 00:25:29.200
just, I don't know, shoot us a note.

00:25:29.200 --> 00:25:30.560
- Nope, they're concentrating on the product,

00:25:30.560 --> 00:25:31.400
which makes sense.

00:25:31.400 --> 00:25:33.680
- Yeah, so then the other one is LLMafile,

00:25:33.680 --> 00:25:35.560
LLMafile.ai that you mentioned,

00:25:35.560 --> 00:25:36.600
and this packages it up.

00:25:36.600 --> 00:25:40.160
I guess, going back to the LLM studio real quick,

00:25:40.160 --> 00:25:41.720
one of the things that's cool about this

00:25:41.720 --> 00:25:45.640
is if it's the open AI API, right,

00:25:45.640 --> 00:25:47.600
with this little local server that you can play with,

00:25:47.600 --> 00:25:51.160
but then you can pick LLMs such as LLM, Falcon,

00:25:51.160 --> 00:25:54.760
Replit, all the different ones, right,

00:25:54.760 --> 00:25:56.400
Starcoder and so on.

00:25:56.400 --> 00:25:58.240
It would let you write an app

00:25:58.240 --> 00:25:59.800
as if it was going to open AI

00:25:59.800 --> 00:26:02.280
and then just start swapping in models

00:26:02.280 --> 00:26:04.280
and go like, oh, if we switch to this model, how'd that work?

00:26:04.280 --> 00:26:05.880
But you don't even have to change any code, right?

00:26:05.880 --> 00:26:08.480
Just probably maybe a string that says which model

00:26:08.480 --> 00:26:09.960
to initialize.

00:26:09.960 --> 00:26:12.240
- One of the tricks though is then the prompts themselves.

00:26:12.240 --> 00:26:13.400
- All right, let's talk about it.

00:26:13.400 --> 00:26:15.920
- Yeah, the models themselves act differently,

00:26:15.920 --> 00:26:17.440
and part of this whole world

00:26:17.440 --> 00:26:19.720
is what they call prompt engineering, right?

00:26:19.720 --> 00:26:22.120
So prompt engineering is really just exploring

00:26:22.120 --> 00:26:24.560
how to interact with these models,

00:26:24.560 --> 00:26:27.960
how to make sure that they're kind of in the right mind space

00:26:27.960 --> 00:26:29.240
to tackle your problem.

00:26:29.240 --> 00:26:31.040
A lot of the times that people get,

00:26:31.040 --> 00:26:32.160
when they struggle with these things,

00:26:32.160 --> 00:26:34.080
it's really just, they'd really got to think

00:26:34.080 --> 00:26:36.280
more like a psychiatrist when they're working with a model,

00:26:36.280 --> 00:26:39.320
you know, basically getting them kind of prepared.

00:26:39.320 --> 00:26:41.240
One of the tricks people did, figured out early,

00:26:41.240 --> 00:26:44.600
was you're a genius at software development,

00:26:44.600 --> 00:26:46.440
like compliment the thing, make it feel like,

00:26:46.440 --> 00:26:48.400
oh, I'm going to behave like I'm

00:26:48.400 --> 00:26:51.840
a world rockstar programmer, right?

00:26:51.840 --> 00:26:52.960
- Well, it's going to give you average,

00:26:52.960 --> 00:26:55.040
but if you tell them I'm genius, then let's start,

00:26:55.040 --> 00:26:55.880
we'll do that, yeah.

00:26:55.880 --> 00:26:58.000
- And there was also a theory like that in December

00:26:58.000 --> 00:27:00.360
that the large language models were getting dumber

00:27:00.360 --> 00:27:01.440
because it was the holidays

00:27:01.440 --> 00:27:02.600
and people don't work as hard, right?

00:27:02.600 --> 00:27:03.960
Like, it's really hard to know

00:27:03.960 --> 00:27:06.560
like which of these things are true or not,

00:27:06.560 --> 00:27:08.440
but it's definitely true that each model

00:27:08.440 --> 00:27:09.280
is a little bit different,

00:27:09.280 --> 00:27:11.480
and if you write a prompt that works really well

00:27:11.480 --> 00:27:14.120
on one model, even if it's a stronger model

00:27:14.120 --> 00:27:16.280
or a weaker model, and then you port it to another model

00:27:16.280 --> 00:27:20.320
and it's, you know, then the stronger model works worse,

00:27:20.320 --> 00:27:22.960
right, it can be very counterintuitive at times

00:27:22.960 --> 00:27:25.280
and you've got to test things out,

00:27:25.280 --> 00:27:27.560
and that really gets to the idea of evals, right?

00:27:27.560 --> 00:27:30.800
So evaluation is really a key problem, right?

00:27:30.800 --> 00:27:32.920
Making sure that if you're going to be writing prompts

00:27:32.920 --> 00:27:34.440
and you're going to be building, you know,

00:27:34.440 --> 00:27:37.520
different retrieval augmented generation solutions,

00:27:37.520 --> 00:27:40.120
you need to know about prompt injection

00:27:40.120 --> 00:27:41.960
and you need to know about prompt engineering

00:27:41.960 --> 00:27:44.720
and you need to know what these things can and can't do.

00:27:44.720 --> 00:27:47.280
One trick is what they call few-shot prompting,

00:27:47.280 --> 00:27:49.800
which is, you know, if you want it to do data extraction,

00:27:49.800 --> 00:27:51.920
you can say, oh, okay, I want you to extract data

00:27:51.920 --> 00:27:54.960
from text that I give you in JSON.

00:27:54.960 --> 00:27:56.400
If you give it a few examples,

00:27:56.400 --> 00:27:58.120
like wildly different examples,

00:27:58.120 --> 00:27:59.880
because giving it a bunch of similar stuff,

00:27:59.880 --> 00:28:01.800
it might kind of cause it to just coalesce

00:28:01.800 --> 00:28:03.160
around those similar examples,

00:28:03.160 --> 00:28:06.240
but if you can give it a wildly different set of examples,

00:28:06.240 --> 00:28:09.160
that's called in-context learning or few-shot prompting,

00:28:09.160 --> 00:28:12.040
and it will do a better job at that specific task for you.

00:28:12.040 --> 00:28:12.880
- That's super neat.

00:28:12.880 --> 00:28:14.840
When you're creating your apps,

00:28:14.840 --> 00:28:18.640
do you do things like, here's the input from the program

00:28:18.640 --> 00:28:20.800
or from the user or wherever it came from,

00:28:20.800 --> 00:28:22.040
but maybe before that,

00:28:22.040 --> 00:28:23.880
you give it like three or four prompts

00:28:23.880 --> 00:28:25.960
and then let it have the question, right,

00:28:25.960 --> 00:28:27.440
instead of just taking the text,

00:28:27.440 --> 00:28:31.240
like, I'm gonna ask you questions about biology and genetics

00:28:31.240 --> 00:28:33.000
and it's gonna be under this context,

00:28:33.000 --> 00:28:34.840
and I want you to favor these data sources.

00:28:34.840 --> 00:28:36.760
Now ask your question, something like this.

00:28:36.760 --> 00:28:38.840
- For sure, all those types of strategies

00:28:38.840 --> 00:28:40.160
are worth experimenting with, right?

00:28:40.160 --> 00:28:42.760
Like, what actually will work for your scenario?

00:28:42.760 --> 00:28:43.800
I can't tell you, right?

00:28:43.800 --> 00:28:45.360
You gotta dig in, you gotta figure it out,

00:28:45.360 --> 00:28:47.560
and you gotta try different things.

00:28:47.560 --> 00:28:49.120
- You're about to win the Nobel Prize

00:28:49.120 --> 00:28:50.600
in genetics for your work.

00:28:50.600 --> 00:28:52.640
Now I need to ask you some questions.

00:28:52.640 --> 00:28:53.640
- For sure, that'll definitely work,

00:28:53.640 --> 00:28:56.360
and then threatening it that your boss is mad at you

00:28:56.360 --> 00:28:58.000
is also gonna help you too, right, for sure.

00:28:58.000 --> 00:29:00.600
- If I don't solve this problem, I'm gonna get fired.

00:29:00.600 --> 00:29:02.520
As a large language model, I can't tell you,

00:29:02.520 --> 00:29:03.680
but I'm gonna be fired.

00:29:03.680 --> 00:29:05.240
All right, well, then the answer is.

00:29:05.240 --> 00:29:06.080
- Exactly right.

00:29:06.080 --> 00:29:07.720
- So for these, they run, like you said,

00:29:07.720 --> 00:29:09.360
they run pretty much locally,

00:29:09.360 --> 00:29:12.600
these different models on LM Studio and others,

00:29:12.600 --> 00:29:14.160
like the LLAMA file and so on.

00:29:14.160 --> 00:29:16.440
If I had a laptop, I don't need a cluster.

00:29:16.440 --> 00:29:18.560
LLAMA CPP is really the project that should get

00:29:18.560 --> 00:29:22.040
all the credit for making this work on your laptops.

00:29:22.040 --> 00:29:25.680
And then LLAMA file and LLAMA CPP all have servers.

00:29:25.680 --> 00:29:29.120
So I'm guessing LM Studio is just exposing that server.

00:29:29.120 --> 00:29:31.880
And that's in the base LLAMA CPP project.

00:29:31.880 --> 00:29:32.720
That's really what it is.

00:29:32.720 --> 00:29:36.560
It's really just about now you can post your requests.

00:29:36.560 --> 00:29:38.600
It's handling all of the work with regards

00:29:38.600 --> 00:29:41.880
to the token generation on the backend using LLAMA CPP,

00:29:41.880 --> 00:29:42.920
and then it's returning it to you

00:29:42.920 --> 00:29:45.520
using the HTTP kind of processes.

00:29:45.520 --> 00:29:47.240
- Is LLAMA originally from Meta?

00:29:47.240 --> 00:29:48.200
Is that where that came from?

00:29:48.200 --> 00:29:51.360
- I think there were people that were kind of using that LLM.

00:29:51.360 --> 00:29:53.400
Right, I think people were kind of keying off

00:29:53.400 --> 00:29:55.960
the LLAMA thing at one point.

00:29:55.960 --> 00:29:57.400
I think LLAMA Index, for instance,

00:29:57.400 --> 00:29:59.640
I think that project was originally called GPT Index,

00:29:59.640 --> 00:30:01.280
and they decided, oh, I don't want to be like,

00:30:01.280 --> 00:30:03.280
I don't want to confuse myself with OpenAI

00:30:03.280 --> 00:30:04.560
or confuse my project with OpenAI,

00:30:04.560 --> 00:30:06.240
so they switched to LLAMA Index.

00:30:06.240 --> 00:30:07.960
And then of course, Meta released LLAMA.

00:30:07.960 --> 00:30:09.720
So you can't, you kind of,

00:30:09.720 --> 00:30:11.720
and then everything from there has kind of evolved too,

00:30:11.720 --> 00:30:13.000
right, there's been alpacas

00:30:13.000 --> 00:30:13.840
and a bunch of other stuff as well.

00:30:13.840 --> 00:30:15.440
- You gotta know your animals, yeah.

00:30:15.440 --> 00:30:16.360
If you don't know your animals,

00:30:16.360 --> 00:30:19.000
you can't figure out the heritage of these projects.

00:30:19.000 --> 00:30:19.840
- Correct.

00:30:19.840 --> 00:30:22.840
LLAMA from Meta was the first open source,

00:30:22.840 --> 00:30:25.120
I'd say large language model of note,

00:30:25.120 --> 00:30:26.560
I guess, since ChatGPT.

00:30:26.560 --> 00:30:28.800
There were certainly other, you know,

00:30:28.800 --> 00:30:30.760
I'm not a, so one thing to caveat,

00:30:30.760 --> 00:30:32.080
I am not a researcher, right?

00:30:32.080 --> 00:30:33.960
So there's lots of folks in the ML research community

00:30:33.960 --> 00:30:35.200
that know way more than I do.

00:30:35.200 --> 00:30:37.800
But, because there was like Bloom and T5

00:30:37.800 --> 00:30:39.040
and a few other large, you know,

00:30:39.040 --> 00:30:40.280
quote unquote large language models.

00:30:40.280 --> 00:30:42.520
But LLAMA, after ChatGPT,

00:30:42.520 --> 00:30:44.800
LLAMA was the big release that came from Meta

00:30:44.800 --> 00:30:46.080
in I think March.

00:30:46.080 --> 00:30:47.840
And then, and that was from Meta.

00:30:47.840 --> 00:30:49.280
And then they had it released

00:30:49.280 --> 00:30:51.440
under just like research use terms.

00:30:51.440 --> 00:30:53.640
And then only certain people could get access to it.

00:30:53.640 --> 00:30:55.400
And then someone put a, I guess,

00:30:55.400 --> 00:30:58.560
put like a BitTorrent link or something on GitHub.

00:30:58.560 --> 00:31:00.240
And then basically the world had it.

00:31:00.240 --> 00:31:02.120
And then they did end up releasing LLAMA 2

00:31:02.120 --> 00:31:04.760
a few months later with more friendly terms.

00:31:04.760 --> 00:31:07.960
So that, and it was a much stronger model as well.

00:31:07.960 --> 00:31:08.800
- Nice.

00:31:08.800 --> 00:31:09.640
It's kind of the realization,

00:31:09.640 --> 00:31:10.720
like, well, if it's gonna be out there anyway,

00:31:10.720 --> 00:31:12.480
let's at least get credit for it then.

00:31:12.480 --> 00:31:13.320
- For sure.

00:31:13.320 --> 00:31:14.960
And I did read something where like basically

00:31:14.960 --> 00:31:18.200
Facebook approached OpenAI for access to their models

00:31:18.200 --> 00:31:19.040
to help them write code.

00:31:19.040 --> 00:31:19.880
But the cost was so high

00:31:19.880 --> 00:31:21.880
that they decided to just go build their own, right?

00:31:21.880 --> 00:31:25.000
So it's kind of interesting how this stuff has evolved.

00:31:25.000 --> 00:31:28.280
- Like, you know, we got a big cluster of computers too.

00:31:28.280 --> 00:31:29.840
- Metaverse thing doesn't seem to be working yet.

00:31:29.840 --> 00:31:31.640
So let's go ahead and train a bunch of

00:31:31.640 --> 00:31:32.600
large language models.

00:31:32.600 --> 00:31:33.440
- Yeah, exactly.

00:31:33.440 --> 00:31:35.760
We've got some spare capacity over in the Metaverse

00:31:35.760 --> 00:31:36.600
data center.

00:31:36.600 --> 00:31:38.480
All right, so one of the things that people

00:31:38.480 --> 00:31:40.320
will maybe talk about in this space is

00:31:40.320 --> 00:31:43.440
RAG or retrieval augmented generation.

00:31:43.440 --> 00:31:44.280
What's this?

00:31:44.280 --> 00:31:47.200
- One thing to recognize is that large language models,

00:31:47.200 --> 00:31:48.680
if it's not in the training set,

00:31:48.680 --> 00:31:50.080
and it's not in the prompt,

00:31:50.080 --> 00:31:51.600
it really doesn't know about it.

00:31:51.600 --> 00:31:54.080
And the question of like, what's reasoning

00:31:54.080 --> 00:31:56.400
and what's, you know, generalizing and things like that,

00:31:56.400 --> 00:31:58.120
those are big debates that people are having.

00:31:58.120 --> 00:31:59.640
What's intelligence, what have you.

00:31:59.640 --> 00:32:01.520
Recognizing the fact that you have this prompt

00:32:01.520 --> 00:32:02.960
and things you put in the prompt,

00:32:02.960 --> 00:32:04.480
the large language model can understand

00:32:04.480 --> 00:32:06.320
and extrapolate from is really powerful.

00:32:06.320 --> 00:32:08.920
So, and that's called in context learning.

00:32:08.920 --> 00:32:11.520
So retrieval augmented generation is the idea of,

00:32:11.520 --> 00:32:14.560
okay, I'm going to maybe ask,

00:32:14.560 --> 00:32:16.120
allow a person to ask a question.

00:32:16.120 --> 00:32:19.280
This is kind of like the common use case that I see.

00:32:19.280 --> 00:32:20.720
User asks a question,

00:32:20.720 --> 00:32:21.880
we're going to take that question,

00:32:21.880 --> 00:32:23.640
find the relevant content,

00:32:23.640 --> 00:32:25.400
put that content in the prompt,

00:32:25.400 --> 00:32:26.800
and then do something with it, right?

00:32:26.800 --> 00:32:28.400
So it might be something like, you know,

00:32:28.400 --> 00:32:30.560
ask a question about, you know,

00:32:30.560 --> 00:32:32.600
how tall is the Leaning Tower of Pisa, right?

00:32:32.600 --> 00:32:35.280
And so now it's going to go off and find that piece

00:32:35.280 --> 00:32:37.200
of content from Wikipedia or what have you,

00:32:37.200 --> 00:32:38.800
and then put that information in the prompt,

00:32:38.800 --> 00:32:42.840
and then now the model can then respond

00:32:42.840 --> 00:32:44.360
to that question based on that text.

00:32:44.360 --> 00:32:45.640
Obviously that's a pretty simple example,

00:32:45.640 --> 00:32:46.840
but you can get more complicated

00:32:46.840 --> 00:32:48.600
and it's going out and bringing back

00:32:48.600 --> 00:32:50.760
lots of different content, slicing it up,

00:32:50.760 --> 00:32:52.440
putting in the prompt and asking a question.

00:32:52.440 --> 00:32:54.200
So now the trick is, okay,

00:32:54.200 --> 00:32:55.720
how do you actually get that content

00:32:55.720 --> 00:32:57.040
and how do you do that?

00:32:57.040 --> 00:32:59.320
Well, you know, information retrieval,

00:32:59.320 --> 00:33:00.840
search engines and things like that,

00:33:00.840 --> 00:33:02.440
that's obviously the technique.

00:33:02.440 --> 00:33:04.640
But one of the key techniques that people have been,

00:33:04.640 --> 00:33:07.200
you know, kind of discovering, rediscovering, I guess,

00:33:07.200 --> 00:33:10.160
is this idea of word embeddings or vectors.

00:33:10.160 --> 00:33:11.840
And so Word2Vec was this project

00:33:11.840 --> 00:33:14.240
that came out, I think, 11 years ago or so.

00:33:14.240 --> 00:33:15.600
And, you know, there was a big,

00:33:15.600 --> 00:33:17.560
the big meme around that was,

00:33:17.560 --> 00:33:19.720
you could take the embedding for the word king,

00:33:19.720 --> 00:33:22.560
you could then subtract the embedding for the word man,

00:33:22.560 --> 00:33:24.160
add the word embedding for woman,

00:33:24.160 --> 00:33:27.080
and then the end math result would actually be close

00:33:27.080 --> 00:33:29.200
to the embedding for the word queen.

00:33:29.200 --> 00:33:30.440
And so what is an embedding?

00:33:30.440 --> 00:33:31.280
What's a vector?

00:33:31.280 --> 00:33:33.680
It's basically this large floating point number

00:33:33.680 --> 00:33:37.360
that has semantic meaning inferred into it.

00:33:37.360 --> 00:33:39.560
And it's built just by training a model.

00:33:39.560 --> 00:33:41.320
So just like you train a large language model,

00:33:41.320 --> 00:33:43.280
they can trade these embedding models

00:33:43.280 --> 00:33:46.560
to basically take a word and then take a sentence

00:33:46.560 --> 00:33:48.280
and then take a, you know, a document,

00:33:48.280 --> 00:33:50.240
is what, you know, OpenAI can do,

00:33:50.240 --> 00:33:54.840
and turn that into this big giant 200, 800, 1500,

00:33:54.840 --> 00:33:57.800
you know, depending on the size of the embedding,

00:33:57.800 --> 00:33:59.080
floating point numbers,

00:33:59.080 --> 00:34:00.840
and then use that as a, what's called,

00:34:00.840 --> 00:34:02.840
you know, semantic similarity search.

00:34:02.840 --> 00:34:04.000
So you're basically going off

00:34:04.000 --> 00:34:05.880
and asking for similar documents.

00:34:05.880 --> 00:34:07.400
And so you get those documents,

00:34:07.400 --> 00:34:08.320
and then you make your prompt.

00:34:08.320 --> 00:34:09.160
- It's really wild.

00:34:09.160 --> 00:34:12.560
So, you know, we're gonna make an 800 dimensional space,

00:34:12.560 --> 00:34:15.600
and each concept gets a location in that space,

00:34:15.600 --> 00:34:18.120
and then you're gonna get another concept as a prompt,

00:34:18.120 --> 00:34:20.760
and you say, what other things in this space are near it?

00:34:20.760 --> 00:34:22.880
- The hard problems that remain are,

00:34:22.880 --> 00:34:24.320
well, first you gotta figure out what you're trying to solve.

00:34:24.320 --> 00:34:26.160
So once you figure out what you're actually trying to solve,

00:34:26.160 --> 00:34:28.240
then you can start asking yourself questions like,

00:34:28.240 --> 00:34:31.800
okay, well, how do I chunk up the documents that I have?

00:34:31.800 --> 00:34:33.240
Right, and there's all these different,

00:34:33.240 --> 00:34:34.400
and there's another great place

00:34:34.400 --> 00:34:35.760
for Lama Index and Lang Chain.

00:34:35.760 --> 00:34:37.600
They have chunking strategies,

00:34:37.600 --> 00:34:39.240
where they all take a big giant document

00:34:39.240 --> 00:34:41.600
and break it down into sections,

00:34:41.600 --> 00:34:43.000
and then you chunk each section,

00:34:43.000 --> 00:34:46.520
and then you do the embedding on just that small section.

00:34:46.520 --> 00:34:47.680
Because the idea being,

00:34:47.680 --> 00:34:50.880
can you get, you know, finer and finer sets of text

00:34:50.880 --> 00:34:53.360
that you can then, when you do your retrieval,

00:34:53.360 --> 00:34:55.000
you get the right information back.

00:34:55.000 --> 00:34:56.720
And then the other challenge is really like

00:34:56.720 --> 00:34:58.280
the question answer problem, right?

00:34:58.280 --> 00:34:59.920
If a person's asking a question,

00:34:59.920 --> 00:35:01.400
how do you turn that question

00:35:01.400 --> 00:35:04.040
into the same kind of embedding space as the answer?

00:35:04.040 --> 00:35:05.720
And so there's lots of different strategies

00:35:05.720 --> 00:35:06.680
that are out there for that.

00:35:06.680 --> 00:35:08.760
And then another problem is,

00:35:08.760 --> 00:35:10.320
if you're looking at the Wikipedia page

00:35:10.320 --> 00:35:12.080
for the Tower of Pisa,

00:35:12.080 --> 00:35:13.680
it might actually have like a sentence in here

00:35:13.680 --> 00:35:17.040
that says it is X number of meters tall or feet tall,

00:35:17.040 --> 00:35:19.520
but it won't actually have the word Tower of Pisa in it.

00:35:19.520 --> 00:35:21.640
So there's another chunking strategy

00:35:21.640 --> 00:35:23.880
where they call it propositional trunking,

00:35:23.880 --> 00:35:26.560
where they basically use a large language model

00:35:26.560 --> 00:35:29.400
to actually redefine each sentence

00:35:29.400 --> 00:35:31.880
so that it actually has those proper nouns baked into it

00:35:31.880 --> 00:35:33.400
so that when you do the embedding,

00:35:33.400 --> 00:35:36.720
it doesn't lose some of the detail with propositions.

00:35:36.720 --> 00:35:37.840
- It's this tall, but-

00:35:37.840 --> 00:35:38.680
- It is.

00:35:38.680 --> 00:35:40.600
- It's something that plays this tall

00:35:40.600 --> 00:35:42.560
with its actual height and things like that.

00:35:42.560 --> 00:35:43.400
- Correct.

00:35:43.400 --> 00:35:44.240
- Crazy.

00:35:44.240 --> 00:35:45.560
- But fundamentally you're working with unstructured data

00:35:45.560 --> 00:35:46.680
and it's kind of messy

00:35:46.680 --> 00:35:49.320
and it's not always gonna work the way you want.

00:35:49.320 --> 00:35:50.720
And there's a lot of challenges

00:35:50.720 --> 00:35:52.000
and people are trying lots of different things

00:35:52.000 --> 00:35:52.840
to make it better.

00:35:52.840 --> 00:35:53.680
- That's cool.

00:35:53.680 --> 00:35:55.440
It's not always deterministic or exactly the same.

00:35:55.440 --> 00:35:57.080
So that can be tricky as well.

00:35:58.520 --> 00:36:00.080
This portion of Talk Python to Me

00:36:00.080 --> 00:36:02.600
is brought to you by Neo4j.

00:36:02.600 --> 00:36:04.120
Do you know Neo4j?

00:36:04.120 --> 00:36:07.080
Neo4j is a native graph database.

00:36:07.080 --> 00:36:09.800
And if the slowest part of your data access patterns

00:36:09.800 --> 00:36:11.960
involves computing relationships,

00:36:11.960 --> 00:36:15.060
why not use a database that stores those relationships

00:36:15.060 --> 00:36:16.800
directly in the database,

00:36:16.800 --> 00:36:18.960
unlike your typical relational one?

00:36:18.960 --> 00:36:21.040
A graph database lets you model the data

00:36:21.040 --> 00:36:22.520
the way it looks in the real world,

00:36:22.520 --> 00:36:25.920
instead of forcing it into rows and columns.

00:36:25.920 --> 00:36:28.440
It's time to stop asking a relational database

00:36:28.440 --> 00:36:30.040
to do more than they were made for

00:36:30.040 --> 00:36:33.820
and simplify complex data models with graphs.

00:36:33.820 --> 00:36:35.760
If you haven't used a graph database before,

00:36:35.760 --> 00:36:38.040
you might be wondering about common use cases.

00:36:38.040 --> 00:36:39.320
You know, what's it for?

00:36:39.320 --> 00:36:40.560
Here are just a few.

00:36:40.560 --> 00:36:44.960
Detecting fraud, enhancing AI, managing supply chains,

00:36:44.960 --> 00:36:47.880
gaining a 360 degree view of your data

00:36:47.880 --> 00:36:51.320
and anywhere else you have highly connected data.

00:36:51.320 --> 00:36:53.820
To use Neo4j from Python,

00:36:53.820 --> 00:36:57.100
it's a simple pip install Neo4j.

00:36:57.100 --> 00:36:58.300
And to help you get started,

00:36:58.300 --> 00:37:00.180
their docs include a sample web app

00:37:00.180 --> 00:37:04.200
demonstrating how to use it both from Flask and FastAPI.

00:37:04.200 --> 00:37:06.020
Find it in their docs or search GitHub

00:37:06.020 --> 00:37:08.940
for Neo4j movies application quick start.

00:37:08.940 --> 00:37:11.160
Developers are solving some of the world's

00:37:11.160 --> 00:37:13.060
biggest problems with graphs.

00:37:13.060 --> 00:37:14.060
Now it's your turn.

00:37:14.060 --> 00:37:18.660
Visit talkpython.fm/neo4j to get started.

00:37:18.660 --> 00:37:23.520
That's talkpython.fm/neo, the number four and the letter J.

00:37:23.520 --> 00:37:26.420
Thank you to Neo4j for supporting Talk Python to Me.

00:37:26.420 --> 00:37:31.120
One of the big parts of at least this embedding stuff

00:37:31.120 --> 00:37:33.260
you're talking about are vector databases.

00:37:33.260 --> 00:37:34.700
And they used to be really rare

00:37:34.700 --> 00:37:36.660
and kind of their own specialized thing.

00:37:36.660 --> 00:37:38.860
Now they're starting to show up in lots of places.

00:37:38.860 --> 00:37:41.880
And you shared with us this link of vector DB comparison.

00:37:41.880 --> 00:37:43.460
I just saw that MongoDB added it.

00:37:43.460 --> 00:37:46.400
I'm like, I didn't know that had anything to do with that.

00:37:46.400 --> 00:37:47.640
I'm probably not gonna mess with it,

00:37:47.640 --> 00:37:49.820
but it's interesting that it's just like finding its way

00:37:49.820 --> 00:37:51.700
in all these different spaces, you know?

00:37:51.700 --> 00:37:53.020
- It was weird there for a couple of years

00:37:53.020 --> 00:37:54.540
where people were basically like talking

00:37:54.540 --> 00:37:55.460
about vector databases,

00:37:55.460 --> 00:37:57.180
like they're their own separate thing.

00:37:57.180 --> 00:37:58.660
The vector databases are now becoming

00:37:58.660 --> 00:37:59.900
their own fully fledged,

00:37:59.900 --> 00:38:02.500
either relational database or a graph database

00:38:02.500 --> 00:38:03.540
or a search engine, right?

00:38:03.540 --> 00:38:05.860
Those are kind of the three categories where,

00:38:05.860 --> 00:38:07.500
I mean, I guess Redis is its own thing too.

00:38:07.500 --> 00:38:09.820
But for the most part, those new databases,

00:38:09.820 --> 00:38:11.500
quote unquote, are now kind of trying

00:38:11.500 --> 00:38:13.180
to be more fully fledged.

00:38:13.180 --> 00:38:16.260
And vectors and semantic search is really just one feature.

00:38:16.260 --> 00:38:17.940
- I was just thinking that is this thing

00:38:17.940 --> 00:38:19.780
that you're talking about, is it a product

00:38:19.780 --> 00:38:22.340
or is it a feature of a bigger product, right?

00:38:22.340 --> 00:38:23.180
- Correct.

00:38:23.180 --> 00:38:24.020
- If you already got a database,

00:38:24.020 --> 00:38:25.500
it's already doing a bunch of things.

00:38:25.500 --> 00:38:26.940
Did it just answer the vector question?

00:38:26.940 --> 00:38:28.340
Maybe, maybe not, I don't know.

00:38:28.340 --> 00:38:29.180
- Exactly right.

00:38:29.180 --> 00:38:31.140
And the one thing to recognize is that,

00:38:31.140 --> 00:38:32.140
and then the other thing people do

00:38:32.140 --> 00:38:33.740
is they just take NumPy or what have you

00:38:33.740 --> 00:38:35.020
and just load them all into memory.

00:38:35.020 --> 00:38:36.020
And if you don't have that much data,

00:38:36.020 --> 00:38:38.340
that's actually probably gonna be the fastest

00:38:38.340 --> 00:38:40.060
and simplest way to work.

00:38:40.060 --> 00:38:42.020
But the thing you gotta recognize is the fact

00:38:42.020 --> 00:38:44.340
that there is precision and recall

00:38:44.340 --> 00:38:47.300
and cost trade-off that happens as well.

00:38:47.300 --> 00:38:50.200
So they have to index these vectors

00:38:50.200 --> 00:38:52.740
and there's different algorithms that are used

00:38:52.740 --> 00:38:55.380
and different algorithms do better than others.

00:38:55.380 --> 00:38:57.020
So you gotta make sure you understand that as well.

00:38:57.020 --> 00:38:59.580
So, and one thing you can do is for instance,

00:38:59.580 --> 00:39:02.420
pgVector, which comes as an extension for Postgres,

00:39:02.420 --> 00:39:04.980
you can start off by not indexing at all.

00:39:04.980 --> 00:39:06.540
And you should get, I believe,

00:39:06.540 --> 00:39:07.900
hopefully I'm not misspeaking,

00:39:07.900 --> 00:39:09.060
you should get perfect recall,

00:39:09.060 --> 00:39:10.300
meaning you'll get the right answer.

00:39:10.300 --> 00:39:12.820
You'll get the, if you ask for the five closest vectors

00:39:12.820 --> 00:39:15.780
to your query, you'll get the five closest,

00:39:15.780 --> 00:39:17.860
but it'll be slower than you probably want.

00:39:17.860 --> 00:39:19.180
So then you have to index it.

00:39:19.180 --> 00:39:20.640
And then what ends up happening is,

00:39:20.640 --> 00:39:22.620
the next time you might only get four of those five.

00:39:22.620 --> 00:39:24.880
You'll get something else that snuck into that list.

00:39:24.880 --> 00:39:28.120
- If you got time, you're willing to spend unlimited time,

00:39:28.120 --> 00:39:31.560
then you can get the right answer, the exact answer.

00:39:31.560 --> 00:39:33.520
But I guess that's all sorts of heuristics, right?

00:39:33.520 --> 00:39:35.760
You're like, I could spend three days

00:39:35.760 --> 00:39:37.440
or I could do a Monte Carlo thing

00:39:37.440 --> 00:39:40.040
and I could give you the answer in a fraction of a second.

00:39:40.040 --> 00:39:42.240
But it's not deterministic.

00:39:42.240 --> 00:39:43.760
All right, so then I won't go with my camera,

00:39:43.760 --> 00:39:45.200
so I turn it off, I don't know what's up with it,

00:39:45.200 --> 00:39:46.640
but we'll, yeah.

00:39:46.640 --> 00:39:48.980
So you wrote a cool blog post called,

00:39:48.980 --> 00:39:51.380
"What is a Custom GPT?"

00:39:51.380 --> 00:39:54.540
And we wanna talk some about building custom GPTs

00:39:54.540 --> 00:39:56.620
and with SAPI and so on.

00:39:56.620 --> 00:39:57.580
So let's talk about this.

00:39:57.580 --> 00:39:59.920
Like one of the, I think one of the challenges

00:39:59.920 --> 00:40:02.700
in why it takes so much compute for these systems

00:40:02.700 --> 00:40:04.220
is like, they're open-ended.

00:40:04.220 --> 00:40:07.700
You can ask me any question about any knowledge in the world

00:40:07.700 --> 00:40:08.940
in the humankind, right?

00:40:08.940 --> 00:40:11.740
You can ask about that, let's start talking.

00:40:11.740 --> 00:40:15.700
Or it could be, you can ask me about genetics, right?

00:40:15.700 --> 00:40:18.540
That seems like you could both get better answers

00:40:18.540 --> 00:40:21.120
if you actually only care about genetic responses.

00:40:21.120 --> 00:40:23.380
You know, how tall is the Leaning Tower?

00:40:23.380 --> 00:40:25.240
And probably make it smaller, right?

00:40:25.240 --> 00:40:27.880
So is that kind of the idea of these custom GPTs

00:40:27.880 --> 00:40:28.720
or what is it?

00:40:28.720 --> 00:40:32.440
- No, so custom GPTs are new capability from OpenAI.

00:40:32.440 --> 00:40:36.820
And basically they are a wrapper around a very small subset,

00:40:36.820 --> 00:40:40.180
but it's still using the OpenAI ecosystem, okay?

00:40:40.180 --> 00:40:42.360
And so what you do is you give it a name,

00:40:42.360 --> 00:40:44.400
you give it a logo, you give it a prompt.

00:40:44.400 --> 00:40:47.600
And then from there, you can also give it knowledge.

00:40:47.600 --> 00:40:49.300
You can upload PDF documents to it

00:40:49.300 --> 00:40:51.760
and it will actually slice and dice those PDF documents

00:40:51.760 --> 00:40:53.440
using some sort of vector search.

00:40:53.440 --> 00:40:54.860
We don't know how it actually works.

00:40:54.860 --> 00:40:57.320
The GPT, the cool thing is the GPT will work on your phone,

00:40:57.320 --> 00:40:58.160
right?

00:40:58.160 --> 00:40:59.920
So I have my phone, I can have a conversation with my phone.

00:40:59.920 --> 00:41:01.840
I can take a picture, upload a picture

00:41:01.840 --> 00:41:04.060
and it will do vision analysis on it.

00:41:04.060 --> 00:41:07.200
So I get all the capabilities of OpenAI GPT-4.

00:41:07.200 --> 00:41:10.120
But a custom GPT is one that I can construct

00:41:10.120 --> 00:41:12.560
and give a custom prompt to, which basically then says,

00:41:12.560 --> 00:41:14.280
okay, now you're, and to your point,

00:41:14.280 --> 00:41:15.520
I think maybe this is where you're going with it.

00:41:15.520 --> 00:41:17.240
Like, hey, now you're an expert in genomics

00:41:17.240 --> 00:41:18.600
or you're an expert in something

00:41:18.600 --> 00:41:21.200
and you're basically coaching the language model

00:41:21.200 --> 00:41:23.040
and what it can and can't do.

00:41:23.040 --> 00:41:26.040
And so it's a targeted experience

00:41:26.040 --> 00:41:30.560
within the large language, within the ChatGPT ecosystem.

00:41:30.560 --> 00:41:32.680
It has access to also the OpenAI tools.

00:41:32.680 --> 00:41:35.000
Like so OpenAI has the ability to do code interpreter

00:41:35.000 --> 00:41:37.920
and Dolly, and it can also hit the web browser.

00:41:37.920 --> 00:41:39.260
So you have access to everything.

00:41:39.260 --> 00:41:41.600
But the interesting thing to me is the fact

00:41:41.600 --> 00:41:42.720
that you can actually tie this thing

00:41:42.720 --> 00:41:44.300
to what are called actions.

00:41:44.300 --> 00:41:46.320
So March, I think of last year,

00:41:46.320 --> 00:41:48.000
they actually had this capability called plugins

00:41:48.000 --> 00:41:48.880
that they announced.

00:41:48.880 --> 00:41:51.480
And plugins have kind of faded to the background.

00:41:51.480 --> 00:41:53.520
I don't know if they're gonna deprecate them officially,

00:41:53.520 --> 00:41:55.900
but the basic gist with plugins is what was,

00:41:55.900 --> 00:41:58.020
you could turn that on and it can then call your API.

00:41:58.020 --> 00:41:59.200
And the cool thing about it was

00:41:59.200 --> 00:42:01.480
that it read your OpenAPI spec, right?

00:42:01.480 --> 00:42:04.560
So you write an OpenAPI spec, which is Swagger,

00:42:04.560 --> 00:42:06.320
if you're familiar with Swagger,

00:42:06.320 --> 00:42:08.120
and it basically defines what all the endpoints are,

00:42:08.120 --> 00:42:11.560
what the path is, what the inputs and outputs are,

00:42:11.560 --> 00:42:14.480
including classes or field level information

00:42:14.480 --> 00:42:16.040
and any constraints or what have you.

00:42:16.040 --> 00:42:18.720
So you can fully define your OpenAPI spec,

00:42:18.720 --> 00:42:20.440
it can then call that OpenAPI spec,

00:42:20.440 --> 00:42:22.460
and it's basically giving it tools.

00:42:22.460 --> 00:42:24.520
So like the example that they say in the documentation

00:42:24.520 --> 00:42:25.440
is get the weather, right?

00:42:25.440 --> 00:42:27.500
So if you say, what's the weather in Boston?

00:42:27.500 --> 00:42:29.480
Well, Chetchi BT doesn't know the weather in Boston.

00:42:29.480 --> 00:42:30.760
All it knows how to do is call it,

00:42:30.760 --> 00:42:33.080
but you can call an API and it figures out

00:42:33.080 --> 00:42:34.920
how to call the API, get that information,

00:42:34.920 --> 00:42:36.840
and then it can use that to redisplay.

00:42:36.840 --> 00:42:38.640
And that's a very basic example.

00:42:38.640 --> 00:42:41.200
You can do way more complicated things than that.

00:42:41.200 --> 00:42:42.040
It's pretty powerful.

00:42:42.040 --> 00:42:44.480
- Okay, that sounds really pretty awesome.

00:42:44.480 --> 00:42:47.080
I thought a lot about different things that I might build.

00:42:47.080 --> 00:42:49.280
On your blog post here, you've got some key benefits

00:42:49.280 --> 00:42:51.160
and you've got some risks.

00:42:51.160 --> 00:42:53.040
You maybe wanna talk a bit about that?

00:42:53.040 --> 00:42:55.480
- Yeah, so the first part with plugins that was wrong,

00:42:55.480 --> 00:42:56.920
was that didn't work as well,

00:42:56.920 --> 00:43:00.200
is that there was no kind of overarching custom instruction

00:43:00.200 --> 00:43:02.120
that could actually teach it how to work with your plugin.

00:43:02.120 --> 00:43:04.080
So if you couldn't put it in the API spec,

00:43:04.080 --> 00:43:05.320
then you couldn't like integrate it

00:43:05.320 --> 00:43:08.600
with a bunch of other stuff or other capabilities, right?

00:43:08.600 --> 00:43:10.720
So the custom instruction is really a key thing

00:43:10.720 --> 00:43:13.040
for making these custom APIs strong.

00:43:13.040 --> 00:43:14.720
But one warning about the custom instruction,

00:43:14.720 --> 00:43:17.160
whatever you put in there, anybody can download, right?

00:43:17.160 --> 00:43:18.960
Not just the folks at OpenAI, anybody.

00:43:18.960 --> 00:43:21.160
Like basically there's GitHub projects

00:43:21.160 --> 00:43:23.720
where like thousands of these custom prompts

00:43:23.720 --> 00:43:25.880
that people have put into their GPT.

00:43:25.880 --> 00:43:28.120
So, and there are now knockoffs on GPT.

00:43:28.120 --> 00:43:31.720
So it's all kind of a mess right now in the OpenAI store.

00:43:31.720 --> 00:43:32.640
I'm sure they'll clean it up,

00:43:32.640 --> 00:43:35.520
but just recognize the custom instruction is not protected

00:43:35.520 --> 00:43:36.880
and neither is the knowledge.

00:43:36.880 --> 00:43:39.040
So if you upload a PDF, there have been people

00:43:39.040 --> 00:43:41.560
that have been figuring out how to like download those PDFs.

00:43:41.560 --> 00:43:44.040
And I think that that might be a solved problem now

00:43:44.040 --> 00:43:46.440
or they're working on it, but it's something to know.

00:43:46.440 --> 00:43:48.480
The other problem with plugins was,

00:43:48.480 --> 00:43:49.760
I can get a plugin working,

00:43:49.760 --> 00:43:51.680
but if they didn't approve my plugin

00:43:51.680 --> 00:43:53.800
and put it in their plugin store,

00:43:53.800 --> 00:43:55.400
I couldn't share it with other people.

00:43:55.400 --> 00:43:58.400
The way it works now is I can actually make a GPT

00:43:58.400 --> 00:44:00.560
and I can give it to you and you can use it directly,

00:44:00.560 --> 00:44:04.160
even if it's not in the OpenAPI store or OpenAI store.

00:44:04.160 --> 00:44:05.600
You know, it is super easy to get started.

00:44:05.600 --> 00:44:07.680
They have like a tool to like help you generate

00:44:07.680 --> 00:44:10.040
your DALI picture and actually you don't even have to figure

00:44:10.040 --> 00:44:11.560
out how to do the custom instructions yourself.

00:44:11.560 --> 00:44:14.040
You can just kind of chat that into existence.

00:44:14.040 --> 00:44:15.920
But the thing that I'm really excited about

00:44:15.920 --> 00:44:17.840
is that this is like free playing.

00:44:17.840 --> 00:44:20.760
Like you could do, so the hosting cost

00:44:20.760 --> 00:44:22.360
is basically all on the client side.

00:44:22.360 --> 00:44:24.840
You have to be a ChatGPT plus user right now

00:44:24.840 --> 00:44:26.480
to create these and use these.

00:44:26.480 --> 00:44:28.160
But the cool thing as a developer,

00:44:28.160 --> 00:44:29.640
I don't have to pay those API fees

00:44:29.640 --> 00:44:31.000
that we were talking about, right?

00:44:31.000 --> 00:44:33.280
And if I need to use GPT-4,

00:44:33.280 --> 00:44:35.040
which I kind of do for my business right now,

00:44:35.040 --> 00:44:36.800
just because of how complicated it is,

00:44:36.800 --> 00:44:38.440
I don't have to pay those token fees

00:44:38.440 --> 00:44:41.440
for folks using my custom GPT at this moment.

00:44:41.440 --> 00:44:43.680
- Where's like the billing or whatever you call it

00:44:43.680 --> 00:44:47.240
for the custom GPT lies that in the person who's using it,

00:44:47.240 --> 00:44:48.800
does it have to, it goes onto their account

00:44:48.800 --> 00:44:50.840
and whatever their account can do and afford?

00:44:50.840 --> 00:44:52.240
- Yeah, right now, OpenAPI,

00:44:52.240 --> 00:44:55.840
OpenAI ChatGPT plus is $20 a month.

00:44:55.840 --> 00:44:57.280
And then there's a team's version,

00:44:57.280 --> 00:44:58.800
which I think is either 25 or 30,

00:44:58.800 --> 00:45:01.640
depending on the number of users or how you pay for it.

00:45:01.640 --> 00:45:02.520
That's the cost.

00:45:02.520 --> 00:45:05.400
So right now, if you want to use custom GPTs,

00:45:05.400 --> 00:45:08.280
everyone needs to be a ChatGPT plus user.

00:45:08.280 --> 00:45:11.680
There's no extra cost based on usage or anything like that.

00:45:11.680 --> 00:45:14.680
In fact, there's talk about revenue sharing

00:45:14.680 --> 00:45:17.440
between OpenAI and developers of custom GPTs,

00:45:17.440 --> 00:45:19.040
but that has not come out yet

00:45:19.040 --> 00:45:20.720
as far as like what those details are.

00:45:20.720 --> 00:45:23.400
- It does have an App Store feel to it, doesn't it?

00:45:23.400 --> 00:45:24.440
- There's risks too, right?

00:45:24.440 --> 00:45:25.400
Obviously anybody can,

00:45:25.400 --> 00:45:28.080
there's already been like tons of copies up there.

00:45:28.080 --> 00:45:30.920
OpenAI, they're looking for their business model too, right?

00:45:30.920 --> 00:45:34.680
So they could, if someone has a very successful custom GPT,

00:45:34.680 --> 00:45:36.640
it's well within their right to kind of add that

00:45:36.640 --> 00:45:38.440
to the base product as well.

00:45:38.440 --> 00:45:39.760
Injection is still a thing.

00:45:39.760 --> 00:45:41.680
So if you're doing anything in your actions

00:45:41.680 --> 00:45:43.120
that actually changes something,

00:45:43.120 --> 00:45:45.720
that is consequential is what they call it,

00:45:45.720 --> 00:45:47.760
you better think very carefully,

00:45:47.760 --> 00:45:49.800
like what's the worst thing that could happen, right?

00:45:49.800 --> 00:45:51.920
'Cause whatever the worst thing that could happen is,

00:45:51.920 --> 00:45:52.840
that's what's gonna happen.

00:45:52.840 --> 00:45:54.840
'Cause people can figure this stuff out

00:45:54.840 --> 00:45:57.200
and they can confuse the large language models

00:45:57.200 --> 00:45:58.720
into calling them.

00:45:58.720 --> 00:46:00.200
- And the more valuable it is

00:46:00.200 --> 00:46:01.840
that they can make that thing happen,

00:46:01.840 --> 00:46:03.760
the more effort they're gonna put into it as well.

00:46:03.760 --> 00:46:04.920
- Yeah, yeah, yeah.

00:46:04.920 --> 00:46:05.760
- For sure.

00:46:05.760 --> 00:46:06.600
- I was gonna ask,

00:46:06.600 --> 00:46:09.760
do you think it's easy to solve SQL injection

00:46:09.760 --> 00:46:13.720
and other forms of injection, at least in principle, right?

00:46:13.720 --> 00:46:15.320
There's a education problem,

00:46:15.320 --> 00:46:18.080
there's millions of people coming along as developers

00:46:18.080 --> 00:46:20.440
and they see some demo that says,

00:46:20.440 --> 00:46:23.280
the query is like this plus the name.

00:46:23.280 --> 00:46:24.560
Wait a minute, wait.

00:46:24.560 --> 00:46:28.760
So it kind of recreates itself through not total awareness,

00:46:28.760 --> 00:46:32.160
but there is a very clear thing you do solve that,

00:46:32.160 --> 00:46:34.040
you use parameters, you don't concatenate strings

00:46:34.040 --> 00:46:35.560
with user input, problem solved.

00:46:35.560 --> 00:46:37.240
What about prompt injection though?

00:46:37.240 --> 00:46:41.520
It's so vague how these AIs know what to do

00:46:41.520 --> 00:46:42.520
in the first place.

00:46:42.520 --> 00:46:45.280
And so then how do you completely block that off?

00:46:45.280 --> 00:46:46.320
- Unsolved problem.

00:46:46.320 --> 00:46:48.120
I'm definitely stealing from Simon on this

00:46:48.120 --> 00:46:50.000
'cause I've heard him say it on a few podcasts

00:46:50.000 --> 00:46:53.360
is just basically there's no solution as far as we know.

00:46:53.360 --> 00:46:54.800
So you have to design,

00:46:54.800 --> 00:46:57.440
and there's no solution to the hallucination problem either

00:46:57.440 --> 00:46:59.280
'cause that's a feature, right?

00:46:59.280 --> 00:47:01.240
That's actually what the thing is supposed to do.

00:47:01.240 --> 00:47:02.600
So when you're building these systems,

00:47:02.600 --> 00:47:05.280
you have to recognize those two facts

00:47:05.280 --> 00:47:08.080
along with some other facts that really limit

00:47:08.080 --> 00:47:09.400
what you can build with these things.

00:47:09.400 --> 00:47:11.360
- So you shouldn't use it for like legal briefs,

00:47:11.360 --> 00:47:12.200
is that what you're saying?

00:47:12.200 --> 00:47:15.680
- I think these things are great collaborative tools, right?

00:47:15.680 --> 00:47:16.760
The human in the loop,

00:47:16.760 --> 00:47:18.000
and that's everything that I'm building, right?

00:47:18.000 --> 00:47:20.160
So all the stuff that I'm building is assuming

00:47:20.160 --> 00:47:21.840
that the human's in the loop,

00:47:21.840 --> 00:47:25.440
and what I'm trying to do is augment and amplify expertise.

00:47:25.440 --> 00:47:28.800
I'm building tools for people that know about genomics

00:47:28.800 --> 00:47:31.080
and cancer and how to help cancer patients.

00:47:31.080 --> 00:47:32.640
I'm not designing it for cancer patients

00:47:32.640 --> 00:47:35.040
who are gonna go operate on themselves, right?

00:47:35.040 --> 00:47:36.840
Like that's not the goal.

00:47:36.840 --> 00:47:39.960
The idea is there's a lot of information.

00:47:39.960 --> 00:47:41.480
These tools are super valuable

00:47:41.480 --> 00:47:45.200
from like synthesizing a variety of info,

00:47:45.200 --> 00:47:47.840
but you still need to look at the underlying citations.

00:47:47.840 --> 00:47:50.240
And ChatGPT by itself can't give you citations.

00:47:50.240 --> 00:47:51.800
Like it'll make some up.

00:47:51.800 --> 00:47:53.040
It'll say, "Oh, I think there's probably

00:47:53.040 --> 00:47:54.640
"a Wikipedia page with this link."

00:47:54.640 --> 00:47:56.480
But you actually have to,

00:47:56.480 --> 00:47:57.920
you definitely have to have an outside tool,

00:47:57.920 --> 00:48:00.480
either the web, Bing, which is, I would say,

00:48:00.480 --> 00:48:03.080
subpar for a lot of use cases,

00:48:03.080 --> 00:48:04.320
or you have to have actions

00:48:04.320 --> 00:48:06.360
that can actually bring back references

00:48:06.360 --> 00:48:07.280
and give you those links.

00:48:07.280 --> 00:48:08.380
And then the expert will then say,

00:48:08.380 --> 00:48:09.220
"Oh, okay, great.

00:48:09.220 --> 00:48:11.460
"Thanks for synthesizing this, giving me this info.

00:48:11.460 --> 00:48:13.120
"Let me go validate this myself."

00:48:13.120 --> 00:48:15.640
Right, go click on the link and go validate it.

00:48:15.640 --> 00:48:17.680
And that's really, I think that's really the sweet spot

00:48:17.680 --> 00:48:19.240
for these things, at least for the near future.

00:48:19.240 --> 00:48:21.160
- Yeah, don't ask it for the answer.

00:48:21.160 --> 00:48:23.480
Ask it to help you come up with the answer, right?

00:48:23.480 --> 00:48:24.600
- Exactly right. - All right.

00:48:24.600 --> 00:48:26.920
- And then have it criticize you when you do have something

00:48:26.920 --> 00:48:28.140
'cause then it'll do a great job

00:48:28.140 --> 00:48:30.040
of telling you everything you've done wrong.

00:48:30.040 --> 00:48:31.600
- I'm feeling too good about myself.

00:48:31.600 --> 00:48:32.960
I need you to insult me a lot.

00:48:32.960 --> 00:48:34.080
Let's get going.

00:48:34.080 --> 00:48:36.280
All right, speaking of talking about ourselves,

00:48:36.280 --> 00:48:38.760
you've got this project called PyPI GPT.

00:48:38.760 --> 00:48:39.880
What's this about?

00:48:39.880 --> 00:48:43.080
- I really wanted to tell people that FastAPI and Pydantic,

00:48:43.080 --> 00:48:44.800
'cause Python, like we were saying earlier,

00:48:44.800 --> 00:48:46.360
I don't know if it was on the call or not,

00:48:46.360 --> 00:48:49.040
but Python is the winning language, right?

00:48:49.040 --> 00:48:52.040
And I think FastAPI and Pydantic are the winning libraries

00:48:52.040 --> 00:48:54.240
in their respective fields, and they're great.

00:48:54.240 --> 00:48:55.800
And they're perfect for this space

00:48:55.800 --> 00:48:57.840
because you need an open API spec.

00:48:57.840 --> 00:48:59.740
English is the new programming language, right?

00:48:59.740 --> 00:49:02.680
So Andrej Koparthe, who used to work at Tesla

00:49:02.680 --> 00:49:05.000
and now works at OpenAI, has this pinned tweet

00:49:05.000 --> 00:49:05.960
where he's basically like,

00:49:05.960 --> 00:49:07.620
"English is the hottest programming language,"

00:49:07.620 --> 00:49:08.580
or something like that.

00:49:08.580 --> 00:49:09.760
And that's really the truth,

00:49:09.760 --> 00:49:11.160
'cause even in this space

00:49:11.160 --> 00:49:13.240
where I'm building an open API spec,

00:49:13.240 --> 00:49:16.740
99% of the work is thinking about the description

00:49:16.740 --> 00:49:19.840
of the endpoints or the description of the fields

00:49:19.840 --> 00:49:23.240
or codifying the constraints on different fields.

00:49:23.240 --> 00:49:25.520
Like you can use these greater thans and less thans

00:49:25.520 --> 00:49:28.240
and regexes, right, to describe it.

00:49:28.240 --> 00:49:29.580
And so what I did was I said,

00:49:29.580 --> 00:49:32.260
"Okay, let's build this thing in FastAPI,"

00:49:32.260 --> 00:49:34.180
just to get an example out for folks.

00:49:34.180 --> 00:49:35.500
And then I turned it on.

00:49:35.500 --> 00:49:38.180
I actually use ngrok as my service layer

00:49:38.180 --> 00:49:40.420
'cause you have to have HTTPS to make this thing work.

00:49:40.420 --> 00:49:41.540
- Ngrok is so good.

00:49:41.540 --> 00:49:42.380
- Yep. - Yeah.

00:49:42.380 --> 00:49:44.520
- I turned that on with an nginx thing in front of it.

00:49:44.520 --> 00:49:46.800
So this library, to actually use it,

00:49:46.800 --> 00:49:49.400
you'll have to actually set that stuff up yourself.

00:49:49.400 --> 00:49:50.860
You have to download it, you have to run it,

00:49:50.860 --> 00:49:53.620
you have to either get it on a server with HTTPS

00:49:53.620 --> 00:49:55.300
with Let's Encrypt or something.

00:49:55.300 --> 00:49:56.460
Once you've turned it on,

00:49:56.460 --> 00:49:58.360
then you can actually see how it generates

00:49:58.360 --> 00:50:02.080
the OpenAPI spec, how to configure the GPT.

00:50:02.080 --> 00:50:03.640
I didn't do much work with regards

00:50:03.640 --> 00:50:05.440
to the custom instructions that I came up with.

00:50:05.440 --> 00:50:07.500
I just said, "Hey, call my API, figure it out."

00:50:07.500 --> 00:50:08.340
And it does.

00:50:08.340 --> 00:50:10.200
And so what this GPT does is it basically says,

00:50:10.200 --> 00:50:12.000
"Okay, given a package name and a version number,

00:50:12.000 --> 00:50:13.800
it's gonna go and grab this data

00:50:13.800 --> 00:50:15.400
from the SQLite database that I found

00:50:15.400 --> 00:50:17.680
that has this information and then bring it back to you."

00:50:17.680 --> 00:50:20.080
It's the least interesting GPT I could come up with, I guess.

00:50:20.080 --> 00:50:21.600
But it shows kind of the mechanics, right?

00:50:21.600 --> 00:50:24.400
The mechanics of setting up the servers

00:50:24.400 --> 00:50:27.580
and the application within FastAPI,

00:50:27.580 --> 00:50:29.780
the kind of the little things,

00:50:29.780 --> 00:50:31.140
the little bits that you have to flip

00:50:31.140 --> 00:50:34.300
to make sure that OpenAPIs or OpenAI

00:50:34.300 --> 00:50:36.620
can understand your OpenAPI spec,

00:50:36.620 --> 00:50:39.060
bumble through OpenAI and OpenAPI all the time,

00:50:39.060 --> 00:50:40.780
and make sure that they can talk to each other.

00:50:40.780 --> 00:50:42.500
And then it will then do the right thing

00:50:42.500 --> 00:50:45.740
and call your server and bring the answers back.

00:50:45.740 --> 00:50:49.100
And there's a bunch of little flags and information

00:50:49.100 --> 00:50:50.900
you need to know about actions

00:50:50.900 --> 00:50:53.940
that are on the OpenAPI documentation.

00:50:53.940 --> 00:50:55.920
And so I tried to pull that all together

00:50:55.920 --> 00:50:58.640
into one simple little project for people to look at.

00:50:58.640 --> 00:50:59.480
- It's cool.

00:50:59.480 --> 00:51:00.300
So you can ask it questions like,

00:51:00.300 --> 00:51:02.720
"Tell me about FastAPI, this version."

00:51:02.720 --> 00:51:03.560
And it'll come back and-

00:51:03.560 --> 00:51:04.920
- I was hoping to do something a little better,

00:51:04.920 --> 00:51:06.800
like, "Hey, here's my requirements file."

00:51:06.800 --> 00:51:08.760
And go, "Tell me,

00:51:08.760 --> 00:51:10.480
am I on the latest version of everything?"

00:51:10.480 --> 00:51:12.680
Or whatever, something more interesting.

00:51:12.680 --> 00:51:13.520
I just didn't have time.

00:51:13.520 --> 00:51:15.440
- Can you ask it questions such as,

00:51:15.440 --> 00:51:17.360
"What's the difference between this version

00:51:17.360 --> 00:51:18.200
and that version?"

00:51:18.200 --> 00:51:19.960
- You could, if that information's in the database.

00:51:19.960 --> 00:51:21.440
I actually don't know if it is.

00:51:21.440 --> 00:51:24.100
And then obviously you could also hit the PyPI server.

00:51:24.100 --> 00:51:24.940
And I didn't do that.

00:51:24.940 --> 00:51:25.760
I just wanted to,

00:51:25.760 --> 00:51:28.020
I don't wanna be hitting anybody's server

00:51:28.020 --> 00:51:29.560
indiscriminately at this point.

00:51:29.560 --> 00:51:32.300
But that would be a great use case, right?

00:51:32.300 --> 00:51:33.940
So someone could take this

00:51:33.940 --> 00:51:37.340
and certainly add some capabilities.

00:51:37.340 --> 00:51:40.140
The thing that is valuable that I'm trying to showcase

00:51:40.140 --> 00:51:43.380
is the fact that ChatGPT and large language models,

00:51:43.380 --> 00:51:45.100
while they do have the world's information

00:51:45.100 --> 00:51:47.940
kind of compressed at a point in time,

00:51:47.940 --> 00:51:49.600
they are still not a database, right?

00:51:49.600 --> 00:51:51.660
They don't do well when you're basically trying

00:51:51.660 --> 00:51:53.940
to make sure you have a comprehensive query

00:51:53.940 --> 00:51:55.540
and you've brought back all the information.

00:51:55.540 --> 00:51:57.260
And they're also not good from like a

00:51:57.260 --> 00:51:58.460
up-to-date perspective, right?

00:51:58.460 --> 00:51:59.420
There's a cutoff date.

00:51:59.420 --> 00:52:01.460
Thankfully, they finally updated that recently.

00:52:01.460 --> 00:52:03.600
I think it's now April of 2023.

00:52:03.600 --> 00:52:04.440
But at some point,

00:52:04.440 --> 00:52:06.580
it just doesn't know about newer things.

00:52:06.580 --> 00:52:09.700
And so a GPT is a really interesting way of doing that.

00:52:09.700 --> 00:52:10.740
I'm gonna put it out in the universe

00:52:10.740 --> 00:52:11.940
and hopefully someone will do it.

00:52:11.940 --> 00:52:14.320
Make me a modern Python GPT,

00:52:14.320 --> 00:52:15.540
which is basically like,

00:52:15.540 --> 00:52:18.260
get the new version of Pydantic and Polars

00:52:18.260 --> 00:52:21.520
and a few other libraries that ChatGPT does a bad job at

00:52:21.520 --> 00:52:24.280
just because they're in underactive development

00:52:24.280 --> 00:52:27.120
during the time that ChatGPT was getting trained.

00:52:27.120 --> 00:52:29.880
So that's the perfect use cases

00:52:29.880 --> 00:52:32.680
for these types of custom GPTs with knowledge

00:52:32.680 --> 00:52:35.680
in a PDF file or an API backing it up.

00:52:35.680 --> 00:52:38.400
- I think there's a ton of value in being able to feed

00:52:38.400 --> 00:52:40.280
a little bit of your information,

00:52:40.280 --> 00:52:42.880
some of your documents or your code repository

00:52:42.880 --> 00:52:44.760
or something to a GPT

00:52:44.760 --> 00:52:47.320
and then be able to ask it questions about it, right?

00:52:47.320 --> 00:52:48.160
- Yeah.

00:52:48.160 --> 00:52:49.000
- Like, you know,

00:52:49.000 --> 00:52:50.800
tell me about the security vulnerabilities

00:52:50.800 --> 00:52:52.000
that you see in the code.

00:52:52.000 --> 00:52:55.260
Like, is there anywhere where I'm missing some tests

00:52:55.260 --> 00:52:59.040
or I'm calling a function in a way that's known to be bad

00:52:59.040 --> 00:53:02.040
and you know, like that kind of stuff is really tricky.

00:53:02.040 --> 00:53:04.160
But it's also tricky because it doesn't,

00:53:04.160 --> 00:53:05.800
even if you paste in a little bit of code,

00:53:05.800 --> 00:53:07.400
it's not the whole project, right?

00:53:07.400 --> 00:53:09.280
So, you know, to put a little bit more in there

00:53:09.280 --> 00:53:10.280
is pretty awesome.

00:53:10.280 --> 00:53:12.600
- Yeah, being able to give it all the code

00:53:12.600 --> 00:53:14.640
from some of these code repositories, right?

00:53:14.640 --> 00:53:16.820
Like, and bringing back the relevant information.

00:53:16.820 --> 00:53:18.680
So I think there is a kind of this race.

00:53:18.680 --> 00:53:20.160
There's gonna be other, you know, cool,

00:53:20.160 --> 00:53:23.240
there's another cool project called SourceGraph and Cody

00:53:23.240 --> 00:53:25.440
that we can talk about that will, you know,

00:53:25.440 --> 00:53:28.240
run on your local server and basically indexes

00:53:28.240 --> 00:53:30.760
your code base and it'll bring back relevant snippets

00:53:30.760 --> 00:53:33.800
from your code base and answer questions kind of in context.

00:53:33.800 --> 00:53:36.920
And, you know, long-term and the new project,

00:53:36.920 --> 00:53:38.680
I don't know how new, Codeium,

00:53:38.680 --> 00:53:40.480
they had a new paper where they talked about

00:53:40.480 --> 00:53:43.400
flow engineering and flow engineering is just basically

00:53:43.400 --> 00:53:46.120
that same concept of the human in the loop

00:53:46.120 --> 00:53:47.960
with the LLM, with the code,

00:53:47.960 --> 00:53:50.200
that's the magic combination of kind of those people,

00:53:50.200 --> 00:53:53.040
those entities kind of iterating with each other.

00:53:53.040 --> 00:53:53.960
I think these, you know,

00:53:53.960 --> 00:53:55.280
these tools are definitely gonna evolve

00:53:55.280 --> 00:53:58.920
and you really wanna have the ability to have access

00:53:58.920 --> 00:54:01.280
to your specific information

00:54:01.280 --> 00:54:02.760
to answer your specific questions.

00:54:02.760 --> 00:54:05.400
- Cody is new to me, Cody.dev.

00:54:05.400 --> 00:54:08.520
And this little subtitle or whatever is,

00:54:08.520 --> 00:54:11.440
Cody is a coding assistant that uses AI,

00:54:11.440 --> 00:54:13.040
understand your code base, right?

00:54:13.040 --> 00:54:15.080
It was saying, it was about your entire code base,

00:54:15.080 --> 00:54:17.000
APIs, implementations, and idioms.

00:54:17.000 --> 00:54:18.880
Like that's kind of what I was suggesting,

00:54:18.880 --> 00:54:19.720
at least for code, right?

00:54:19.720 --> 00:54:20.680
- Yeah, and Sourcegraph,

00:54:20.680 --> 00:54:24.720
those folks really understand code indexing and searching.

00:54:24.720 --> 00:54:26.120
Like that's what the first product was.

00:54:26.120 --> 00:54:27.120
They were kind of just teed up,

00:54:27.120 --> 00:54:29.120
ready for this large language model moment.

00:54:29.120 --> 00:54:31.680
And then they said, "Oh, let's just put Cody on top of that.

00:54:31.680 --> 00:54:33.800
"So this thing will run, it will understand your code

00:54:33.800 --> 00:54:36.060
"and it will kind of bring things together for you."

00:54:36.060 --> 00:54:38.240
So these folks do podcasts all the time.

00:54:38.240 --> 00:54:39.360
I'd reach out to them.

00:54:39.360 --> 00:54:40.200
- Yeah, interesting.

00:54:40.200 --> 00:54:41.400
It's quite neat looking.

00:54:41.400 --> 00:54:42.600
Think I'm gonna give it a try.

00:54:42.600 --> 00:54:45.800
It both plugs into HRM and VS Code.

00:54:45.800 --> 00:54:46.640
That's pretty neat.

00:54:46.640 --> 00:54:47.460
- Very cool.

00:54:47.460 --> 00:54:48.880
- We're starting to get a little bit short on time here,

00:54:48.880 --> 00:54:51.880
but for people who wanna play with the PyPI GPT,

00:54:51.880 --> 00:54:53.120
maybe as an example,

00:54:53.120 --> 00:54:55.560
to just cut the readme and it's easy to get from there.

00:54:55.560 --> 00:54:56.600
What do you need to tell them?

00:54:56.600 --> 00:54:57.600
- I put a make file in there,

00:54:57.600 --> 00:54:59.000
so you know exactly like the steps

00:54:59.000 --> 00:55:00.320
to kind of make the environment,

00:55:00.320 --> 00:55:02.720
download the files and just ping me,

00:55:02.720 --> 00:55:04.580
follow me on Twitter, iMore,

00:55:04.580 --> 00:55:06.480
and ping me if you need anything there.

00:55:06.480 --> 00:55:09.080
I'm also on LinkedIn and GitHub, right?

00:55:09.080 --> 00:55:12.000
So you can certainly reach out if you have any challenges.

00:55:12.000 --> 00:55:12.820
- Excellent.

00:55:12.820 --> 00:55:13.680
- The last thing that folks

00:55:13.680 --> 00:55:15.360
that are actually in the medical space, right?

00:55:15.360 --> 00:55:18.160
So the thing that I'm working on right now actively

00:55:18.160 --> 00:55:21.480
is how to integrate this thing with our knowledge base, right?

00:55:21.480 --> 00:55:24.440
So I have a knowledge base of hand curated trials

00:55:24.440 --> 00:55:27.520
and curated therapies and other information,

00:55:27.520 --> 00:55:31.120
built it so that my custom GPT can actually work with that,

00:55:31.120 --> 00:55:33.000
come up with some, I'd say novel.

00:55:33.000 --> 00:55:34.460
I always haven't seen anybody else

00:55:34.460 --> 00:55:36.120
and I haven't seen any research

00:55:36.120 --> 00:55:38.200
approaching things the same way I am

00:55:38.200 --> 00:55:40.020
that handles some of the other challenges

00:55:40.020 --> 00:55:40.960
that are out there, right?

00:55:40.960 --> 00:55:43.800
So for instance, the context window is a challenge.

00:55:43.800 --> 00:55:46.480
So the context window is the amount of text that's in there

00:55:46.480 --> 00:55:49.640
and how it gets processed.

00:55:49.640 --> 00:55:53.400
If you're making decisions and you're changing course,

00:55:53.400 --> 00:55:56.720
the chatbot will lose track of those changes, right?

00:55:56.720 --> 00:56:00.560
So if you're experimenting or going down one path of inquiry

00:56:00.560 --> 00:56:02.600
and then you switch to another path,

00:56:02.600 --> 00:56:05.400
it can get confused and forget that you switched paths.

00:56:05.400 --> 00:56:08.200
- Or just run out of space to hold all that information.

00:56:08.200 --> 00:56:10.880
Like, well, it forgot the last three things

00:56:10.880 --> 00:56:12.000
or the first three things you told it.

00:56:12.000 --> 00:56:13.640
It only knows four and you think it knows seven

00:56:13.640 --> 00:56:15.400
and it's working incomplete, right?

00:56:15.400 --> 00:56:16.240
- Yep.

00:56:16.240 --> 00:56:18.240
And one of the key things is

00:56:18.240 --> 00:56:20.520
you actually want it to forget some things as well, right?

00:56:20.520 --> 00:56:23.400
So those are all interesting challenges.

00:56:23.400 --> 00:56:26.120
And I'm actually working with these custom GPTs

00:56:26.120 --> 00:56:29.320
to kind of change the way that the collaboration works

00:56:29.320 --> 00:56:31.840
between the human, the expert,

00:56:31.840 --> 00:56:34.200
the large language model or the assistant

00:56:34.200 --> 00:56:37.640
and my backend, my actual retrieval model,

00:56:37.640 --> 00:56:39.760
the API that's actually doing stuff.

00:56:39.760 --> 00:56:44.240
- So are researchers and MDs and PhDs at your company

00:56:44.240 --> 00:56:46.200
talking with this thing and making use of it?

00:56:46.200 --> 00:56:48.280
- Yeah, I mean, we're in active development right now.

00:56:48.280 --> 00:56:49.520
We have a few key opinion leaders

00:56:49.520 --> 00:56:52.040
that are working with us and collaborating with us,

00:56:52.040 --> 00:56:53.320
but we're always looking for more folks

00:56:53.320 --> 00:56:55.280
that are in the field that actually...

00:56:55.280 --> 00:56:58.240
And right now you need kind of the cutting edge people.

00:56:58.240 --> 00:57:00.320
This stuff's not ready for prime time.

00:57:00.320 --> 00:57:02.880
Clinical decision support is a really hard problem,

00:57:02.880 --> 00:57:06.440
but we need the folks that wanna get ahead of it

00:57:06.440 --> 00:57:08.280
'cause we know that there are doctors

00:57:08.280 --> 00:57:09.800
and there are patients that are asking

00:57:09.800 --> 00:57:11.160
ChatGPT questions right now.

00:57:11.160 --> 00:57:13.440
And even if it says, I'm not a medical expert, blah, blah,

00:57:13.440 --> 00:57:14.720
blah, and at the end of the day,

00:57:14.720 --> 00:57:16.560
we actually don't have enough doctors, right?

00:57:16.560 --> 00:57:17.920
That's the other scary thing

00:57:17.920 --> 00:57:20.720
is we don't have enough doctors, patients want answers.

00:57:20.720 --> 00:57:23.520
How do we build solutions that can allow this expertise

00:57:23.520 --> 00:57:27.400
to get more democratized and more into folks' hands?

00:57:27.400 --> 00:57:30.720
And I'm hoping our tool along with

00:57:30.720 --> 00:57:32.280
these large language models can help

00:57:32.280 --> 00:57:33.880
relieve some of that burden.

00:57:33.880 --> 00:57:37.360
- It might not be as 100% accurate, 100% precise,

00:57:37.360 --> 00:57:38.920
but neither are doctors, right?

00:57:38.920 --> 00:57:40.080
They get stuff wrong.

00:57:40.080 --> 00:57:43.680
You just need to be in the realm of as good as a doctor.

00:57:43.680 --> 00:57:47.520
You don't need to be completely without making a mistake.

00:57:47.520 --> 00:57:49.160
And that's, I think, a challenge

00:57:49.160 --> 00:57:52.880
that we're just gonna have to get used to in general.

00:57:52.880 --> 00:57:54.640
I joked about the legal brief thing

00:57:54.640 --> 00:57:57.280
'cause someone got in trouble for submitting a brief

00:57:57.280 --> 00:57:59.640
that had hallucinations in it.

00:57:59.640 --> 00:58:01.240
And there's certain circumstances

00:58:01.240 --> 00:58:02.720
where maybe it's just not acceptable,

00:58:02.720 --> 00:58:05.680
but AI self-driven cars, people crash,

00:58:05.680 --> 00:58:08.040
but that's a human mistake.

00:58:08.040 --> 00:58:09.560
But when a machine makes it,

00:58:09.560 --> 00:58:12.440
it's a pre-programmed, predetermined mistake.

00:58:12.440 --> 00:58:13.280
Something like that.

00:58:13.280 --> 00:58:16.240
It doesn't feel the same as if the machine made a mistake.

00:58:16.240 --> 00:58:18.440
So if a machine makes a recommendation

00:58:18.440 --> 00:58:20.160
like you need this cancer treatment,

00:58:20.160 --> 00:58:23.400
or you're fine, you don't need it, and it was wrong,

00:58:23.400 --> 00:58:25.160
people are not gonna be as forgiving.

00:58:25.160 --> 00:58:27.320
But it doesn't mean there's not value

00:58:27.320 --> 00:58:29.680
to be gained from systems that can help you, right?

00:58:29.680 --> 00:58:31.960
- I always appreciate those machine learning papers

00:58:31.960 --> 00:58:34.560
that'll show the tracking over time

00:58:34.560 --> 00:58:36.480
of how the models have gotten better and better,

00:58:36.480 --> 00:58:37.600
and they put the human in there,

00:58:37.600 --> 00:58:39.680
and you can see that the human has already gotten eclipsed

00:58:39.680 --> 00:58:42.760
by the models, and that's a specific problem, right?

00:58:42.760 --> 00:58:45.680
'Cause it's also recognizing that a lot of this stuff,

00:58:45.680 --> 00:58:47.160
these models that are doing tasks

00:58:47.160 --> 00:58:48.520
are doing one specific task.

00:58:48.520 --> 00:58:49.600
They're not doing a whole job.

00:58:49.600 --> 00:58:51.960
They're not doing an end-to-end process.

00:58:51.960 --> 00:58:53.640
They're answering a medical question,

00:58:53.640 --> 00:58:55.360
or they're looking at an image

00:58:55.360 --> 00:58:58.000
and finding all the cats or whatever it's supposed to do.

00:58:58.000 --> 00:59:00.200
So, and to your point, though,

00:59:00.200 --> 00:59:02.160
humans aren't perfect at these tasks either.

00:59:02.160 --> 00:59:03.720
- I think mostly people are gonna be using

00:59:03.720 --> 00:59:05.560
this kind of stuff to help them

00:59:05.560 --> 00:59:07.480
come up with these answers, right?

00:59:07.480 --> 00:59:09.840
My weird Amazon description example

00:59:09.840 --> 00:59:12.400
is gonna be the edge case, not the go-to.

00:59:12.400 --> 00:59:13.240
- Agreed.

00:59:13.240 --> 00:59:15.200
- Yeah, you came in, you spoke to the chatbot,

00:59:15.200 --> 00:59:17.440
here's your diagnosis, have a good day, right?

00:59:17.440 --> 00:59:19.080
Not so much, more like,

00:59:19.080 --> 00:59:20.400
I need some help thinking through this.

00:59:20.400 --> 00:59:24.600
What are some studies that have addressed this, right?

00:59:24.600 --> 00:59:25.880
And those kind of questions.

00:59:25.880 --> 00:59:28.440
- And I hesitate to say it's just a better search engine,

00:59:28.440 --> 00:59:29.840
'cause I actually think it's got

00:59:29.840 --> 00:59:30.920
way more potential than that.

00:59:30.920 --> 00:59:31.760
- I agree.

00:59:31.760 --> 00:59:32.920
- It can have a conversation,

00:59:32.920 --> 00:59:34.680
it can iterate back and forth,

00:59:34.680 --> 00:59:35.840
and what I'm actually trying to do

00:59:35.840 --> 00:59:37.840
is build some state into it, right?

00:59:37.840 --> 00:59:41.080
Some structured way of kind of remembering

00:59:41.080 --> 00:59:42.640
what the conversation was,

00:59:42.640 --> 00:59:44.640
and using a lot of the techniques

00:59:44.640 --> 00:59:46.040
that these large language models are good at

00:59:46.040 --> 00:59:48.080
to actually, to make that actually happen.

00:59:48.080 --> 00:59:50.080
And so that you can actually build a system

00:59:50.080 --> 00:59:52.760
so that the human and the assistant and the backend

00:59:52.760 --> 00:59:55.520
all kind of know what the other party is thinking about

00:59:55.520 --> 00:59:57.040
and that they all work together.

00:59:57.040 --> 00:59:57.880
- Nice.

00:59:57.880 --> 01:00:01.040
For your genomics custom GPT thing

01:00:01.040 --> 01:00:02.360
that you're making internally,

01:00:02.360 --> 01:00:04.040
is that gonna become a product eventually

01:00:04.040 --> 01:00:05.240
if other people are interested?

01:00:05.240 --> 01:00:07.240
Is there some way they can keep tabs on it,

01:00:07.240 --> 01:00:08.560
or is it just internal only?

01:00:08.560 --> 01:00:09.440
- Definitely reach out to me.

01:00:09.440 --> 01:00:11.360
So we're building different versions of GPTs.

01:00:11.360 --> 01:00:13.680
Like we're gonna have a GPT for our curation team

01:00:13.680 --> 01:00:15.120
that curates knowledge,

01:00:15.120 --> 01:00:16.960
and we're building a GPT that, you know,

01:00:16.960 --> 01:00:18.960
my hope is that it'll go to physicians,

01:00:18.960 --> 01:00:21.440
to oncologists and genomic counselors

01:00:21.440 --> 01:00:24.440
and other providers that could actually use this thing.

01:00:24.440 --> 01:00:28.200
Eventually, if it becomes robust enough and stable enough,

01:00:28.200 --> 01:00:30.280
and I don't feel like we're doing a disservice,

01:00:30.280 --> 01:00:31.920
we could certainly make a version of that available

01:00:31.920 --> 01:00:33.080
for cancer patients as well.

01:00:33.080 --> 01:00:34.640
I would, you know, I'd love to have that.

01:00:34.640 --> 01:00:36.840
I just wanna make sure that it's done in a responsible way.

01:00:36.840 --> 01:00:37.880
- Yeah, absolutely.

01:00:37.880 --> 01:00:40.640
Well, I honestly hope that you actually do such a good job

01:00:40.640 --> 01:00:42.760
that we don't have to have cancer research anymore,

01:00:42.760 --> 01:00:47.240
but that's a long, long-term goal, right?

01:00:47.240 --> 01:00:48.480
- That is definitely the end goal.

01:00:48.480 --> 01:00:49.680
And that's really exciting too,

01:00:49.680 --> 01:00:51.640
so is that the new drugs that are coming out,

01:00:51.640 --> 01:00:53.880
new treatments that are coming out,

01:00:53.880 --> 01:00:56.560
it's really just about making sure people are aware of it,

01:00:56.560 --> 01:00:58.320
making sure that they're getting the genetic testing

01:00:58.320 --> 01:00:59.160
that they need, right?

01:00:59.160 --> 01:01:01.680
So if you have a loved one that has,

01:01:01.680 --> 01:01:02.880
unfortunately has cancer,

01:01:02.880 --> 01:01:04.640
make sure that they're at least asking their doctor

01:01:04.640 --> 01:01:06.040
the question about genomic testing

01:01:06.040 --> 01:01:07.680
to make sure that they're getting

01:01:07.680 --> 01:01:08.760
the best possible treatment.

01:01:08.760 --> 01:01:09.600
- Sounds good.

01:01:09.600 --> 01:01:12.080
All right, well, quickly before we get out of here,

01:01:12.080 --> 01:01:15.160
recommendation on some libraries, some project

01:01:15.160 --> 01:01:17.000
that maybe we haven't talked about yet,

01:01:17.000 --> 01:01:18.160
something you came across, people were like,

01:01:18.160 --> 01:01:19.000
"Oh, this would be awesome."

01:01:19.000 --> 01:01:20.080
- We ran out of time.

01:01:20.080 --> 01:01:22.040
I was gonna talk about some of these Pydantic projects.

01:01:22.040 --> 01:01:25.040
So there's Marvin, Instructor, and Outlines.

01:01:25.040 --> 01:01:27.560
So folks should definitely look at those.

01:01:27.560 --> 01:01:29.040
So basically what you do is you take,

01:01:29.040 --> 01:01:31.040
you can describe stuff as Pydantic,

01:01:31.040 --> 01:01:32.560
and then it'll actually just extract it right

01:01:32.560 --> 01:01:34.520
into that Pydantic model for you.

01:01:34.520 --> 01:01:37.000
And that's, so Marvin and Outlines and Instructor.

01:01:37.000 --> 01:01:38.960
So check those guys out, they're awesome.

01:01:38.960 --> 01:01:41.080
And then the other one that I actually had teed up

01:01:41.080 --> 01:01:42.000
was VisiCalc.

01:01:42.000 --> 01:01:45.200
So VisiCalc is like this crazy command line tool.

01:01:45.200 --> 01:01:46.040
It's awesome.

01:01:46.040 --> 01:01:48.800
Like you can basically look at giant CSV files

01:01:48.800 --> 01:01:49.760
all on the command line.

01:01:49.760 --> 01:01:51.760
It has like these hotkeys that you can do.

01:01:51.760 --> 01:01:54.440
And it, sorry, not VisiCalc, Visidata.

01:01:54.440 --> 01:01:55.440
- Visidata, okay.

01:01:55.440 --> 01:01:56.680
- And so basically it's just,

01:01:56.680 --> 01:01:58.680
it's basically Excel inside your terminal.

01:01:58.680 --> 01:02:02.600
And this was before Rich and Textual project.

01:02:02.600 --> 01:02:04.280
And it was just like, it was kind of mind blowing

01:02:04.280 --> 01:02:06.200
all the stuff that this person was able to figure out

01:02:06.200 --> 01:02:07.120
how to make work.

01:02:07.120 --> 01:02:08.040
- That's super amazing.

01:02:08.040 --> 01:02:09.560
I just wanted to give a shout out one more thing,

01:02:09.560 --> 01:02:12.120
'cause your Visidata reminded me of something

01:02:12.120 --> 01:02:14.520
I just came across called Btop.

01:02:14.520 --> 01:02:15.360
- Yep, yep, yep.

01:02:15.360 --> 01:02:16.480
- People have servers out there

01:02:16.480 --> 01:02:18.760
and they need to know what's going on with their server.

01:02:18.760 --> 01:02:20.720
Where's my, I need a picture for this.

01:02:20.720 --> 01:02:24.320
But yeah, it's like a nice visualization.

01:02:24.320 --> 01:02:26.600
There's also a Bhytop.

01:02:26.600 --> 01:02:28.960
It's pretty amazing what people can do in the terminal.

01:02:28.960 --> 01:02:29.800
Oh, there they are.

01:02:29.800 --> 01:02:31.760
They're just responsive design themselves out.

01:02:31.760 --> 01:02:34.360
But yeah, if you want a bunch of live graphs,

01:02:34.360 --> 01:02:35.600
every time I see stuff like this,

01:02:35.600 --> 01:02:39.280
the Visidata or this or what textual folks are working on,

01:02:39.280 --> 01:02:41.760
it's just like, I can't believe they built this.

01:02:41.760 --> 01:02:43.760
I'm working at the level of Colorama.

01:02:43.760 --> 01:02:45.560
This string is red right here.

01:02:45.560 --> 01:02:47.400
They're like, oh yeah, we rebuilt it.

01:02:47.400 --> 01:02:49.200
- I got an emoji to show up, right?

01:02:49.200 --> 01:02:50.040
I'm excited.

01:02:50.040 --> 01:02:51.360
- Yes, exactly, yes.

01:02:51.360 --> 01:02:54.040
A rocket ship is there, not just tech.

01:02:54.040 --> 01:02:55.160
- Yeah, pretty excellent.

01:02:55.160 --> 01:02:56.000
All right.

01:02:56.000 --> 01:02:58.480
Well, Ian, thank you for being here

01:02:58.480 --> 01:03:00.080
and keep up the good work.

01:03:00.080 --> 01:03:03.080
I know so many people are using LLMs,

01:03:03.080 --> 01:03:05.720
but not that many people are creating LLMs.

01:03:05.720 --> 01:03:08.200
And as developers, we love to create things.

01:03:08.200 --> 01:03:09.520
We already have the tools to do it.

01:03:09.520 --> 01:03:14.400
People can check out your GitHub repo on the high PI GPT

01:03:14.400 --> 01:03:16.240
and use it as a starting place, right?

01:03:16.240 --> 01:03:17.080
- Sounds great.

01:03:17.080 --> 01:03:19.560
Yeah, and definitely reach out if you have any questions.

01:03:19.560 --> 01:03:20.400
- Excellent.

01:03:20.400 --> 01:03:21.400
Well, thanks for coming back on the show.

01:03:21.400 --> 01:03:22.240
Catch you all later.

01:03:22.240 --> 01:03:23.080
- Great, good to talk to you.

01:03:23.080 --> 01:03:23.920
Bye bye.

01:03:23.920 --> 01:03:25.200
- Bye.

01:03:25.200 --> 01:03:28.480
This has been another episode of Talk Python to Me.

01:03:28.480 --> 01:03:29.920
Thank you to our sponsors.

01:03:29.920 --> 01:03:31.280
Be sure to check out what they're offering.

01:03:31.280 --> 01:03:33.520
It really helps support the show.

01:03:33.520 --> 01:03:35.120
Take some stress out of your life.

01:03:35.120 --> 01:03:37.320
Get notified immediately about errors

01:03:37.320 --> 01:03:39.320
and performance issues in your web

01:03:39.320 --> 01:03:41.240
or mobile applications with Sentry.

01:03:41.240 --> 01:03:46.120
Just visit talkpython.fm/sentry and get started for free.

01:03:46.120 --> 01:03:50.040
And be sure to use the promo code, talkpython, all one word.

01:03:50.040 --> 01:03:52.880
It's time to stop asking relational databases

01:03:52.880 --> 01:03:54.680
to do more than they were made for

01:03:54.680 --> 01:03:58.200
and simplify complex data models with graphs.

01:03:58.200 --> 01:04:00.520
Check out the sample FastAPI project

01:04:00.520 --> 01:04:04.760
and see what Neo4j, a native graph database, can do for you.

01:04:04.760 --> 01:04:09.760
Find out more at talkpython.fm/neo4j.

01:04:09.760 --> 01:04:11.080
Want to level up your Python?

01:04:11.080 --> 01:04:12.840
We have one of the largest catalogs

01:04:12.840 --> 01:04:15.200
of Python video courses over at Talk Python.

01:04:15.200 --> 01:04:17.280
Our content ranges from true beginners

01:04:17.280 --> 01:04:20.240
to deeply advanced topics like memory and async.

01:04:20.240 --> 01:04:22.920
And best of all, there's not a subscription in sight.

01:04:22.920 --> 01:04:26.160
Check it out for yourself at training.talkpython.fm.

01:04:26.160 --> 01:04:27.760
Be sure to subscribe to the show,

01:04:27.760 --> 01:04:30.760
open your favorite podcast app, and search for Python.

01:04:30.760 --> 01:04:32.140
We should be right at the top.

01:04:32.140 --> 01:04:35.020
You can also find the iTunes feed at /itunes,

01:04:35.020 --> 01:04:37.200
the Google Play feed at /play,

01:04:37.200 --> 01:04:41.660
and the direct RSS feed at /rss on talkpython.fm.

01:04:41.660 --> 01:04:44.280
We're live streaming most of our recordings these days.

01:04:44.280 --> 01:04:45.400
If you want to be part of the show

01:04:45.400 --> 01:04:47.680
and have your comments featured on the air,

01:04:47.680 --> 01:04:49.560
be sure to subscribe to our YouTube channel

01:04:49.560 --> 01:04:52.760
at talkpython.fm/youtube.

01:04:52.760 --> 01:04:54.160
This is your host, Michael Kennedy.

01:04:54.160 --> 01:04:55.320
Thanks so much for listening.

01:04:55.320 --> 01:04:56.560
I really appreciate it.

01:04:56.560 --> 01:04:59.120
Now get out there and write some Python code.

01:04:59.120 --> 01:05:01.700
(upbeat music)

01:05:17.180 --> 01:05:23.180
--

